# Introduction {.unnumbered}

In this class we are going to look at the basic concepts behind modern day _Data Science_ and _machine learning_ techniques for industrial image processing.
We will always try illustrate the theory directly with [Python](https://www.python.org/) to show how the concepts work programmatically.

The following books cover large sections of the content and serve as the main references for these notes.
They are an excellent read to get further into the topics presented, some also include code in Python and MATLAB:

- Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control - @Brunton2022
- Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow - @Geron2022-xh
- Deep Learning - @Goodfellow-et-al-2016
- Practical Deep Learning for Computer Vision with Python - @Landup
- Designing Machine Learning Systems - @Huyen2022-wh


These notes are intended for engineering students and therefore the mathematical concepts will rarely include rigorous proofs.

We start by discussing _Clustering and Classification_ for @sec-clustering-usl, followed by @sec-clustering-sl.
In both sections we discuss some of the most prominent examples, how they work, and embed them in the context and the various aspects of the classification task. 
In @sec-clustering-ssl we address a topic of label quality and showcase how we can work with only a view labels and still provide good data for supervised methods. 

Next we take a look at _Data management and data engineering_ where we discuss the basic strategies to store data and models, and pipelines to process and move data. 
With `dvc` we introduce a tool for connecting data, code, and trained models in a reliable and reproducible way.
Furthermore, we discuss the importance of an ETL and give a simple yet powerful example with transforming the image basis to Wavelets.

In the third and final part of this lecture we focus on _Neural Networks and Deep Learning_.
After introducing neural networks and connecting a very simple network back to known regression techniques we start building our own neural networks in @sec-nn-nn.
For this we work with `pytorch` but also provide a reference in `keras` in the appendix.
We discuss _backward propagation_ as the key idea behind neural networks. 
After this general introduction and definition of the necessary terms we discuss specific classes @sec-nn-cnn, @sec-nn-autoenc, and @sec-nn-transferl.
For each we provide the general idea and showcase the capabilities.
In @sec-nn-data we address topics regarding the _training data_ and the _labelling_.
This section is the one with the least code as these topics are of a general form and usually already included for the datasets used here.
To finish up the section on neural networks, we discuss some common challenges in the field (@sec-nn-further).
The idea of this last section is to provide some guidance if something is not working as expected. 

With these notes we can not hope to cover everything of importance with the length it needs but we can give a glimpse into

1. data management, labeling and preprocessing
1. life-cycle management of models
1. machine learning operations (MLOps)
1. current and research topics
