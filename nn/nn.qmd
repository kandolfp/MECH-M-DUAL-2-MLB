# Neural Networks {#sec-nn-nn}

Now that we constructed one of the simplest NNs possible we build on this and extend our model.
The first step is to introduce some (non-linear) activation functions or transfer functions
$$
y = f(A, b, x).
$$

Some common functions are:

1. **linear**
    $$ f(x) = x. $$

1. **binary step**
    $$ f(x) = 
        \begin{cases} 
            0 & \text{for}\quad x \leq 0,\\
            1 & \text{for}\quad x > 0.\\
        \end{cases}
    $$

1. **logistic (soft step)** or **sigmoid**
    $$ f(x) = \frac{1}{1 + \exp (-x)}. $$

1. **tanh**
    $$ f(x) = \tanh (x). $$

1. **rectified linear unit** (ReLU)
    $$ \begin{cases} 
            0 & \text{for}\quad x \leq 0,\\
            x & \text{for}\quad x > 0.\\
        \end{cases}
    $$

We can visualize them and their derivatives (we need them later).

```{python}
#| label: fig-nn-singlelayer
#| fig-cap: "Different activation functions and their derivatives."
#| fig-subcap: 
#|   - "Activation functions."
#|   - "Numerical derivates of the activation functions."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import sklearn
import scipy
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

import scipy.differentiate

x = np.linspace(-4, 4, 1025, endpoint=True)
linear = lambda x: x
binary = lambda x: np.heaviside(x, 1)
logistic = lambda x: 1 / (1 + np.exp(-x))
tanh = lambda x: np.tanh(x)
relu = lambda x: np.maximum(0, x)

plt.figure()
plt.plot(x, linear(x), "-", label="Linear")
plt.plot(x, binary(x), ":", label="Binary")
plt.plot(x, logistic(x),"--",  label="Logistic")
plt.plot(x, tanh(x), "-.", label="tanh")
plt.plot(x, relu(x), label="ReLU",
         marker="o", markevery=100, markersize=3)
plt.legend()
plt.xlim([-4, 4])
plt.ylim([-2, 2])
plt.gca().set_aspect(8 / (4 * 3))

plt.figure()
plt.plot(x, scipy.differentiate.derivative(linear, x).df, "-", label="Linear")
plt.plot(x, scipy.differentiate.derivative(binary, x).df, ":", label="Binary")
plt.plot(x, scipy.differentiate.derivative(logistic, x).df,"--",  label="Logistic")
plt.plot(x, scipy.differentiate.derivative(tanh, x).df, "-.", label="tanh")
plt.plot(x, scipy.differentiate.derivative(relu, x).df, label="ReLU",
         marker="o", markevery=100, markersize=3)
plt.legend()
plt.xlim([-4, 4])
plt.ylim([-0.1, 1.1])
plt.gca().set_aspect(8 / (1.2 * 3))
plt.show()
```


## The trainable parameters of a Neural Network

Before we build our first NN in the next section we need to get a better understanding on the dimensions and available parameters for our optimization involved in training a NN.
As discussed, these parameters are the matrices consisting on the weights, the biases and are of course related to the neurons. 

![First weight matrix for a generic NN, where no connection is present a 0 is inserted.](../_assets/nn/generic_nn_get_dim){#fig-nn-nn-get_dim}


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-params}

## Compute the parameters of the Neural Network

For the NN display in @fig-nn-nn-get_dim answer the following questions:

1. What is the shape of $A_1, A_2, A_3, A_4$?
1. Which of the matrices is sparse?
1. What is the shape of $b_1, b_2, b_3, b_4$?
1. Write down the (expanded) formula for the output $y$ with respect to the input $x$ resulting from the composition of the different layers for $f(x) = x$ in each layer.
1. Is this formulation a good option to compute $y$ from $x$ (provide some reasoning)?
::::
:::

In @fig-nn-nn-get_dim we also used the formulation of _hidden layers_ for all the layers _inside_ the NN.
This is a common formulation reflecting the nature of the NN that their _activations_ (transfer from their specific input to output $f_j(A_j, b_j, x^{j-1})$) are not exposed to the user and can not be observed directly.

Now let us build our first NN.

## Our first Neural Network

First we split our data into a training and test set.
It is important, that we scale our image data.
This wa not necessary for the other methods but will make sure we get feasible results in this case. 
We scale by $255$ our theoretical maximum in an image. 

```{python}
#| code-fold: true
#| code-summary: "Show the code for loading, splitting and scaling the images."
import numpy as np
import scipy
import requests
import io
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]


response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

X_train = np.concatenate((dogs_w[:, :40], cats_w[:, :40]), axis=1).T / 255.0
y_train = np.repeat(np.array([0, 1]), 40)
X_test = np.concatenate((dogs_w[:, 40:], cats_w[:, 40:]), axis=1).T / 255.0
y_test = np.repeat(np.array([0, 1]), 40)


def myplot(y):
    plt.figure()
    n = y.shape[0]
    plt.bar(range(n), y)
    plt.plot([-0.5, n - 0.5], [0, 0], "k", linewidth=1.0)
    plt.plot([n // 2 - 0.5, y.shape[0] // 2 - 0.5], [-1.1, 1.1], "r-.", linewidth=3)
    plt.yticks([-0.5, 0.5], ["cats", "dogs"], rotation=90, va="center")
    plt.text(n // 4, 1.05, "dogs")
    plt.text(n // 4 * 3, 1.05, "cats")
    plt.gca().set_aspect(n / (2 * 3))
```

Now we can build our model.
In order to do so we need to slightly modify our output.
Instead of having a single neuron that is zero or one we will have two neurons, i.e. $y\in\mathbb{R^2}$
$$
y = \left[\begin{array}{c}1 \\ 0\end{array}\right] = \text{dog},
\quad\text{and}\quad
y = \left[\begin{array}{c}0 \\ 1\end{array}\right] = \text{cat}.
$$

The model will provide us with the probability of the image being a dog on $y_1$ and a cat on $y_2$.

Next, we define our activation function in the first layer by the non-linear function $f_1 = \tanh$, the rest stays the same as before with our weights (combined to matrix $A_1$ and our bias vector $b_1$).
The result needs to be translated into probabilities, this is done via the `softmax` function $\sigma$ (see @sec-appendix-softmax for a more detailed explanation and example)
$$
\sigma: \mathbb{R^n} \to (0, 1)^n, \quad \sigma(x)_i = \frac{\exp(x_i)}{\sum_j{\exp(x_j)}}.
$$

We visualized the network in @fig-nn-cvd-nl.

![A two layer structure for the cats vs. dogs classification with a non-linear model.](../_assets/nn/cats_vs_dogs_nonlinear){#fig-nn-cvd-nl}

::: {.callout-important}
There are multiple frameworks available for NN.
We will focus on [`pytorch`](https://pytorch.org/).
:::

The following couple of code sections translate @fig-nn-cvd-nl into a `pytorch` model.
The main idea is that we create a model class that inherits from `torch.nn.Module`.
```{python}
import torch

# Initialize everything for reproducibility
torch.manual_seed(6020)
np.random.seed(6020)

class MyFirstNN(torch.nn.Module):                       # <1>
    def __init__(self, input_params):                   # <2>
        super(MyFirstNN, self).__init__()               # <3>
        self.model = torch.nn.Sequential(               # <4>
            torch.nn.Linear(input_params, 2),           # <5>
            torch.nn.Tanh(),                            # <6>
            torch.nn.Linear(2, 2),                      # <7>
            torch.nn.Softmax(dim=1),                    # <8>
        )                                               # <4>
                                                        # <2>
    def forward(self, x):                               # <9>
        y = self.model(x)                               # <9>
        return y                                        # <9>
                                                        # <1>
```
1. Create a class inheriting from `torch.nn.Module`.
1. The layers of the NN are defined in the initialization of the class. 
1. Do not forget to initialize the super class.
1. We define a sequential model, this reflects best our desired structure.
1. We define our first neurons that take in our input vector and reduce them to 2.
1. Our activation function for this layer is `tanh`.
1. Definition of the next layer (optional but reflects the structure better).
1. Activation function for the second layer.
1. In `forward` we define how data moves through the network.


```{python}
#| classes: styled-output
import torchinfo

model = MyFirstNN(X_train.shape[1])
batch_size = 8
info = torchinfo.summary(model, (batch_size, X_train.shape[1]), col_width=12)
# Nicer formatting for the notes
info.layer_name_width=15
print(info)
```
It is often also quite useful to export the model as an image.
In our case we can work again with a `dot` file rendered as @fig-nn-nn-model-visual.
```{python}
#| output: false
import torchviz
x = torch.randn(1, X_train.shape[1])
y = model(x)
dot = torchviz.make_dot(y, params=dict(model.named_parameters()))
dot.save("model.dot")
```
```{dot}
//| label: fig-nn-nn-model-visual
//| fig-cap: "Model with all its parameters visualized."
//|file: model.dot
```

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-params}

## Explain the output of `.summary()`

For the (extended) output of `.summary()`
```{python}
#| classes: styled-output
import torchinfo

model = MyFirstNN(X_train.shape[1])
batch_size = 8
info = torchinfo.summary(model, (batch_size, X_train.shape[1]),
                  col_names=["input_size", "output_size", "num_params"],
                  col_width=12)
info.layer_name_width=15
print(info)

print(f"The estimated total size in KB {info.total_param_bytes / 1024}")
```
answer the following questions.

1. Explain how the `Param #` column is computed and retrace the details.
1. If we expand the `Params size` to (KB) we get `{python} float(np.round(info.total_param_bytes / 1024, 2))`, what does this imply on the data type of the parameters?
::::
:::

Next, we need to prepare our data for for the model.
The easiest way is to use the provided `DataLoader` class.
As we have only a very limited number of training data we split our available data into a training and validation set in each epoch.
To provide a better overview, we use a separate function for this procedure. 

```{python}
from torch.utils.data import DataLoader, TensorDataset, random_split


def get_dataset(dataset, validation_split, batch_size):
    val_size = int(validation_split * len(dataset))
    train_size = len(dataset) - val_size
    train_ds, val_ds = random_split(dataset, [train_size, val_size])

    # Create a dataset
    train = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val = DataLoader(val_ds, batch_size=batch_size, shuffle=False)
    return train, val
```

In `pytorch` we are responsible for our training function.
This allows for a lot of flexibility.
The structure is always quite similar.
```{python}
def train_model(model, dataset, loss_fn, optimizer,
                epochs=130, validation_split=0.1, 
                batch_size=8):

    met = {                                                            # <1>
        "train_loss": [],
        "val_loss": [],
        "train_acc": [],
        "val_acc": []}                                                     # <1>

    for epoch in range(epochs):                                            # <2>
        model.train()                                                      # <3>
        train_loss, train_corr = 0, 0
        train_dl, val_dl = get_dataset(dataset, validation_split, batch_size)

        for X_batch, y_batch in train_dl:                                  # <4>
            y_pred = model(X_batch)            # Forward pass through the model
            loss = loss_fn(y_pred, y_batch)    # Compute the loss
            loss.backward()                    # Backpropagation
            optimizer.step()                   # Update the model parameters
            optimizer.zero_grad()              # Reset the gradients

            train_loss += loss.item()
            train_corr += (y_pred.argmax(1) == y_batch).sum().item()       # <4>

        model.eval()                                                       # <5>
        val_loss, val_corr = 0, 0                                       
        with torch.no_grad():                   # No gradient calculation
            for X_val, y_val in val_dl:                                    # <6>
                y_val_pred = model(X_val)
                val_loss += loss_fn(y_val_pred, y_val).item()
                val_corr += (y_val_pred.argmax(1) == y_val).sum().item()   # <6>

        met["train_loss"].append(train_loss / len(train_dl))               # <7>
        met["val_loss"].append(val_loss / len(val_dl))
        met["train_acc"].append(train_corr / len(train_dl.dataset))
        met["val_acc"].append(val_corr / len(val_dl.dataset))              # <7>
                                                                           # <2>
    return met
```
1. Create a dict for our metrics.
1. The training loop for each epoch.
1. The model needs to be in the `train` mode, otherwise the parameters can not be changed.
1. The actual training, the data is processed in batches and the optimization is computed for each batch.
1. To allow validation, the model needs to be in `eval` mode.
1. Validation step, that computes the necessary metrics for our validation set.
1. Keep track of the metrics of our training. We log the average loss and the accuracy.

Now all parts are available and we can finally train our model.
```{python}
from torch.utils.data import TensorDataset

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)                # <1>
y_train_tensor = torch.tensor(y_train, dtype=torch.long)                   # <1>
dataset = TensorDataset(X_train_tensor, y_train_tensor)                    # <2>

loss_fn = torch.nn.CrossEntropyLoss()                                      # <3>
optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)                   # <4>

history = train_model(model, dataset, loss_fn, optimizer,                  # <5>
                      epochs=130,
                      validation_split=0.1,
                      batch_size=batch_size)                               # <5>
```
1. We need to convert the training data to _PyTorch tensors_ unfortinatly, this might result in larger data if we are not careful, `long` is required for the loss computation.
1. Combine $X$ and $Y$ such that it can easily be used for a `DataLoader`.
1. Define the loss function.
1. Define the optimizer.
1. Call the training loop.

This looks like a lot but actually it is boiler plate code if needed and allows for quite a lot of flexibility if required. 

Now that training is complete we can have a look at the performance.
```{python}
#| label: fig-nn-nn-dvc-non-linear
#| fig-cap: "Performance of our model."
#| fig-subcap: 
#|   - "Classification for our test set."
#|   - "Probabilities of the two classes - our actual output of the model."
#|   - "Summary of the key metrics of the model training."
#| code-fold: true
#| code-summary: "Show the code for the figure"
model.eval()
with torch.no_grad():
    y_proba = model(torch.tensor(X_test, dtype=torch.float32))
y_predict = y_proba.argmax(axis=-1)
myplot(y_predict * (-2) + 1)
acc = (y_predict == torch.tensor(y_test)).sum().item() / len(y_test)

n = y_proba.shape[0]
plt.figure()
plt.bar(range(n), y_proba[:, 0], color='y', label=r"p(dog)")
plt.bar(range(n), y_proba[:, 1], color='b', label=r"p(cat)", 
        bottom=y_proba[:, 0])
plt.plot([-0.5, n - 0.5], [0.5, 0.5], "r", linewidth=1.0)
plt.gca().set_aspect(n / (1 * 3))
plt.legend()

plt.figure()

epochs = range(len(history["train_acc"]))
plt_list = [["train_acc", "-"], ["train_loss", ":"],
            ["val_acc", "--"], ["val_loss", "-."]]

for name, style in plt_list:
    plt.plot(epochs, history[name], style, label=name)
plt.legend(loc="lower right")
plt.gca().set_aspect(130 / (1 * 3))
plt.show()
```
Overall, we have an accuracy of `{python} float(np.round(acc  * 100, 2))`% for our test set, better than with our linear models.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-lr}

## Learning rate and momentum

The used optimizer [`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) has two options we want to investigate further. The learning rate `lr` and the `momentum`.

1. Change the learning rate and see how this influences performance, you might want to increase `epochs` as well. Try $lr \in [10^{-1}, 10^{-4}]$.
1. Change the momentum between $0$ and $1$, can this _improve_ the predictions for different learning rates and such that the NN is _more sure_ of its decision (in @fig-nn-nn-dvc-non-linear-2 the bars are not almost equal but lean to one side). 
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-adam}

## Change the optimizer

As we have seen so often before, the optimizer used for the problem can change how fast convergence is reached (if at all). 

1. Have a look at the possibilites provided in the framework [Overview - Optimizers](https://pytorch.org/docs/stable/optim.html#algorithms) - they are basically all improvements on Stochastic Gradient Decent.

1. Test different approaches, especially one of the Adam (`Adam`, `AdamW`, `Adamax`) and Ada (`Adadelta`, `Adafactor`, `Adagrad`, `SparseAdam`) implementations and record the performance (final accuracy).
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-val}

## Train and validation split

In the above version of the train loop we split our dataset in each epoch.
Change this such that the split is done once per call of the training. 

1. What is the influence on the training?
1. How is the performance for different optimizers?
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-lightning}

## Optional: PyTorch Lightning

The module [`pytorch lightning`](https://lightning.ai/docs/pytorch/stable/)[^1] promises to streamline the process for the training and to reduce the code.

With the _Lightning in 15 minutes_ (or any other tutorial) rewrite your code to use this framework.

**Note**: This will make the `dvclive` integration required below slightly easier. 
::::
:::


## How to save a `pytroch` model

Of course we can save a `pytorch` model with the methods discussed in @sec-data-mp but it is more convenient to use the dedicated functions, see [docs](https://pytorch.org/tutorials/beginner/saving_loading_models.html).

In short, we only need to call `torch.save(model.state_dict(), file)` to save in the default format using `pickle`, be careful when using it, see @sec-data-model-pickle.

```{.python}
torch.save("model.pt")
loaded_model = model = MyFirstNN(X_train.shape[1])
loaded_model.load_state_dict(torch.load("model.pt", weights_only=True))
```

It is also possible to implement continuous checkpoints during training, see [docs](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training).
This allows us to resume training after an interrupt or simply store the model at the end of the training. 


All of this can be done via the already discussed `dvclive` interface, the implementation is defined as an exercise (if your module uses `lightning` this is even easier). 

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-dvc}

## `dvclive` integration into the `pytorch` training

Implement the [`DVCLive`](https://dvc.org/doc/dvclive/ml-frameworks/pytorch) integration to track the metrics.
::::
:::

::: {.callout-tip}
## Store the model as `ONNX`

It is also possible to export the model in the ONNX format, see @sec-data-model-onnx and for specifics the [docs](https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html).
:::

[^1]: Install it via `pdm add lightning`.