# Neural Networks {#sec-nn-nn}

Now that we constructed one of the simplest NNs possible we build on this and extend our model.
The first step is to introduce some (non-linear) activation functions or transfer functions
$$
y = f(A, b, x).
$$

Some common functions are:

1. **linear**
    $$ f(x) = x. $$

1. **binary step**
    $$ f(x) = 
        \begin{cases} 
            0 & \text{for}\quad x \leq 0,\\
            1 & \text{for}\quad x > 0.\\
        \end{cases}
    $$

1. **logistic (soft step)** or **sigmoid**
    $$ f(x) = \frac{1}{1 + \exp (-x)}. $$

1. **tanh**
    $$ f(x) = \tanh (x). $$

1. **rectified linear unit** (ReLU)
    $$ \begin{cases} 
            0 & \text{for}\quad x \leq 0,\\
            x & \text{for}\quad x > 0.\\
        \end{cases}
    $$

We can visualize them and their derivatives (we need them later).

```{python}
#| label: fig-nn-singlelayer
#| fig-cap: "Different activation functions and their derivatives."
#| fig-subcap: 
#|   - "Activation functions."
#|   - "Numerical derivates of the activation functions."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import sklearn
import scipy
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

import scipy.differentiate

x = np.linspace(-4, 4, 1025, endpoint=True)
linear = lambda x: x
binary = lambda x: np.heaviside(x, 1)
logistic = lambda x: 1 / (1 + np.exp(-x))
tanh = lambda x: np.tanh(x)
relu = lambda x: np.maximum(0, x)

plt.figure()
plt.plot(x, linear(x), "-", label="Linear")
plt.plot(x, binary(x), ":", label="Binary")
plt.plot(x, logistic(x),"--",  label="Logistic")
plt.plot(x, tanh(x), "-.", label="tanh")
plt.plot(x, relu(x), "-.", label="ReLU",
         marker="o", markevery=100, markersize=3)
plt.legend()
plt.xlim([-4, 4])
plt.ylim([-2, 2])
plt.gca().set_aspect(8 / (4 * 3))

plt.figure()
plt.plot(x, scipy.differentiate.derivative(linear, x).df, "-", label="Linear")
plt.plot(x, scipy.differentiate.derivative(binary, x).df, ":", label="Binary")
plt.plot(x, scipy.differentiate.derivative(logistic, x).df,"--",  label="Logistic")
plt.plot(x, scipy.differentiate.derivative(tanh, x).df, "-.", label="tanh")
plt.plot(x, scipy.differentiate.derivative(relu, x).df, "-.", label="ReLU",
         marker="o", markevery=100, markersize=3)
plt.legend()
plt.xlim([-4, 4])
plt.ylim([-0.1, 1.1])
plt.gca().set_aspect(8 / (1.2 * 3))
plt.show()
```

## The trainable parameters of a Neural Network

Before we build our first NN in the next section we need to get a better understanding on the dimensions and available parameters for our optimization involved in training a NN.
As discussed, these parameters are the matrices consisting on the weights, the biases and are of course related to the neurons. 

![Generic linear NN with weights and biases.](../_assets/nn/generic_nn_get_dim){#fig-nn-nn-get_dim}


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-params}

## Compute the parameters of the Neural Network

For the NN display in @fig-nn-nn-get_dim answer the following questions:

1. What is the shape of $A_1, A_2, A_3, A_4$?
1. Which of the matrices is sparse?
1. What is the shape of $b_1, b_2, b_3, b_4$?
1. Write down the (expanded) formula for the output $y$ with respect to the input $x$ resulting from the composition of the different layers for $f(x) = x$ in each layer.
1. Is this formulation a good option to compute $y$ from $x$ (provide some reasoning)?
::::
:::

In @fig-nn-nn-get_dim we also used the formulation of _hidden layers_ for all the layers _inside_ the NN.
This is a common formulation reflecting the nature of the NN that their _activations_ (transfer from their specific input to output $f_j(A_j, b_j, x^{j-1})$) are not exposed to the user and can not be observed directly.

Now let us build our first NN.

## Our first Neural Network

First we split our data into a training and test set.
It is important, that we scale our image data.
This wa not necessary for the other methods but will make sure we get feasible results in this case. 
We scale by $255$ our theoretical maximum in an image. 

```{python}
#| code-fold: true
#| code-summary: "Show the code for loading, splitting and scaling the images."
import numpy as np
import scipy
import requests
import io
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]


response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

X_train = np.concatenate((dogs_w[:, :40], cats_w[:, :40]), axis=1).T / 255.0
y_train = np.repeat(np.array([0, 1]), 40)
X_test = np.concatenate((dogs_w[:, 40:], cats_w[:, 40:]), axis=1).T / 255.0
y_test = np.repeat(np.array([0, 1]), 40)


def myplot(y):
    plt.figure()
    n = y.shape[0]
    plt.bar(range(n), y)
    plt.plot([-0.5, n - 0.5], [0, 0], "k", linewidth=1.0)
    plt.plot([n // 2 - 0.5, y.shape[0] // 2 - 0.5], [-1.1, 1.1], "r-.", linewidth=3)
    plt.yticks([-0.5, 0.5], ["cats", "dogs"], rotation=90, va="center")
    plt.text(n // 4, 1.05, "dogs")
    plt.text(n // 4 * 3, 1.05, "cats")
    plt.gca().set_aspect(n / (2 * 3))
```

Now we can build our model.
In order to do so we need to slightly modify our output.
Instead of having a single neuron that is zero or one we will have two neurons, i.e. $y\in\mathbb{R^2}$
$$
y = \left[\begin{array}{c}1 \\ 0\end{array}\right] = \text{dog},
\quad\text{and}\quad
y = \left[\begin{array}{c}0 \\ 1\end{array}\right] = \text{cat}.
$$

The model will provide us with the probability of the image being a dog on $y_1$ and a cat on $y_2$.

Next, we define our activation function in the first layer by the non-linear function $f_1 = \tanh$, the rest stays the same as before with our weights (combined to matrix $A_1$ and our bias vector $b_1$).
The result needs to be translated into probabilities, this is done via the `softmax` function $\sigma$ (see @sec-appendix-softmax for a more detailed explanation and example)
$$
\sigma: \mathbb{R^n} \to (0, 1)^n, \quad \sigma(x)_i = \frac{\exp(x_i)}{\sum_j{\exp(x_j)}}.
$$

We visualized the network in @fig-nn-cvd-nl.

![A two layer structure for the cats vs. dogs classification with a non-linear model.](../_assets/nn/cats_vs_dogs_nonlinear){#fig-nn-cvd-nl}

::: {.callout-important}
There are multiple frameworks available for NN.
We will focus on [`pytorch`](https://pytorch.org/).
:::

The following couple of code sections translate @fig-nn-cvd-nl into a `pytorch` model.
The main idea is that we create a model class that inherits from `torch.nn.Module`.
```{python}
import torch

# Initialize everything for reproducibility
torch.manual_seed(6020)
np.random.seed(6020)

class MyFirstNN(torch.nn.Module):                       # <1>
    def __init__(self, input_params):                   # <2>
        super(MyFirstNN, self).__init__()               # <3>
        self.model = torch.nn.Sequential(               # <4>
            torch.nn.Linear(input_params, 2),           
            torch.nn.Tanh(),                            
            torch.nn.Linear(2, 2),                      
            torch.nn.Softmax(dim=1),                    
        )                                               # <4>
                                                        # <2>
    def forward(self, x):                               # <5>
        y = self.model(x)                               # <5>
        return y                                        # <5>
                                                        # <1>
```
1. Create a class inheriting from `torch.nn.Module`.
1. The layers of the NN are defined in the initialization of the class. 
1. Do not forget to initialize the super class.
1. We define a sequential model, this reflects best our desired structure, the first layer that reduces down to two neurons and applies the function $\tanh$, and a second for `softmax`.
1. In `forward` we define how data moves through the network.


```{python}
#| classes: styled-output
import torchinfo

model = MyFirstNN(X_train.shape[1])
batch_size = 8
info = torchinfo.summary(model, (batch_size, X_train.shape[1]), col_width=12)
# Nicer formatting for the notes
info.layer_name_width=15
print(info)
```
It is often also quite useful to export the model as an image.
In our case we can work again with a `dot` file rendered as @fig-nn-nn-model-visual.
```{python}
#| output: false
import torchviz
x = torch.randn(1, X_train.shape[1])
y = model(x)
dot = torchviz.make_dot(y, params=dict(model.named_parameters()))
dot.save("model.dot")
```
```{dot}
//| label: fig-nn-nn-model-visual
//| fig-cap: "Model with all its parameters visualized."
//|file: model.dot
```

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-params}

## Explain the output of `.summary()`

For the (extended) output of `.summary()`
```{python}
#| classes: styled-output
import torchinfo

model = MyFirstNN(X_train.shape[1])
batch_size = 8
info = torchinfo.summary(model, (batch_size, X_train.shape[1]),
                  col_names=["input_size", "output_size", "num_params"],
                  col_width=12)
info.layer_name_width=15
print(info)

print(f"The estimated total size in KB {info.total_param_bytes / 1024}")
```
answer the following questions.

1. Explain how the `Param #` column is computed and retrace the details.
1. If we expand the `Params size` to (KB) we get `{python} float(np.round(info.total_param_bytes / 1024, 2))`, what does this imply on the data type of the parameters?
::::
:::

Next, we need to prepare our data for for the model.
The easiest way is to use the provided `DataLoader` class.
As we have only a very limited number of training data we split our available data into a training and validation set in each epoch.
To provide a better overview, we use a separate function for this procedure. 

```{python}
from torch.utils.data import DataLoader, TensorDataset, random_split


def get_dataset(dataset, validation_split, batch_size):
    val_size = int(validation_split * len(dataset))
    train_size = len(dataset) - val_size
    train_ds, val_ds = random_split(dataset, [train_size, val_size])

    # Create a dataset
    train = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val = DataLoader(val_ds, batch_size=batch_size, shuffle=False)
    return train, val
```

In `pytorch` we are responsible for our training function.
This allows for a lot of flexibility.
The structure is always quite similar.
```{python}
def train_model(model, dataset, loss_fn, optimizer,
                epochs=130, validation_split=0.1, 
                batch_size=8):

    met = {                                                            # <1>
        "train_loss": [],
        "val_loss": [],
        "train_acc": [],
        "val_acc": []}                                                     # <1>

    for epoch in range(epochs):                                            # <2>
        model.train()                                                      # <3>
        train_loss, train_corr = 0, 0
        train_dl, val_dl = get_dataset(dataset, validation_split, batch_size)

        for X_batch, y_batch in train_dl:                                  # <4>
            y_pred = model(X_batch)            # Forward pass through the model
            loss = loss_fn(y_pred, y_batch)    # Compute the loss
            loss.backward()                    # Backpropagation
            optimizer.step()                   # Update the model parameters
            optimizer.zero_grad()              # Reset the gradients

            train_loss += loss.item()
            train_corr += (y_pred.argmax(1) == y_batch).sum().item()       # <4>

        model.eval()                                                       # <5>
        val_loss, val_corr = 0, 0                                       
        with torch.no_grad():                   # No gradient calculation
            for X_val, y_val in val_dl:                                    # <6>
                y_val_pred = model(X_val)
                val_loss += loss_fn(y_val_pred, y_val).item()
                val_corr += (y_val_pred.argmax(1) == y_val).sum().item()   # <6>

        met["train_loss"].append(train_loss / len(train_dl))               # <7>
        met["val_loss"].append(val_loss / len(val_dl))
        met["train_acc"].append(train_corr / len(train_dl.dataset))
        met["val_acc"].append(val_corr / len(val_dl.dataset))              # <7>
                                                                           # <2>
    return met
```
1. Create a dict for our metrics.
1. The training loop for each epoch.
1. The model needs to be in the `train` mode, otherwise the parameters can not be changed.
1. The actual training, the data is processed in batches and the optimization is computed for each batch.
1. To allow validation, the model needs to be in `eval` mode.
1. Validation step, that computes the necessary metrics for our validation set.
1. Keep track of the metrics of our training. We log the average loss and the accuracy.

Now all parts are available and we can finally train our model.
```{python}
from torch.utils.data import TensorDataset

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)                # <1>
y_train_tensor = torch.tensor(y_train, dtype=torch.long)                   # <1>
dataset = TensorDataset(X_train_tensor, y_train_tensor)                    # <2>

loss_fn = torch.nn.CrossEntropyLoss()                                      # <3>
optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)                   # <4>

history = train_model(model, dataset, loss_fn, optimizer,                  # <5>
                      epochs=130,
                      validation_split=0.1,
                      batch_size=batch_size)                               # <5>
```
1. We need to convert the training data to _PyTorch tensors_ unfortinatly, this might result in larger data if we are not careful, `long` is required for the loss computation.
1. Combine $X$ and $Y$ such that it can easily be used for a `DataLoader`.
1. Define the loss function.
1. Define the optimizer.
1. Call the training loop.

This looks like a lot but actually it is boiler plate code if needed and allows for quite a lot of flexibility if required. 

Now that training is complete we can have a look at the performance.
```{python}
#| label: fig-nn-nn-dvc-non-linear
#| fig-cap: "Performance of our model."
#| fig-subcap: 
#|   - "Classification for our test set."
#|   - "Probabilities of the two classes - our actual output of the model."
#|   - "Summary of the key metrics of the model training."
#| code-fold: true
#| code-summary: "Show the code for the figure"
model.eval()
with torch.no_grad():
    y_proba = model(torch.tensor(X_test, dtype=torch.float32))
y_predict = y_proba.argmax(axis=-1)
myplot(y_predict * (-2) + 1)
acc = (y_predict == torch.tensor(y_test)).sum().item() / len(y_test)

n = y_proba.shape[0]
plt.figure()
plt.bar(range(n), y_proba[:, 0], color='y', label=r"p(dog)")
plt.bar(range(n), y_proba[:, 1], color='b', label=r"p(cat)", 
        bottom=y_proba[:, 0])
plt.plot([-0.5, n - 0.5], [0.5, 0.5], "r", linewidth=1.0)
plt.gca().set_aspect(n / (1 * 3))
plt.legend()

plt.figure()

epochs = range(len(history["train_acc"]))
plt_list = [["train_acc", "-"], ["train_loss", ":"],
            ["val_acc", "--"], ["val_loss", "-."]]

for name, style in plt_list:
    plt.plot(epochs, history[name], style, label=name)
plt.legend(loc="lower right")
plt.gca().set_aspect(130 / (1 * 3))
plt.show()
```
Overall, we have an accuracy of `{python} float(np.round(acc  * 100, 2))`% for our test set, better than with our linear models.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-lr}

## Learning rate and momentum

The used optimizer [`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) has two options we want to investigate further. The learning rate `lr` and the `momentum`.

1. Change the learning rate and see how this influences performance, you might want to increase `epochs` as well. Try $lr \in [10^{-1}, 10^{-4}]$.
1. Change the momentum between $0$ and $1$, can this _improve_ the predictions for different learning rates and such that the NN is _more sure_ of its decision (in @fig-nn-nn-dvc-non-linear-2 the bars are not almost equal but lean to one side). 
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-adam}

## Change the optimizer

As we have seen so often before, the optimizer used for the problem can change how fast convergence is reached (if at all). 

1. Have a look at the possibilites provided in the framework [Overview - Optimizers](https://pytorch.org/docs/stable/optim.html#algorithms) - they are basically all improvements on Stochastic Gradient Decent.

1. Test different approaches, especially one of the Adam (`Adam`, `AdamW`, `Adamax`) and Ada (`Adadelta`, `Adafactor`, `Adagrad`, `SparseAdam`) implementations and record the performance (final accuracy).
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-val}

## Train and validation split

In the above version of the train loop we split our dataset in each epoch.
Change this such that the split is done once per call of the training. 

1. What is the influence on the training?
1. How is the performance for different optimizers?
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-lightning}

## Optional: PyTorch Lightning

The module [`pytorch lightning`](https://lightning.ai/docs/pytorch/stable/)[^1] promises to streamline the process for the training and to reduce the code.

With the _Lightning in 15 minutes_ (or any other tutorial) rewrite your code to use this framework.

**Note**: This will make the `dvclive` integration required below slightly easier. 
::::
:::


## How to save a `pytroch` model

Of course we can save a `pytorch` model with the methods discussed in @sec-data-mp but it is more convenient to use the dedicated functions, see [docs](https://pytorch.org/tutorials/beginner/saving_loading_models.html).

In short, we only need to call `torch.save(model.state_dict(), file)` to save in the default format using `pickle`, be careful when using it, see @sec-data-model-pickle.

```{.python}
torch.save("model.pt")
loaded_model = model = MyFirstNN(X_train.shape[1])
loaded_model.load_state_dict(torch.load("model.pt", weights_only=True))
```

It is also possible to implement continuous checkpoints during training, see [docs](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training).
This allows us to resume training after an interrupt or simply store the model at the end of the training. 


All of this can be done via the already discussed `dvclive` interface, the implementation is defined as an exercise (if your module uses `lightning` this is even easier). 

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-dvc-dvc}

## `dvclive` integration into the `pytorch` training

Implement the [`DVCLive`](https://dvc.org/doc/dvclive/ml-frameworks/pytorch) integration to track the metrics.
::::
:::

::: {.callout-tip}
## Store the model as `ONNX`

It is also possible to export the model in the ONNX format, see @sec-data-model-onnx and for specifics the [docs](https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html).
:::

[^1]: Install it via `pdm add lightning`.

## Backward Propagation of Error - Backpropagation

Our first NN is a success, but how did it learn the necessary parameters to perform its task? 

The answer is a technique called _Backward Propagation of Error_ or in short _Backpropagation_. 
This essential component of for machine learning helps us to work out how the loss we compute translates into changes of the weights in our network.
In our training loop `train_model` the lines
```{.python}
y_pred = model(X_batch)            # Forward pass through the model
loss = loss_fn(y_pred, y_batch)    # Compute the loss
loss.backward()                    # Backpropagation
optimizer.step()                   # Update the model parameters
optimizer.zero_grad()              # Reset the gradients
```
are what we are focusing on in this section.

::: {.callout-important}
A very nice and structured introduction by IBM can be found [here](https://www.ibm.com/think/topics/backpropagation#:~:text=Backpropagation%20ist%20die%20Abk%C3%BCrzung%20f%C3%BCr,die%20Genauigkeit%20von%20Modellvorhersagen%20auswirken.)[^ibm].

An introduction closer to the mathematics is shown in [@Brunton2022, Chapter 6.3].
:::

The introduction of the technique is contributed to the paper @Rumelhart1986.
As usual, predecessors and independent similar proposals go back to the 1960s.
As we have seen, our NN can mathematically be described by nested functions, that are called inside the loss function, $\mathscr{L}(\Theta)$, for $\Theta$ being all trainable parameters.

During the training we can now compute the _change_ between the NN output and the provided label and use it in a gradient method[^gradient].
Resulting in the iteration 
$$
\Theta^{(n+1)} = \Theta^{(n)} - \delta \nabla \mathscr{L}\left(\Theta^{(n)}\right),
$$
where $\delta$ is called the _learning rate_ in this context. 

To compute the derivative we can use the chain rule as this _propagates the error backwards_ through the network. For $h(x) = g(f(x))$ the derivative $h'(x)$ is computed as
$$
h'(x) = g'(f(x))f'(x) \quad \Leftrightarrow \quad
\frac{\partial\, h}{\partial\, x} (x) = \frac{\partial\, g}{\partial\, x}(f(x)) \cdot \frac{\partial\, f}{\partial\, x}(x),
$$
or in Leibniz notation for a variable $z$ that depends on $y$, which itself depends on $x$ we get
$$
\frac{\mathrm{d}\, z}{\mathrm{d}\, x} = \frac{\mathrm{d}\, z}{\mathrm{d}\, y} \cdot \frac{\mathrm{d}\, y}{\mathrm{d}\, x}.
$$

To get a better idea on how this works, let us look a the following simple example. 

::: {.callout-tip appearance="simple" collapse="true" icon=false}
:::: {#exm-nn-nn-backpropagaion}

## A simple case

To illustrate the procedure we start with the simplest example, illustrated in @fig-nn-nn-backprop_simple.

![One node, one layer model for illustration of the backpropagation algorithm.](../_assets/nn/backprop_simple){#fig-nn-nn-backprop_simple}

To get the output $y$ from $x$ the following computation is required
$$
y = g(z, b) = g(f(x, a), b).
$$

If we now assume a means square error for the final loss, we get an error, depending on the weights $a$ and $b$
$$
\mathscr{L} = \frac12 (y_0 - y)^2,
$$
for $y_0$ being the ground truth or correct result.
In order to minimize the error according to $a$ and $b$ we need to compute the derivatives

$$
\begin{align}
\frac{\partial\, \mathscr{L}}{\partial\, a} 
&= \frac{\partial\, \mathscr{L}}{\partial\, y} \cdot \frac{\partial\, y}{\partial\, a} &=& \big[y = g(z, b)\big]\\
&= \frac{\partial\, \mathscr{L}}{\partial\, y}\cdot \frac{\partial\, g}{\partial\, z} \cdot \frac{\partial\, z}{\partial\, a} &=& \big[z = f(x, a)\big]\\
&= \frac{\partial\, \mathscr{L}}{\partial\, y}\cdot \frac{\partial\, g}{\partial\, z} \cdot \frac{\partial\, f}{\partial\, a}
\end{align}
$$

$$
\begin{align}
\frac{\partial\, \mathscr{L}}{\partial\, b} 
&= \frac{\partial\, \mathscr{L}}{\partial\, y} \cdot \frac{\partial\, y}{\partial\, b} &=& \big[y = g(z,b)\big] \\
&= \frac{\partial\, \mathscr{L}}{\partial\, y}\cdot \frac{\partial\, g}{\partial\, b} \phantom{ \cdot \frac{\partial\, f}{\partial\, a}}
\end{align}
$$

For a particular $\mathscr{L}$, $g$, and $f$ we can compute it explicitly, e.g. $\mathscr{L}=\tfrac12(y_0 - y)^2$, $g(z,b) = b z$, and  $f(x,a) = \tanh(a x)$

$$
\begin{align}
\frac{\partial\, \mathscr{L}}{\partial\, a} &=  \frac{\partial\, \mathscr{L}}{\partial\, y} \frac{\partial\, g}{\partial\, z}\frac{\partial\, f}{\partial\, a} &=& - (y_0 - y) \cdot b \cdot x \cdot (1 - \tanh^2(a x)),\\
\frac{\partial\, \mathscr{L}}{\partial\, b} &=  \frac{\partial\, \mathscr{L}}{\partial\, y} \frac{\partial\, g}{\partial\, b} &=& - (y_0 - y) \cdot \tanh(a x).
\end{align}
$$

With this information we can define the gradient descent update
$$
\begin{align}
a^{(k+1)} &= a^{(k)} - \delta \frac{\partial\, \mathscr{L}}{\partial\, a} = a^{(k)} - \delta \left(- (y_0 - y) \cdot b^{(k)} \cdot (1 - \tanh^2(a^{(k)} x))\right), \\
b^{(k+1)} &= b^{(k)} - \delta \frac{\partial\, \mathscr{L}}{\partial\, b} = b^{(k)} - \delta \left( - (y_0 - y) \cdot \tanh(a^{(k)} x)\right).
\end{align}
$$
::::
:::

Now that we have a better understanding we can define the backdrop procedure, see @Brunton2022

1. Specify the NN along with the labeled training data.
1. Initialize the weights and biases of the NN with random values. If they are initialized with zero the gradient method will update all of them in the same fashion which is not what we are looking for.
1. In a loop until convergence or a maximum of iterations is achieved:
    i. Run the training data through the NN to compute $y$. Compute the according loss and its derivatives with respect to each weight and bias.
    i. For a given learning rate $\delta$ update the NN parameters via the gradient method.

We can see this reflected in our code for the NN above.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nn-nn-backpro√º}

## Our dogs and cats example.

Let us translate this findings to our example visualized in @fig-nn-cvd-nl.
In order to do so, we need to specify our variables in more detail.
To simplify it a bit we set the biases to the zero vector. 

First, we call our output $p = [p_1, p_2]$ and our labels are encoded in one-hot encoding $y = [y_1, y_2]$ where $y_1=1$ and $y_2=0$ if the image is a dog, and vice versa if the image is a cat.
To simplify it a bit more we do not use any bias.

As our loss function we use cross-entropy
$$
\mathscr{L}(\Theta) = - \frac12 \sum_{i=1}^2 y_i \log(p_i) = \frac{y_1 \log(p_1) + y_2 \log(p_2)}{2},
$$
for a single sample with the above notation.

In order to make the computation of the derivates easier we use the variables as described in @fig-nn-nn-cvd_comp.

![One node, one layer model for illustration of the backpropagation algorithm.](../_assets/nn/cats_vs_dogs_comp){#fig-nn-nn-cvd_comp}

Therefore we get
$p = g(z)=\sigma(B v)$ (softmax), and $v = f(u)= \tanh(A x)$.

Overall we want to compute the change in $B_{i, j}$ (for some fixed indices $i$, and $j$).
$$
\frac{\partial\, \mathscr{L}}{\partial \, B_{i, j}} = \frac{\partial\, \mathscr{L}}{\partial \, p} \cdot \frac{\partial\, p}{\partial \, z} \cdot \frac{\partial\, z}{\partial \, B_{i, j}}
$$

1. Compute $\partial_{p_i} \mathscr{L}$.
1. The computation of the Jacobian of $\sigma(z)$ is tricky but together with the cross-entropy loss it is straight forward, therefore compute $\partial_{z_i} \mathscr{L}(\sigma(z))$.
1. Compute $\partial_{B_{i, j}} z$.
1. Write down the components showing up for the chain rule for $\frac{\partial\, \mathscr{L}}{\partial \, A_{i, j}}$ (similar as above).
::::
:::

[^ibm]: Access on 4th April 2025.

[^gradient]: see @Kandolf_GDM, Section 6.1 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/regression/nonlinear.html#sec-regression-nonlinear-gd).