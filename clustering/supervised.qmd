# Supervised learning {#sec-clustering-sl}

If we recall @Fischer from the Iris data set, we can also find one of the first supervised learning methods in this paper.
The introduced _linear discriminant analysis_ (LDA) has survived over time and is still one of the standard techniques for classification, even though we use a more generalized and improved method nowadays.

## Linear Discriminant Analysis (LDA)
::: {.callout-important}
The following introduction and illustrational data set, as well as the basic structure of the code is from [@Brunton2022, Code 5.9].
Also see [GitHub](https://github.com/dynamicslab/databook_python).
:::

The idea if LDA is to find a linear combination of features that optimally separates two or more classes.
The crucial part in the algorithm is that it is guided by labelled data.
At its core, the algorithm aims to solve an optimization problem: find an optimal low-dimensional embedding of the data that shows a clear separation between their point distribution, or maximize the distance between the inter-class data and minimize the intra-class data distance.

::: {.callout-tip}
For supervised learning, it is always good practice to split up your data set into a _training_ and _testing_ section.
**It is important to have a test set that the algorithm has never seen!**
In general a $80:20$ split is common but other rations might be advisable, depending on the data set. 

It is also common practice to use $k$-folds cross-validation for the training set. 
:::

::: {.callout appearance="simple"}
:::: {#def-crossvalidation}

## $k$-folds cross-validation

The $k$-folds cross-validation technique is a method to allow better _hyperparameter tuning_, especially for smaller data sets where training and validation data is small.
The main idea is that you split your iterate over different validation sets by splitting up your training set.
Lets say we use $5$-fold cross-validation we split the training set into 5 parts.
Train with 4 parts and validate against the 5th.
We than rotate and select a different validation fold.
At the end we average over the 5 iterations to get our final parameters.

This looks something like this:

![Common split with the folds 1 to 4 for training and 5 for validation in the first iteration and folds 2 to 5 for training and 1 for validation in the last. The test set is not touched.](../_assets/clustering/kfoldcrossvalidation){#fig-clustering-sl-5foldcrossvalidation}

It is important that the test set is not included in the folds to make sure you test against observations that the algorithm has never seen!
::::
:::

The main idea of LDA is to use projection.
For a two-class LDA this becomes
$$
w = \operatorname{arg\, max}_w \frac{w^\mathrm{T}S_B w}{w^\mathrm{T}S_W w},
$$ {#eq-grayleigh}
(the generalized Rayleigh quotient) where $w$ is our thought after projection and the two scatter matrices
$$
S_B = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^\mathrm{T}
$$
for between-class relation as well as
$$
S_W = \sum_{j=1}^2 \sum_{x\in\mathcal{D}_j} (x - \mu_j)(x - \mu_j)^\mathrm{T}
$$
for within-class data.
The set $\mathcal{D}_j$ denotes the subdomain of the data associated with cluster $j$.
The two matrices measure the variance of the data set as well as the means.
To solve @eq-grayleigh we need to solve the generalized eigenvalue problem[^GEV]
$$
S_B w = \lambda S_w w
$$
where the maximal eigenvalue and the corresponding eigenvector are our solution.

We try this with the cats and dogs data set in both basis.

```{python}
#| label: fig-clustering-sl-lda
#| fig-cap: "Evaluation of the LDA for the second and fourth principal component on the test set of 40 animals. A bar on the in the top half corresponds to dogs and in the bottom half to cats. The first 20 individuals should be dogs, the second 20 cats. The red dotted line shows the split. True positive can be found in the top-left as well as the bottom-right."
#| fig-subcap: 
#|   - "Trained and evaluated against the raw data."
#|   - "Trained and evaluated against the data in wavelet basis."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

def analyse(CD):
    U, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)
    v = VT.T
    xtrain = np.concatenate((v[:60, [1, 3]], v[80:140, [1, 3]]))
    label = np.repeat(np.array([1, -1]), 60)
    test = np.concatenate((v[60:80, [1, 3]], v[140:160, [1, 3]]))
    lda = LinearDiscriminantAnalysis()
    lda.fit(xtrain, label)
    test_class = lda.predict(test)
    truth = np.repeat(np.array([1, -1]), 20)
    E = 100 * (1 - np.sum(np.abs(test_class - truth) / 2) / 40)
    plt.figure()
    plt.bar(range(40), test_class)
    plt.plot([-0.5, 39.5], [0, 0], "k", linewidth=1.0)
    plt.plot([19.5, 19.5], [-1.1, 1.1], "r-.", linewidth=3)
    plt.yticks([-0.5, 0.5], ["cats", "dogs"], rotation=90, va="center")
    plt.text(10, 1.05, "dogs")
    plt.text(30, 1.05, "cats")
    plt.gca().set_aspect(40 / (2 * 3))
    return (test_class, E, v)


response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData.mat")
cats = scipy.io.loadmat(io.BytesIO(response.content))["cat"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData.mat")
dogs = scipy.io.loadmat(io.BytesIO(response.content))["dog"]

test_class, E, v = analyse(np.concatenate((dogs, cats), axis=1))

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

test_class, E_w, v_w = analyse(np.concatenate((dogs_w, cats_w), axis=1))
plt.show()
```

If we use our raw data set for the classification we get an overall accuracy of `{python} float(np.round(E, 2))`% with $\tfrac{4}{20}$ wrongly labelled dogs and $\tfrac{9}{20}$ wrongly labelled cats.
We can increase this to an accuracy of `{python} float(np.round(E_w, 2))`% with $\tfrac{5}{20}$ wrongly labelled dogs and $\tfrac{2}{20}$ wrongly labelled cats.

This could be expected, see @fig-clustering-dvc-wavelet-pca_results_overview for the separation of the principal values for the two basis. 

Of course we have very limited data with only 80 images for each of the classes.
In this case we should do a cross-validation and we have not shuffled the data.

Let us see how selecting different test and training sets influence the behaviour.

```{python}
#| label: fig-clustering-sl-lda2
#| fig-cap: "Cross validation for the data set in wavelet basis, use 100 run with different training and test sets. We always use 120 images for training and 40 for testing."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

E = np.zeros(100)
for j in range(100):
    r1 = np.random.permutation(80)
    r2 = np.random.permutation(80) + 60
    ind1 = r1[:60]
    ind2 = r2[:60]
    ind1t = r1[60:80]
    ind2t = r2[60:80]
    
    xtrain = np.concatenate((v_w[ind1, :][:, [1, 3]], v_w[ind2, :][:, [1, 3]]))
    test = np.concatenate((v_w[ind1t, :][:, [1, 3]], v_w[ind2t, :][:, [1, 3]]))
    label = np.repeat(np.array([1, -1]), 60)

    lda = LinearDiscriminantAnalysis()
    test_class = lda.fit(xtrain, label).predict(test)

    truth = np.repeat(np.array([1, -1]),20)
    E[j] = 100 * (1 - np.sum(np.abs(test_class - truth) / 2) / 40)

plt.figure()
plt.bar(range(100), E)
plt.plot([0, 100], [E.mean(), E.mean()], "r-.", label="mean")
plt.plot([0, 100], [50, 50], "y-.", label='"coin toss"')
plt.xlim((-1, 100))
plt.ylim((45, 90))
plt.gca().set_aspect(100 / (45 * 3))
plt.ylabel("accuracy")
plt.xlabel("trial number")
plt.legend(loc="lower right")
plt.show()
```

With a maximal accuracy of `{python} float(E.max())`% and a minimal accuracy of `{python} float(E.min())`% our initial result with `{python} float(E_w)`% was quite good and above average (`{python} float(E.mean())`%).
We can also see that training the model is always better than just a simple _coin toss_ or random guessing for cat or dog.

Instead of a linear discriminants, we can also use quadratic discriminants.
To show the difference let us look at the classification line of the two methods for our data in wavelet basis

```{python}
#| label: fig-clustering-sl-lda3
#| fig-cap: 
#|    - "Classification line for the LDA together with actual instances."
#|    - "Classification line for the QDA together with actual instances."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.inspection import DecisionBoundaryDisplay
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

xtrain = np.concatenate((v_w[:60, [1, 3]], v_w[80:140, [1, 3]]))
label = np.repeat(np.array([1, -1]), 60)
test = np.concatenate((v_w[60:80, [1, 3]], v_w[140:160, [1, 3]]))

plt.figure()
plt.scatter(v_w[:80, 1], v_w[:80, 3], label="dogs")
plt.scatter(v_w[80:, 1], v_w[80:, 3], label="cats")

lda = LinearDiscriminantAnalysis().fit(xtrain, label)
K = -lda.intercept_[0]
L = -lda.coef_[0]
x = np.arange(-0.12, 0.25, 0.005)
plt.plot(x, -(L[0] * x + K) / L[1], "k", label="classification line")
plt.ylim([-0.25, 0.2])
plt.xlim([-0.15, 0.25])
plt.xlabel(r"$PC_2$")
plt.ylabel(r"$PC_4$")
plt.legend()

plt.figure()
plt.scatter(v_w[:80, 1], v_w[:80, 3], label="dogs")
plt.scatter(v_w[80:, 1], v_w[80:, 3], label="cats")

qda = QuadraticDiscriminantAnalysis().fit(xtrain, label)
DecisionBoundaryDisplay.from_estimator(
        qda,
        xtrain,
        grid_resolution=2000,
        ax=plt.gca(),
        response_method="predict",
        plot_method="contour",
        alpha=1.0,
        levels=[0],
    )
plt.ylim([-0.25, 0.2])
plt.xlim([-0.15, 0.25])
plt.xlabel(r"$PC_2$")
plt.ylabel(r"$PC_4$")
plt.legend(["dogs", "cats", "classification line"])
plt.show()
```

As we can see in @fig-clustering-sl-lda3, having a quadratic discriminant classification line can be rather beneficial, like always depending on the observations.
The QDA arises from LDA when we do not assume that the covariance of each of the classes is the same. 

::: {.callout-note}
LDA and QDA assume a normal distribution as the basis for each of the clusters.
This allows us to write it also not in the here presented geometric interpretation as projection but as a update procedure with Bayes[^1] theorem,
:::

::: {.callout-tip}
Where for the LDA it is possible to get the correct function for the classification line this is tricky for the QDA.
Luckily the `scikit-learn` class/function [`DecisionBoundaryDisplay.from_estimator`](https://scikit-learn.org/1.6/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html#sklearn.inspection.DecisionBoundaryDisplay.from_estimator) can help in such cases.
:::

[^GEV]: see [@Kandolf_GDM, Definition 3.5] or the direct link [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/matrixdc/eigen.html#generalized-eigenvalue-problem)

[^1]: see [@Kandolf_GDM, Theorem 14.4] or the direct link [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/statistics/bayesian.html#def-statistics-bayesianth)

## Measuring Performance {#sec-clustering-sl-performance}

As in most applications the question how good an algorithm performs is not easy to establish.
In @fig-clustering-sl-lda2 we said we are doing better than a coin toss but we should be able to characterize this more precise.

::: {.callout-important}
The following approach and the basic structure of the code is from @Geron2022-xh, see [GitHub](https://github.com/ageron/handson-ml3/blob/main/03_classification.ipynb).
:::

In order to illustrate basic properties found in machine learning we use the MNIST data set together with a binary classifier based on Stochastic Gradient Descent. 

::: {.callout-note}
Stochastic Gradient Descent (SGD)[^SGD] is an optimization algorithm.
The key idea is to replace the actual gradient by a stochastic approximation in the optimization of the loss function.
This allows especially good performance for large-scale learning and sparse machine learning problems.

We can use this training method for classification to find the optimal parameters for our loss function an in turn, this can be used as a binary classifier.

As SGD methods are prone to a sensitivity in feature scaling and order we need to make sure to use normalized data and we should shuffle.

In `scikit-learn` we can use the class [`SGDClassifier`](https://scikit-learn.org/stable/modules/sgd.html).
:::

First we load the MNIST data set again and split it into a training and testing section, see @sec-clustering-ul-me.

```{python}
#| code-fold: true
#| code-summary: "Show the code for loading and splitting the dataset"
import numpy as np
import pandas as pd
import sklearn
from sklearn.datasets import fetch_openml
from sklearn.linear_model import SGDClassifier
np.random.seed(6020)

mnist = fetch_openml('mnist_784', as_frame=False)

X_train, X_test = mnist.data[:60000], mnist.data[60000:]
y_train, y_test = mnist.target[:60000], mnist.target[60000:]
```

Next, as we only want a binary classifier, we select one number, in our case `5` and relabel our data. 
With the new labels we can train our classifier.

```{python}
#| output: false
y_train_5 = (y_train == "5")
y_test_5 = (y_test == "5")

SGD = SGDClassifier(random_state=6020)
SGD.fit(X_train, y_train_5)
```

In order to cet a score for our method we use $k$-folds cross-validation @def-crossvalidation and the corresponding `scikit-learn` function `cross_val_score` to perform this task for our model.

```{python}
#| output: false
scores = sklearn.model_selection.cross_val_score(
    SGD, X_train, y_train_5, cv=5, scoring="accuracy")
```
```{python}
#| echo: false
pd.DataFrame({"accuracy [%]": np.round(scores * 100, 2)}).T
```

With results in the high $90\%$ range the results look promession if not great but are they really that good.
In order to get a better idea just always guess that we do not see a `5` that should be the most common class in our case.
To simulate this we use the `DummyClassifier` class.

```{python}
#| output: false
dummy = sklearn.dummy.DummyClassifier()
dummy.fit(X_train, y_train_5)
scores_dummy = sklearn.model_selection.cross_val_score(
    dummy, X_train, y_train_5, cv=5, scoring="accuracy")
```
```{python}
#| echo: false
pd.DataFrame({"accuracy [%]": np.round(scores_dummy * 100, 2)}).T
```

As this is pretty much $91\%$ (as expected there are only about $10\%$ of `5`s in the data set).
Just using accuracy is apparently not the gold standard to measure performance, what other possibilities are there?

::: {.callout appearance="simple"}
:::: {#def-confusionmatrix}

## Confusion Matrix
The confusion matrix, error matrix or for unsupervised learning sometimes called matching matrix allows an easy way of visualizing the performance of an algorithm.

The rows represent the true observations in each class, and the columns the predicted observations for each class.

In our $2\times 2$ case we get

![Names and abbreviations for a $2\times 2$ confusion matrix together with an example form our test case.](../_assets/clustering/confusion_matrix){#fig-clustering-sl-confusionmatrix}

but it can be extended for multi-class segmentation.

::::
:::

To compute the confusion matrix we first need predictions.
This can be achieved by `cross_val_predict` instead of `cross_val_score` and than we use [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). 

Combined, for our example this reads as:
```{python}
#| output: false
y_train_pred = sklearn.model_selection.cross_val_predict(
                    SGD, X_train, y_train_5, cv=3)
cm = sklearn.metrics.confusion_matrix(y_train_5, y_train_pred)                   
```
```{python}
#| echo: false
df = pd.DataFrame(cm)
df.columns = ["PP", "PN"]
df["AC"] = ["P", "N"]
df.set_index("AC").rename_axis(None)
```

From the values in the confusion matrix a lot of metrics can be computed[^wikicm]:

1. **Accuracy**:
$$
ACC = \frac{TP + TN}{P + N}
$$

1. **True positive rate** (TPR) or **recall**:
$$
TPR = \frac{TP}{P}
$$

1. **False negative rate** (FNR):
$$
FNR = \frac{FN}{P}
$$

1. **False positive rate** (FPR):
$$
FPR = \frac{FP}{N}
$$

1. **True negative rate** (TNR):
$$
TNR = \frac{TN}{N}
$$

1. **Positive predictive value** (PPV) or **precission**:
$$
PPV = \frac{TP}{TP + FP}
$$

1. **False discovery rate** (FDR):
$$
FDR = \frac{FP}{TP + FP}
$$

1. **False omission rate** (FOR):
$$
FOR = \frac{FN}{TN + FN}
$$

1. **Negative predictive value** (NPV):
$$
NPV = \frac{TN}{TN + FN}
$$

1. **$F_1$ score**:
$$
F_1 = \frac{2 TP}{2 TP + FP + FN}
$$

In the [`sklearn.metrics`](https://scikit-learn.org/stable/api/sklearn.metrics.html) most of these values have a corresponding function. 
If we look at _precission_, _recall_, and the $F_1$ score, for our example we see that our performance is viewed under a different light:

```{python}
#| classes: styled-output
precision = sklearn.metrics.precision_score(y_train_5, y_train_pred)
recall = sklearn.metrics.recall_score(y_train_5, y_train_pred)
f1_score = sklearn.metrics.f1_score(y_train_5, y_train_pred)
```

This tells us that our classifier correctly classifies a `5` `{python} float(np.round(precision * 100, 2))`% of the time.
On the other hand it only _recalls_ or detects `{python} float(np.round(recall * 100, 2))`% of our `5`s.
The $F_1$-score is a combination of the two (the harmonic mean) and in our case `{python} float(np.round(f1_score * 100, 2))`%.

Depending on the application we might want to have high precision (e.g. medical diagnosis to have no unnecessary treatment) or high recall (e.g. fraud detection where a missed fraudulent transaction can be costly).
If we increase precision we reduce recall and the other way round so we can hardly have both.
This dilemma is called _precision/recall trade-off_, see @sec-appendix-pvsr for some more explanations.

An alternative way to look at accuracy for binary classifiers is to look at the _receiver operating characteristic_ (ROC).
It looks at recall (TPR) vs. the _false positive rate_ (FPR).
Other than that it works similar.

[^SGD]: see [@Kandolf_GDM, Section 6.2] or the direct link [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/regression/nonlinear.html#stochastic-gradient-descent)

[^wikicm]: see Wikipedia overview [Link](https://en.wikipedia.org/wiki/Confusion_matrix)

## Support Vector Machines (SVM) {#sec-clustering-sv-svm}

The basic idea of Support Vector Machines (SVM) is to split observations into distinct clusters via hyperplanes.
The have a long history in data science and come in different forms and fashions.
Over the years they became more flexible and are still one of the most used tools in industry and science. 

### Linear SVM
We start of with the linear SVM where we construct a hyperplane
$$
\langle w, x\rangle + b = 0
$$
with a vector $w$ and a constant $b$.
There is a natural degree of freedom in this selection of the hyperplane, see @fig-clustering-sl-svm for two different choices.

::: {#fig-clustering-sl-svm}

![Hyperplane with small margin.](../_assets/clustering/SVM_hyperplain1){#fig-clustering-sl-svm-1}

![Hyperplane with large margin.](../_assets/clustering/SVM_hyperplain2){#fig-clustering-sl-svm-2}

We see the hyperplane for the SVM classification scheme. The margin is much larger in the second choice.
:::

The optimization inside the SVM aims to find the line that separates the classes best (fewest wrong classifications) and also keeps the largest margin between the observations (the yellow region).
The vectors touching the edge of the yellow regions are called _support vectors_ giving the name to the algorithm.

With the hyperplane it is easy to classify an observation by simply computing the sign of the projection, i.e.
$$
y_j (\langle w, x_j \rangle + b) = \operatorname{sign}(\langle w, x_j \rangle + b) = \begin{cases} +1\\-1\end{cases},
$$
where $1$ corresponds to the versicolor (orange) and $-1$ setosa (blue) observations in @fig-clustering-sl-svm.
Therefore, the classifier depends on the position of the observation and is not invariant under scaling.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-linear-SVM}

## Linear SVM

1. Compute the vector $w$ in the two cases of @fig-clustering-sl-svm.
The vector $w$ is normal to the line. 
For @fig-clustering-sl-svm-1 two points on the line are $v_1 = [1.25, 4.1]^\mathrm{T}$, $v_2 = [5, 7.4]^\mathrm{T}$. For @fig-clustering-sl-svm-2 two points on the line are $z_1 = [2.6, 4.25]^\mathrm{T}$, $z_2 = [2, 7]^\mathrm{T}$. 

1. Classify the two points 
$$
a = [1.4, 5.1]^\mathrm{T},
$$
$$
b = [4.7, 7.0]^\mathrm{T}.
$$
::::
:::

Stating the optimization function such that it is smooth for a linear SVM is a bit tricky.
This on the other hand is needed to allow for most optimization algorithm to work, as they require a gradient to some sort. 

Therefore, the following formulation is quite common.
$$
\underset{w, b}{\operatorname{argmin}} \sum_j H(y_j, \overline{y}_j) + \frac12\|w\|^2 \quad \text{subject to}\quad \min_j|\langle x_j, w\rangle| = 1,
$$
with $H(y_j, \overline{y}_j) = \max(0, 1 - \langle y_j, \overline{y}_j\rangle)$, the so called _Hinge loss_ function for counting the number of errors. 
Furthermore, $\overline{y}_j = \operatorname{sign}(\langle w, x_j\rangle + b)$.

### Nonlinear SVM

In order to extend the SVM to more complex classification curves the feature space of the SVM can be extended.
In order to do so, SVM introduces nonlinear features and computes the hyperplane on these features via a mapping $x \to \Phi(x)$ and the hyperplane function becomes
$$
f(x) = \langle w, \Phi(x)\rangle + b
$$
and accordingly we classify along
$$
\operatorname{sign}(\langle w, \Phi(x_j)\rangle + b) = \operatorname{sign}(f(x_j)).
$$

Essentially, we change the feature space such that a separation is (hopefully) easier.
To illustrate this we use a simple one dimensional example as illustrated in @fig-clustering-sl-svm-nl-1.
Clearly there is no _linear_ separation possible.
On the other hand, if we use 
$$
\Phi(x_j) = (x_j, x_j^2) 
$$
as our transformation function we move to 2D space and the problem can easily be solved by a line at $y=0.25$.

```{python}
#| label: fig-clustering-sl-svm-nl
#| fig-cap: "Nonlinear classification with SVM."
#| fig-subcap: 
#|   - "Observations that can not be separated linearly."
#|   - "Enriched feature set with Φ(x)=(x, x^2)."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

x = np.linspace(-1, 1, 11, endpoint=True)
x2 = np.zeros_like(x)
y = np.zeros_like(x)
y[np.abs(x) < 0.5] = 1

plt.figure()
plt.scatter(x[y==0], x2[y==0], label="class 1")
plt.scatter(x[y==1], x2[y==1], label="class 2")
plt.ylim([-0.1, 1])
plt.xlim([-1, 1])
plt.gca().set_aspect(2/3.3)
plt.legend()

x2 = np.power(x, 2)

plt.figure()
data = np.stack([x.flatten(), x2.flatten()]).T
svm = LinearSVC(random_state=6020).fit(data, y.flatten())
plt.scatter(x[y==0], x2[y==0], label="class 1")
plt.scatter(x[y==1], x2[y==1], label="class 2")
plt.ylim([-0.1, 1])
plt.xlim([-1, 1])
w = svm.coef_[0]
d = svm.intercept_[0]
line = lambda x: -w[0] / w[1] * x - d / w[1]
plt.plot([-1, 1], [line(-1), line(1)], "k", label="classification line")
plt.plot([-1, 1], [0.25, 0.25], "k:", label="actual boundary line")
plt.gca().set_aspect(2/3.3)
plt.legend()

plt.show()
```

As can be seen in @fig-clustering-sl-svm-nl-2 the SVM does a great job in finding a split for the two classes, even though not select the _optimal_ line, which is not surprising for the given amount of observations. 

As mentioned before, SVMs are sensitive to scaling.
Let us use this example to illustrate the difference together with the concept of _pipelines_ often used in data science context.

::: {.callout-note}
## Pipeline
The main idea of a pipeline is to create a composite as a ordered chain of transformations and estimators, see [docs](https://scikit-learn.org/stable/modules/compose.html) for insights.
:::

We can use the pipeline to
- create the polynomial observations
- apply a scaler to our observations
- apply the Linear SVM

```{python}
composit_svm = sklearn.pipeline.make_pipeline(
    sklearn.preprocessing.PolynomialFeatures(2),
    sklearn.preprocessing.StandardScaler(),
    LinearSVC(random_state=6020)
)
```

```{python}
#| label: fig-clustering-sl-svm-nl-3
#| fig-cap: "Classification with autoscaler vs. no scaler."
#| fig-subcap: 
#|   - "Classification in the enriched Φ(x)=(x^0, x^1, x^2) and scaled space. Note the first dimension is ignored."
#|   - "Difference between the classification lines when transformed back into the original space."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

x = np.linspace(-1, 1, 11, endpoint=True).reshape(-1, 1)
y = np.zeros_like(x).flatten()
y[np.abs(x.flatten()) < 0.5] = 1

composit_svm.fit(x, y)

xx = composit_svm[:2].fit_transform(x)

plt.figure()
plt.scatter(xx[y==0, 1], xx[y==0, 2], label="class 1")
plt.scatter(xx[y==1, 1], xx[y==1, 2], label="class 2")

w = composit_svm['linearsvc'].coef_[0][1:]
d = composit_svm['linearsvc'].intercept_[0]
plt.plot([-1.6, 1.6], [line(-1.5), line(1.5)],
         "k", label="scaled classification line")

plt.ylim([-1.25, 1.75])
plt.xlim([-1.6, 1.6])
plt.gca().set_aspect(3/9)
plt.legend()

xx = composit_svm[:1].fit_transform(x)
plt.figure()
plt.scatter(xx[y==0, 1], xx[y==0, 2], label="class 1")
plt.scatter(xx[y==1, 1], xx[y==1, 2], label="class 2")

w = composit_svm['linearsvc'].coef_[0][1:]
d = composit_svm['linearsvc'].intercept_[0]
a = composit_svm["standardscaler"].inverse_transform(
    [np.array([0, -1.6, line(-1.6)])])[0]
b = composit_svm["standardscaler"].inverse_transform(
    [np.array([0, 1.6, line(1.6)])])[0]
plt.plot([a[1], b[1]], [a[2], b[2]], 
         "k", label="scaled classification line")

w = svm.coef_[0]
d = svm.intercept_[0]
plt.plot([-1, 1], [line(-1), line(1)], 
            "k:", label="unscaled classification line")

plt.ylim([-0.1, 1])
plt.xlim([-1, 1])
plt.gca().set_aspect(2/3.3)
plt.legend()
plt.show()
```

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-SVM}

## Nonlinear SVM

1. Extend the above findings to an example in 2D with a circular classification line.
Create tests data of your classification by changing to `np.linspace(-1, 1, 12)`.
```{python}
#| label: fig-clustering-sl-svm-nl-exr
#| fig-cap: "Data set in 2D"
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.svm import LinearSVC
import sklearn
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

np.random.seed(6020)
x = np.linspace(-1, 1, 11, endpoint=True)
x2 = np.linspace(-1, 1, 11, endpoint=True)
XX, XX2 = np.meshgrid(x, x2)
ZZ = np.pow(XX, 2) + np.pow(XX2, 2)
y = np.zeros_like(XX)

i = np.sqrt(ZZ) < 0.5
y[i] = 1
plt.scatter(XX[y == 0], XX2[y == 0], label="class 1")
plt.scatter(XX[y == 1], XX2[y == 1], label="class 2")

data = np.stack([XX.flatten(), XX2.flatten(), ZZ.flatten()]).T
label = y.flatten()
```

2. Recall the moons example from @sec-clustering-ul-dbscan and use a degree $3$ `PolynomialFeatures` for classification.

In both cases, plot the classification line in a projection onto the original 2D space.
::::
:::

### Kernel Methods for SVM

While enriching the feature space is, without doubt, extremal helpful the curse of dimensionality is quickly starting to influence the performance.
The computation of $w$ is getting harder.
The so called _kernel trick_ is solving this problem.
We express $w$ in a different basis and solve for the parameters of the basis, i.e.
$$
w = \sum_{j=1}^m \alpha_j \Phi(x_j)
$$
where $\alpha_j$ are called the weights of the different nonlinear observable functions $\Phi(x_j)$.
Our $f$ becomes
$$
f(x) = \sum_{j=1}^m \alpha_j \langle \Phi(x_j), \Phi(x) \rangle + b.
$$
The so called _kernel function_ is defined as the inner product involved, i.e.
$$
K(x_j, x) = \langle \Phi(x_j), \Phi(x) \rangle.
$$
The optimization problem for $w$ no reads as
$$
\underset{\alpha, b}{\operatorname{argmin}} \sum_j H(y_j, \overline{y}_j) + \frac12\left\|\sum_{j=1} \alpha_j \Phi(x_j)\right\|^2 \quad \text{subject to}\quad \min_j|\langle x_j, w\rangle| = 1,
$$
with $\alpha$ representing the vector of all the $\alpha_j$. 
The important part here is that we now minimize of $\alpha$, which is easier.

The kernel function allow almost arbitrary number of observables as it, for example, can represent a Taylor series expansion.
Furthermore, it allows an implicit computation in higher dimensions by simply computing the inner product of differences between observations. 

One of these functions are so called _radial basis functions_ (RBF) with the simplest being a Gaussian kernel
$$
K(x_j, x) = \exp\left(-\gamma\|x_j - x\|^2\right).
$$

In `scikit-learn` this is supported via the [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) class.

Let us test this implementation with the help of our dogs and cats example.

```{python}
#| label: fig-clustering-sl-svm-rbf
#| fig-cap: "Training a SVM with an RBF kernel for the singular vectors 2 to 22. The picture shows the classification results projected for the principal components 2 and 4."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn import svm
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

CD = np.concatenate((dogs_w, cats_w), axis=1)
U, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)
v = VT.T

features = np.arange(1, 21)
xtrain = np.concatenate((v[:60, features], v[80:140, features]))
label = np.repeat(np.array([1, -1]), 60)
xtest = np.concatenate((v[60:80, features], v[140:160, features]))
truth = np.repeat(np.array([1, -1]), 20)

svc = svm.SVC(kernel="rbf", gamma="auto").fit(xtrain, label)
test_label = svc.predict(xtest)
train_label = svc.predict(xtrain)
cm = sklearn.metrics.confusion_matrix(test_label, truth)
plt.figure()
plt.scatter(xtrain[train_label == 1, 1], xtrain[train_label == 1, 3],
            alpha=0.5, color="C0", label="train_dogs")
plt.scatter(xtrain[train_label == -1, 1], xtrain[train_label == -1, 3],
            alpha=0.5, color="C1", label="train_cats")
plt.scatter(xtest[test_label == 1, 1], xtest[test_label == 1, 3],
            color="C0", label="test_dogs")
plt.scatter(xtest[test_label == -1, 1], xtest[test_label == -1, 3],
            color="C1", label="test_cats")
error = np.vstack((xtrain[label != train_label, :][:, [1,3]],
                   xtest[truth != test_label, :][:, [1, 3]]))
plt.scatter(error[:, 0], error[:, 1],
            color="k", marker="x", label="wrong classification")
plt.legend()
plt.ylim([-0.25, 0.2])
plt.xlim([-0.15, 0.25])
plt.xlabel(r"$PC_2$")
plt.ylabel(r"$PC_4$")
plt.show()
```
We get a confusion matrix for our test set as
```{python}
#| echo: false
df = pd.DataFrame(cm)
df.columns = ["PP", "PN"]
df["AC"] = ["P", "N"]
df.set_index("AC").rename_axis(None)
```
In @fig-clustering-sl-svm-rbf we can see the results of the classification for the entire set of observations, shaded for the training set, and crosses marking the wrongly classified data.
With `{python} error.shape[0]` wrongly classified images we have quite a good result, compared to LDA or QDA @fig-clustering-sl-lda3.
Note, the classification is hard to recognise for the two classes in the simple projection.
With the parameters `C` and `gamma` we can influence the classification.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-SVM-rbf}

## Nonlinear SVM with RBF

Recall the moons example from @sec-clustering-ul-dbscan and use a `SVC` classification to distinguish the clusters.
Look at four different results for $\gamma \in \{0.1, 5\}$ and $C \in \{0.001, 1000\}$, compare @Geron2022-xh.

In all of the four images plot the classification line in a projection onto the original 2D space.
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-SVM-regression}

## Nonlinear SVM for regression

We can use SVM for regression.
Have a look at [docs](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py) and use the findings to fit the following observations with various degrees and kernel functions.

```{python}
import matplotlib.pyplot as plt
import numpy as np
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

m = 100
x = 6 * np.random.rand(m) - 3
y = 1/2 * x ** 2 + x + 2 + np.random.randn(m)
fig = plt.figure()
plt.scatter(x, y, label="observations")
plt.show()
```

Compare [@Kandolf_GDM, Example 5.2] [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/regression/linear.html#exm-regression-linear-poly).
::::
:::

## Decision trees

Decision trees are widely used in data science for classification and regression.
The are a powerful class of algorithms that can fit not only numerical data.
Furthermore, the form the basis of _random forests_, one of the most powerful machine learning algorithms available to date.

The where not invented for machine learning but have been a staple in business for centuries.
The basic idea is that they establish an algorithmic flow chart for making decisions.
The criteria that creates the splits in each branch is related to a desired outcome and are therefore _important_.
Often experts are called upon creating such a decision tree.

The decision tree learning follows the same principals to create a predictive classification model based on the provided observations and labels.
Similar to DBSCAN they form a hierarchical structure that tries to split in an optimal way.
In this regard they are the counterpart to DBSCAN but they move from top to bottom and of course use the labels to guide the process. 

The following key feature make them wildly use:

1. The usually produce interpretable results (we can draw the graph)
1. The algorithm mimics human decision making, which helps for the interpretation
1. The can handle numerical and categorical data
1. They perform well with large sets of data
1. The reliability of the classification can be assessed with statistical validation

While there are a lot of different optimizations the base algorithm follows these steps:

1. Look through all components (features) of an observation $x_j$ that gives the best labeling prediction $y_j$
1. Compare the prediction accuracy over all observations, the best result is used
1. Proceed with the two new branches in the same fashion

Let us apply it to the Fischer Iris data set to better understand what is happening.

```{python}
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import sklearn
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from six import StringIO
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

iris = load_iris(as_frame=True)
X_iris = iris.data.values
y_iris = iris.target

decision_tree = DecisionTreeClassifier(max_depth=2, random_state=6020)
decision_tree.fit(X_iris, y_iris)

dot_data = "fischer_tree.dot"

sklearn.tree.export_graphviz(decision_tree, out_file=dot_data,  
                filled=True, rounded=True,
                feature_names=iris.feature_names,
                class_names=iris.target_names,
                special_characters=True)
```
```{dot}
//| label: fig-clustering-sl-tree-iris
//| fig-cap: "Decision tree for the Fischer iris data set and depth 2."
//|file: fischer_tree.dot
```
::: {.callout-important}

## Displaying `dot` files

In the code for @fig-clustering-sl-tree-iris we generate a `dot` file, that is interpreted with quarto to show the graph for a better integration.
In order to do this offline you need to install [`graphviz`](https://www.graphviz.org/download/) for the installation of `dot` and also install the python package `pydotplus`.

Than you should be able to use:
```{.python}
import pydotplus
from six import StringIO

dot_data = StringIO()
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.create_png()
```
:::

As we can see, for our tree with depth $2$ we only need to split along `petal length (cm)` and `petal width (cm)`, leaving the two other features untouched, compare @fig-clustering-iris. 

As we only have the splits happening in these two variables we can also visualize them easy.

```{python}
#| label: fig-clustering-sl-tree-iris-split
#| fig-cap: "Splits for the Fischer Iris data set with the first two split form the above tree and the third split would be the next step for a larger tree."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

iris.frame["target"] = iris.target_names[iris.target]
df = iris.frame
plt.figure()
for name in iris.target_names:
    index = df["target"] == name
    plt.scatter(df.iloc[:, 2][index], df.iloc[:, 3][index], label=name)

plt.xlim([0.5, 7.5])
plt.ylim([0, 2.6])
th = decision_tree.tree_.threshold[[0, 2, 3, 4]]
plt.plot([th[0], th[0]], plt.gca().get_ylim(), 'C1-.',
          linewidth=2, label="split 1")
plt.plot([th[0], plt.gca().get_xlim()[1]], [th[1], th[1]], 'C2:',
          linewidth=2, label="split 2")
plt.plot([4.95, 4.95], [0, th[1]], 'C3--',
          linewidth=2, label="(split 3)")
plt.xlabel(iris.feature_names[2])
plt.ylabel(iris.feature_names[3])
plt.legend()
plt.show()
```
In @fig-clustering-sl-tree-iris-split we can see the the first two splits and the next split if we would increase the tree.
With the first split, we immediately separate _setosa_ with $100\%$ accuracy.
The two other classes are a bit tricky and we can not classify everything correct right away.
In total $6$ out of $150$ observations are wrongly classified with this simple tree.

Let us also apply the tree classification to our cats and dogs example.

```{python}
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from six import StringIO
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

CD = np.concatenate((dogs_w, cats_w), axis=1)
U, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)
v = VT.T

features = np.arange(1, 21)
xtrain = np.concatenate((v[:60, features], v[80:140, features]))
label = np.repeat(np.array([1, -1]), 60)
xtest = np.concatenate((v[60:80, features], v[140:160, features]))
truth = np.repeat(np.array([1, -1]), 20)

decision_tree_cvd = DecisionTreeClassifier(max_depth=2).fit(xtrain, label)
test_label = decision_tree_cvd.predict(xtest)
cm = sklearn.metrics.confusion_matrix(test_label, truth)

test_label = svc.predict(xtest)
cm = sklearn.metrics.confusion_matrix(test_label, truth)

dot_data = "cvsd_tree.dot"

sklearn.tree.export_graphviz(decision_tree_cvd,
                out_file=dot_data,  
                filled=True, rounded=True,
                class_names=["dog", "cat"],
                special_characters=True)

score = decision_tree_cvd.score(xtest, truth)
```
```{dot}
//| label: fig-clustering-sl-tree-cvsd
//| fig-cap: "Decision tree for the Fischer iris data set and depth 2."
//| file: cvsd_tree.dot
```
```{python}
#| echo: false
df = pd.DataFrame(cm)
df.columns = ["PP", "PN"]
df["AC"] = ["P", "N"]
df.set_index("AC").rename_axis(None)
```
If we compare our confusion matrix for the test cases to the one of SVM we get comparable results.
In general, we can see that the first split is along $PC_2$ (note that we do not use the $PC_1$ in the code and therefore $x_0=PC_2$) and our second split is along $PC_4$.
We used these components before for our classification.
The third split is along $PC_5$ which we did not consider before hand. 
Overall the mean accuracy for out test set is `{python} float(np.round(score * 100, 2))`%.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-tree-moons}

## A tree on the moon

Recall the moons example from @sec-clustering-ul-dbscan and use a `DecisionTreeClassifier` classification to distinguish the clusters and plot the decision splits. 

Play around with the parameters, e.g. `min_samples_leaf = 5` and see how this influences the score for a test set.
::::
:::