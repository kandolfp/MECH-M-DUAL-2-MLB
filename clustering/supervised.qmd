# Supervised learning {#sec-clustering-sl}

If we recall @Fischer from the Iris data set, we can also find one of the first supervised learning methods in this paper.
The introduced _linear discriminant analysis_ (LDA) has survived over time and is still one of the standard techniques for classification, even though we use a more generalized and improved method nowadays.

## Linear Discriminant Analysis (LDA) {#sec-clustering-sl-lda}
::: {.callout-important}
The following introduction and illustrational data set, as well as the basic structure of the code is from [@Brunton2022, Code 5.9].
Also see [GitHub](https://github.com/dynamicslab/databook_python).
:::

The idea if LDA is to find a linear combination of features that optimally separates two or more classes.
The crucial part in the algorithm is that it is guided by labelled data.
At its core, the algorithm aims to solve an optimization problem: find an optimal low-dimensional embedding of the data that shows a clear separation between their point distribution, or maximize the distance between the inter-class data and minimize the intra-class data distance.

::: {.callout-tip}
For supervised learning, it is always good practice to split up our data set into a _training_ and _testing_ section.
**It is important to have a test set that the algorithm has never seen!**
In general a $80:20$ split is common but other rations might be advisable, depending on the data set. 

It is also common practice to use $k$-folds cross-validation for the training set. 
:::

::: {.callout appearance="simple"}
:::: {#def-crossvalidation}

## $k$-folds cross-validation

The $k$-folds cross-validation technique is a method to allow better _hyperparameter tuning_, especially for smaller data sets where training and validation data is small.
The main idea is that we split our iterate over different validation sets by splitting up our training set.
Lets say we use $5$-fold cross-validation we split the training set into 5 parts.
Train with 4 parts and validate against the 5th.
We than rotate and select a different validation fold.
At the end we average over the 5 iterations to get our final parameters.

This looks something like this:

![Common split with the folds 1 to 4 for training and 5 for validation in the first iteration and folds 2 to 5 for training and 1 for validation in the last. The test set is not touched.](../_assets/clustering/kfoldcrossvalidation){#fig-clustering-sl-5foldcrossvalidation}

It is important that the test set is not included in the folds to make sure we test against observations that the algorithm has never seen!
::::
:::

The main idea of LDA is to use projection.
For a two-class LDA this becomes
$$
w = \operatorname{arg\, max}_w \frac{w^\mathrm{T}S_B w}{w^\mathrm{T}S_W w},
$$ {#eq-grayleigh}
(the generalized Rayleigh quotient) where $w$ is our thought after projection and the two scatter matrices
$$
S_B = (\mu_2 - \mu_1)(\mu_2 - \mu_1)^\mathrm{T}
$$
for between-class relation as well as
$$
S_W = \sum_{j=1}^2 \sum_{x\in\mathcal{D}_j} (x - \mu_j)(x - \mu_j)^\mathrm{T}
$$
for within-class data.
The set $\mathcal{D}_j$ denotes the subdomain of the data associated with cluster $j$.
The two matrices measure the variance of the data set as well as the means.
To solve @eq-grayleigh we need to solve the generalized eigenvalue problem[^GEV]
$$
S_B w = \lambda S_w w
$$
where the maximal eigenvalue and the corresponding eigenvector are our solution.

We try this with the cats and dogs data set in both basis.

```{python}
#| label: fig-clustering-sl-lda
#| fig-cap: "Evaluation of the LDA for the second and fourth principal component on the test set of 40 animals. A bar on the in the top half corresponds to dogs and in the bottom half to cats. The first 20 individuals should be dogs, the second 20 cats. The red dotted line shows the split. True positive can be found in the top-left as well as the bottom-right."
#| fig-subcap: 
#|   - "Trained and evaluated against the raw data."
#|   - "Trained and evaluated against the data in wavelet basis."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

def analyse(CD):
    U, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)
    v = VT.T
    xtrain = np.concatenate((v[:60, [1, 3]], v[80:140, [1, 3]]))
    label = np.repeat(np.array([1, -1]), 60)
    test = np.concatenate((v[60:80, [1, 3]], v[140:160, [1, 3]]))
    lda = LinearDiscriminantAnalysis()
    lda.fit(xtrain, label)
    test_class = lda.predict(test)
    truth = np.repeat(np.array([1, -1]), 20)
    E = 100 * (1 - np.sum(np.abs(test_class - truth) / 2) / 40)
    plt.figure()
    plt.bar(range(40), test_class)
    plt.plot([-0.5, 39.5], [0, 0], "k", linewidth=1.0)
    plt.plot([19.5, 19.5], [-1.1, 1.1], "r-.", linewidth=3)
    plt.yticks([-0.5, 0.5], ["cats", "dogs"], rotation=90, va="center")
    plt.text(10, 1.05, "dogs")
    plt.text(30, 1.05, "cats")
    plt.gca().set_aspect(40 / (2 * 3))
    return (test_class, E, v)


response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData.mat")
cats = scipy.io.loadmat(io.BytesIO(response.content))["cat"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData.mat")
dogs = scipy.io.loadmat(io.BytesIO(response.content))["dog"]

test_class, E, v = analyse(np.concatenate((dogs, cats), axis=1))

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

test_class, E_w, v_w = analyse(np.concatenate((dogs_w, cats_w), axis=1))
plt.show()
```

If we use our raw data set for the classification we get an overall accuracy of `{python} float(np.round(E, 2))`% with $\tfrac{4}{20}$ wrongly labelled dogs and $\tfrac{9}{20}$ wrongly labelled cats.
We can increase this to an accuracy of `{python} float(np.round(E_w, 2))`% with $\tfrac{5}{20}$ wrongly labelled dogs and $\tfrac{2}{20}$ wrongly labelled cats.

This could be expected, see @fig-clustering-dvc-wavelet-pca_results_overview for the separation of the principal values for the two basis. 

Of course we have very limited data with only 80 images for each of the classes.
In this case we should do a cross-validation and we have not shuffled the data.

Let us see how selecting different test and training sets influence the behaviour.

```{python}
#| label: fig-clustering-sl-lda2
#| fig-cap: "Cross validation for the data set in wavelet basis, use 100 run with different training and test sets. We always use 120 images for training and 40 for testing."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

E = np.zeros(100)
for j in range(100):
    r1 = np.random.permutation(80)
    r2 = np.random.permutation(80) + 60
    ind1 = r1[:60]
    ind2 = r2[:60]
    ind1t = r1[60:80]
    ind2t = r2[60:80]
    
    xtrain = np.concatenate((v_w[ind1, :][:, [1, 3]], v_w[ind2, :][:, [1, 3]]))
    test = np.concatenate((v_w[ind1t, :][:, [1, 3]], v_w[ind2t, :][:, [1, 3]]))
    label = np.repeat(np.array([1, -1]), 60)

    lda = LinearDiscriminantAnalysis()
    test_class = lda.fit(xtrain, label).predict(test)

    truth = np.repeat(np.array([1, -1]),20)
    E[j] = 100 * (1 - np.sum(np.abs(test_class - truth) / 2) / 40)

plt.figure()
plt.bar(range(100), E)
plt.plot([0, 100], [E.mean(), E.mean()], "r-.", label="mean")
plt.plot([0, 100], [50, 50], "y-.", label='"coin toss"')
plt.xlim((-1, 100))
plt.ylim((45, 90))
plt.gca().set_aspect(100 / (45 * 3))
plt.ylabel("accuracy")
plt.xlabel("trial number")
plt.legend(loc="lower right")
plt.show()
```

With a maximal accuracy of `{python} float(E.max())`% and a minimal accuracy of `{python} float(E.min())`% our initial result with `{python} float(E_w)`% was quite good and above average (`{python} float(E.mean())`%).
We can also see that training the model is always better than just a simple _coin toss_ or random guessing for cat or dog.

Instead of a linear discriminants, we can also use quadratic discriminants.
To show the difference let us look at the classification line of the two methods for our data in wavelet basis

```{python}
#| label: fig-clustering-sl-lda3
#| fig-cap: 
#|    - "Classification line for the LDA together with actual instances."
#|    - "Classification line for the QDA together with actual instances."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.inspection import DecisionBoundaryDisplay
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

xtrain = np.concatenate((v_w[:60, [1, 3]], v_w[80:140, [1, 3]]))
label = np.repeat(np.array([1, -1]), 60)
test = np.concatenate((v_w[60:80, [1, 3]], v_w[140:160, [1, 3]]))

plt.figure()
plt.scatter(v_w[:80, 1], v_w[:80, 3], label="dogs")
plt.scatter(v_w[80:, 1], v_w[80:, 3], label="cats")

lda = LinearDiscriminantAnalysis().fit(xtrain, label)
K = -lda.intercept_[0]
L = -lda.coef_[0]
x = np.arange(-0.12, 0.25, 0.005)
plt.plot(x, -(L[0] * x + K) / L[1], "k", label="classification line")
plt.ylim([-0.25, 0.2])
plt.xlim([-0.15, 0.25])
plt.xlabel(r"$PC_2$")
plt.ylabel(r"$PC_4$")
plt.legend()

plt.figure()
plt.scatter(v_w[:80, 1], v_w[:80, 3], label="dogs")
plt.scatter(v_w[80:, 1], v_w[80:, 3], label="cats")

qda = QuadraticDiscriminantAnalysis().fit(xtrain, label)
DecisionBoundaryDisplay.from_estimator(
        qda,
        xtrain,
        grid_resolution=2000,
        ax=plt.gca(),
        response_method="predict",
        plot_method="contour",
        alpha=1.0,
        levels=[0],
    )
plt.ylim([-0.25, 0.2])
plt.xlim([-0.15, 0.25])
plt.xlabel(r"$PC_2$")
plt.ylabel(r"$PC_4$")
plt.legend(["dogs", "cats", "classification line"])
plt.show()
```

As we can see in @fig-clustering-sl-lda3, having a quadratic discriminant classification line can be rather beneficial, like always depending on the observations.
The QDA arises from LDA when we do not assume that the covariance of each of the classes is the same. 

::: {.callout-note}
LDA and QDA assume a normal distribution as the basis for each of the clusters.
This allows us to write it also not in the here presented geometric interpretation as projection but as a update procedure with Bayes[^1] theorem,
:::

::: {.callout-tip}
Where for the LDA it is possible to get the correct function for the classification line this is tricky for the QDA.
Luckily the `scikit-learn` class/function [`DecisionBoundaryDisplay.from_estimator`](https://scikit-learn.org/1.6/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html#sklearn.inspection.DecisionBoundaryDisplay.from_estimator) can help in such cases.
:::


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-lda}

## Application of LDA

1. Apply the LDA algorithm to the toy example (see @fig-clustering-unsupervised_lloyd) to recover the two clusters as good as possible.

1. Additionally, for a higher dimensional problem, using LDA split the Iris data set (@@sec-fischer-iris-data-set) into two clusters. Try for the harder split between `versicolor` and `virginica` classes of flowers.
::::
:::

[^GEV]: see [@Kandolf_GDM, Definition 3.5] or the direct link [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/matrixdc/eigen.html#generalized-eigenvalue-problem)

[^1]: see [@Kandolf_GDM, Theorem 14.4] or the direct link [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/statistics/bayesian.html#def-statistics-bayesianth)

## Measuring Performance {#sec-clustering-sl-performance}

As in most applications the question how good an algorithm performs is not easy to establish.
In @fig-clustering-sl-lda2 we said we are doing better than a coin toss but we should be able to characterize this more precise.

::: {.callout-important}
The following approach and the basic structure of the code is from @Geron2022-xh, see [GitHub](https://github.com/ageron/handson-ml3/blob/main/03_classification.ipynb).
:::

In order to illustrate basic properties found in machine learning we use the MNIST data set together with a binary classifier based on Stochastic Gradient Descent, see @sec-clustering-ssl for more on the MNIST data set. 

::: {.callout-note}
Stochastic Gradient Descent (SGD)[^SGD] is an optimization algorithm.
The key idea is to replace the actual gradient by a stochastic approximation in the optimization of the loss function.
This allows especially good performance for large-scale learning and sparse machine learning problems.

We can use this training method for classification to find the optimal parameters for our loss function an in turn, this can be used as a binary classifier.

As SGD methods are prone to a sensitivity in feature scaling and order we need to make sure to use normalized data and we should shuffle.

In `scikit-learn` we can use the class [`SGDClassifier`](https://scikit-learn.org/stable/modules/sgd.html).
:::

First we load the MNIST data set again and split it into a training and testing section, see @sec-clustering-ul-me.

```{python}
#| code-fold: true
#| code-summary: "Show the code for loading and splitting the dataset"
import numpy as np
import pandas as pd
import sklearn
from sklearn.datasets import fetch_openml
from sklearn.linear_model import SGDClassifier
np.random.seed(6020)

mnist = fetch_openml('mnist_784', as_frame=False)

X_train, X_test = mnist.data[:60000], mnist.data[60000:]
y_train, y_test = mnist.target[:60000], mnist.target[60000:]
```

Next, as we only want a binary classifier, we select one number, in our case `5` and relabel our data. 
With the new labels we can train our classifier.

```{python}
#| output: false
y_train_5 = (y_train == "5")
y_test_5 = (y_test == "5")

SGD = SGDClassifier(random_state=6020)
SGD.fit(X_train, y_train_5)
```

In order to cet a score for our method we use $k$-folds cross-validation @def-crossvalidation and the corresponding `scikit-learn` function `cross_val_score` to perform this task for our model.

```{python}
#| output: false
scores = sklearn.model_selection.cross_val_score(
    SGD, X_train, y_train_5, cv=5, scoring="accuracy")
```
```{python}
#| echo: false
pd.DataFrame({"accuracy [%]": np.round(scores * 100, 2)}).T
```

With results in the high $90\%$ range the results look promession if not great but are they really that good.
In order to get a better idea just always guess that we do not see a `5` that should be the most common class in our case.
To simulate this we use the `DummyClassifier` class.

```{python}
#| output: false
dummy = sklearn.dummy.DummyClassifier()
dummy.fit(X_train, y_train_5)
scores_dummy = sklearn.model_selection.cross_val_score(
    dummy, X_train, y_train_5, cv=5, scoring="accuracy")
```
```{python}
#| echo: false
pd.DataFrame({"accuracy [%]": np.round(scores_dummy * 100, 2)}).T
```

As this is pretty much $91\%$ (as expected there are only about $10\%$ of `5`s in the data set).
Just using accuracy is apparently not the gold standard to measure performance, what other possibilities are there?

::: {.callout appearance="simple"}
:::: {#def-confusionmatrix}

## Confusion Matrix
The confusion matrix, error matrix or for unsupervised learning sometimes called matching matrix allows an easy way of visualizing the performance of an algorithm.

The rows represent the true observations in each class, and the columns the predicted observations for each class.

In our $2\times 2$ case we get

![Names and abbreviations for a $2\times 2$ confusion matrix together with an example form our test case.](../_assets/clustering/confusion_matrix){#fig-clustering-sl-confusionmatrix}

but it can be extended for multi-class segmentation.

::::
:::

To compute the confusion matrix we first need predictions.
This can be achieved by `cross_val_predict` instead of `cross_val_score` and than we use [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). 

Combined, for our example this reads as:
```{python}
#| output: false
y_train_pred = sklearn.model_selection.cross_val_predict(
                    SGD, X_train, y_train_5, cv=3)
cm = sklearn.metrics.confusion_matrix(y_train_5, y_train_pred)                   
```
```{python}
#| echo: false
df = pd.DataFrame(cm)
df.columns = ["PP", "PN"]
df["AC"] = ["P", "N"]
df.set_index("AC").rename_axis(None)
```

From the values in the confusion matrix a lot of metrics can be computed[^wikicm]:

1. **Accuracy**:
$$
ACC = \frac{TP + TN}{P + N}
$$

1. **True positive rate** (TPR) or **recall**:
$$
TPR = \frac{TP}{P}
$$

1. **False negative rate** (FNR):
$$
FNR = \frac{FN}{P}
$$

1. **False positive rate** (FPR):
$$
FPR = \frac{FP}{N}
$$

1. **True negative rate** (TNR):
$$
TNR = \frac{TN}{N}
$$

1. **Positive predictive value** (PPV) or **precission**:
$$
PPV = \frac{TP}{TP + FP}
$$

1. **False discovery rate** (FDR):
$$
FDR = \frac{FP}{TP + FP}
$$

1. **False omission rate** (FOR):
$$
FOR = \frac{FN}{TN + FN}
$$

1. **Negative predictive value** (NPV):
$$
NPV = \frac{TN}{TN + FN}
$$

1. **$F_1$ score**:
$$
F_1 = \frac{2 TP}{2 TP + FP + FN}
$$

In the [`sklearn.metrics`](https://scikit-learn.org/stable/api/sklearn.metrics.html) most of these values have a corresponding function. 
If we look at _precission_, _recall_, and the $F_1$ score, for our example we see that our performance is viewed under a different light:

```{python}
#| classes: styled-output
precision = sklearn.metrics.precision_score(y_train_5, y_train_pred)
recall = sklearn.metrics.recall_score(y_train_5, y_train_pred)
f1_score = sklearn.metrics.f1_score(y_train_5, y_train_pred)
```

This tells us that our classifier correctly classifies a `5` `{python} float(np.round(precision * 100, 2))`% of the time.
On the other hand it only _recalls_ or detects `{python} float(np.round(recall * 100, 2))`% of our `5`s.
The $F_1$-score is a combination of the two (the harmonic mean) and in our case `{python} float(np.round(f1_score * 100, 2))`%.

Depending on the application we might want to have high precision (e.g. medical diagnosis to have no unnecessary treatment) or high recall (e.g. fraud detection where a missed fraudulent transaction can be costly).
If we increase precision we reduce recall and the other way round so we can hardly have both.
This dilemma is called _precision/recall trade-off_, see @sec-appendix-pvsr for some more explanations.

An alternative way to look at accuracy for binary classifiers is to look at the _receiver operating characteristic_ (ROC).
It looks at recall (TPR) vs. the _false positive rate_ (FPR).
Other than that it works similar.

[^SGD]: see [@Kandolf_GDM, Section 6.2] or the direct link [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/regression/nonlinear.html#stochastic-gradient-descent)

[^wikicm]: see Wikipedia overview [Link](https://en.wikipedia.org/wiki/Confusion_matrix)

## Support Vector Machine (SVM) {#sec-clustering-sv-svm}

The basic idea of Support Vector Machines (SVMs) is to split observations into distinct clusters via hyperplanes.
The have a long history in data science and come in different forms and fashions.
Over the years they became more flexible and are still one of the most used tools in industry and science. 

### Linear SVM
We start of with the linear SVM where we construct a hyperplane
$$
\langle w, x\rangle + b = 0
$$
with a vector $w$ and a constant $b$.
There is a natural degree of freedom in this selection of the hyperplane, see @fig-clustering-sl-svm for two different choices.

::: {#fig-clustering-sl-svm}

![Hyperplane with small margin.](../_assets/clustering/SVM_hyperplain1){#fig-clustering-sl-svm-1}

![Hyperplane with large margin.](../_assets/clustering/SVM_hyperplain2){#fig-clustering-sl-svm-2}

We see the hyperplane for the SVM classification scheme. The margin is much larger in the second choice.
:::

The optimization inside the SVM aims to find the line that separates the classes best (fewest wrong classifications) and also keeps the largest margin between the observations (the yellow region).
The vectors touching the edge of the yellow regions are called _support vectors_ giving the name to the algorithm.

With the hyperplane it is easy to classify an observation by simply computing the sign of the projection, i.e.
$$
y_j (\langle w, x_j \rangle + b) = \operatorname{sign}(\langle w, x_j \rangle + b) = \begin{cases} +1\\-1\end{cases},
$$
where $1$ corresponds to the versicolor (orange) and $-1$ setosa (blue) observations in @fig-clustering-sl-svm.
Therefore, the classifier depends on the position of the observation and is not invariant under scaling.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-linear-SVM}

## Linear SVM

1. Compute the vector $w$ in the two cases of @fig-clustering-sl-svm.
The vector $w$ is normal to the line. 
For @fig-clustering-sl-svm-1 two points on the line are $v_1 = [1.25, 4.1]^\mathrm{T}$, $v_2 = [5, 7.4]^\mathrm{T}$. For @fig-clustering-sl-svm-2 two points on the line are $z_1 = [2.6, 4.25]^\mathrm{T}$, $z_2 = [2, 7]^\mathrm{T}$. 

1. Classify the two points 
$$
a = [1.4, 5.1]^\mathrm{T},
$$
$$
b = [4.7, 7.0]^\mathrm{T}.
$$
::::
:::

Stating the optimization function such that it is smooth for a linear SVM is a bit tricky.
This on the other hand is needed to allow for most optimization algorithm to work, as they require a gradient to some sort. 

Therefore, the following formulation is quite common.
$$
\underset{w, b}{\operatorname{argmin}} \sum_j H(y_j, \overline{y}_j) + \frac12\|w\|^2 \quad \text{subject to}\quad \min_j|\langle x_j, w\rangle| = 1,
$$
with $H(y_j, \overline{y}_j) = \max(0, 1 - \langle y_j, \overline{y}_j\rangle)$, the so called _Hinge loss_ function for counting the number of errors. 
Furthermore, $\overline{y}_j = \operatorname{sign}(\langle w, x_j\rangle + b)$.

### Nonlinear SVM

In order to extend the SVM to more complex classification curves the feature space of the SVM can be extended.
In order to do so, SVM introduces nonlinear features and computes the hyperplane on these features via a mapping $x \to \Phi(x)$ and the hyperplane function becomes
$$
f(x) = \langle w, \Phi(x)\rangle + b
$$
and accordingly we classify along
$$
\operatorname{sign}(\langle w, \Phi(x_j)\rangle + b) = \operatorname{sign}(f(x_j)).
$$

Essentially, we change the feature space such that a separation is (hopefully) easier.
To illustrate this we use a simple one dimensional example as illustrated in @fig-clustering-sl-svm-nl-1.
Clearly there is no _linear_ separation possible.
On the other hand, if we use 
$$
\Phi(x_j) = (x_j, x_j^2) 
$$
as our transformation function we move to 2D space and the problem can easily be solved by a line at $y=0.25$.

```{python}
#| label: fig-clustering-sl-svm-nl
#| fig-cap: "Nonlinear classification with SVM."
#| fig-subcap: 
#|   - "Observations that can not be separated linearly."
#|   - "Enriched feature set with Φ(x)=(x, x^2)."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

x = np.linspace(-1, 1, 11, endpoint=True)
x2 = np.zeros_like(x)
y = np.zeros_like(x)
y[np.abs(x) < 0.5] = 1

plt.figure()
plt.scatter(x[y==0], x2[y==0], label="class 1")
plt.scatter(x[y==1], x2[y==1], label="class 2")
plt.ylim([-0.1, 1])
plt.xlim([-1, 1])
plt.gca().set_aspect(2/3.3)
plt.legend()

x2 = np.power(x, 2)

plt.figure()
data = np.stack([x.flatten(), x2.flatten()]).T
svm = LinearSVC(random_state=6020).fit(data, y.flatten())
plt.scatter(x[y==0], x2[y==0], label="class 1")
plt.scatter(x[y==1], x2[y==1], label="class 2")
plt.ylim([-0.1, 1])
plt.xlim([-1, 1])
w = svm.coef_[0]
d = svm.intercept_[0]
line = lambda x: -w[0] / w[1] * x - d / w[1]
plt.plot([-1, 1], [line(-1), line(1)], "k", label="classification line")
plt.plot([-1, 1], [0.25, 0.25], "k:", label="actual boundary line")
plt.gca().set_aspect(2/3.3)
plt.legend()

plt.show()
```

As can be seen in @fig-clustering-sl-svm-nl-2 the SVM does a great job in finding a split for the two classes, even though not select the _optimal_ line, which is not surprising for the given amount of observations. 

As mentioned before, SVMs are sensitive to scaling.
Let us use this example to illustrate the difference together with the concept of _pipelines_ often used in data science context.

::: {.callout-note}
## Pipeline
The main idea of a pipeline is to create a composite as a ordered chain of transformations and estimators, see [docs](https://scikit-learn.org/stable/modules/compose.html) for insights.
:::

We can use the pipeline to
- create the polynomial observations
- apply a scaler to our observations
- apply the Linear SVM

```{python}
composit_svm = sklearn.pipeline.make_pipeline(
    sklearn.preprocessing.PolynomialFeatures(2),
    sklearn.preprocessing.StandardScaler(),
    LinearSVC(random_state=6020)
)
```

```{python}
#| label: fig-clustering-sl-svm-nl-3
#| fig-cap: "Classification with autoscaler vs. no scaler."
#| fig-subcap: 
#|   - "Classification in the enriched Φ(x)=(x^0, x^1, x^2) and scaled space. Note the first dimension is ignored."
#|   - "Difference between the classification lines when transformed back into the original space."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

x = np.linspace(-1, 1, 11, endpoint=True).reshape(-1, 1)
y = np.zeros_like(x).flatten()
y[np.abs(x.flatten()) < 0.5] = 1

composit_svm.fit(x, y)

xx = composit_svm[:2].fit_transform(x)

plt.figure()
plt.scatter(xx[y==0, 1], xx[y==0, 2], label="class 1")
plt.scatter(xx[y==1, 1], xx[y==1, 2], label="class 2")

w = composit_svm['linearsvc'].coef_[0][1:]
d = composit_svm['linearsvc'].intercept_[0]
plt.plot([-1.6, 1.6], [line(-1.5), line(1.5)],
         "k", label="scaled classification line")

plt.ylim([-1.25, 1.75])
plt.xlim([-1.6, 1.6])
plt.gca().set_aspect(3/9)
plt.legend()

xx = composit_svm[:1].fit_transform(x)
plt.figure()
plt.scatter(xx[y==0, 1], xx[y==0, 2], label="class 1")
plt.scatter(xx[y==1, 1], xx[y==1, 2], label="class 2")

w = composit_svm['linearsvc'].coef_[0][1:]
d = composit_svm['linearsvc'].intercept_[0]
a = composit_svm["standardscaler"].inverse_transform(
    [np.array([0, -1.6, line(-1.6)])])[0]
b = composit_svm["standardscaler"].inverse_transform(
    [np.array([0, 1.6, line(1.6)])])[0]
plt.plot([a[1], b[1]], [a[2], b[2]], 
         "k", label="scaled classification line")

w = svm.coef_[0]
d = svm.intercept_[0]
plt.plot([-1, 1], [line(-1), line(1)], 
            "k:", label="unscaled classification line")

plt.ylim([-0.1, 1])
plt.xlim([-1, 1])
plt.gca().set_aspect(2/3.3)
plt.legend()
plt.show()
```

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-SVM}

## Nonlinear SVM

1. Extend the above findings to an example in 2D with a circular classification line.
Create tests data of your classification by changing to `np.linspace(-1, 1, 12)`.
```{python}
#| label: fig-clustering-sl-svm-nl-exr
#| fig-cap: "Data set in 2D"
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.svm import LinearSVC
import sklearn
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

np.random.seed(6020)
x = np.linspace(-1, 1, 11, endpoint=True)
x2 = np.linspace(-1, 1, 11, endpoint=True)
XX, XX2 = np.meshgrid(x, x2)
ZZ = np.pow(XX, 2) + np.pow(XX2, 2)
y = np.zeros_like(XX)

i = np.sqrt(ZZ) < 0.5
y[i] = 1
plt.scatter(XX[y == 0], XX2[y == 0], label="class 1")
plt.scatter(XX[y == 1], XX2[y == 1], label="class 2")

data = np.stack([XX.flatten(), XX2.flatten(), ZZ.flatten()]).T
label = y.flatten()
```

2. Recall the moons example from @sec-clustering-ul-dbscan and use a degree $3$ `PolynomialFeatures` for classification.

In both cases, plot the classification line in a projection onto the original 2D space.
::::
:::

### Kernel Methods for SVM

While enriching the feature space is, without doubt, extremal helpful the curse of dimensionality is quickly starting to influence the performance.
The computation of $w$ is getting harder.
The so called _kernel trick_ is solving this problem.
We express $w$ in a different basis and solve for the parameters of the basis, i.e.
$$
w = \sum_{j=1}^m \alpha_j \Phi(x_j)
$$
where $\alpha_j$ are called the weights of the different nonlinear observable functions $\Phi(x_j)$.
Our $f$ becomes
$$
f(x) = \sum_{j=1}^m \alpha_j \langle \Phi(x_j), \Phi(x) \rangle + b.
$$
The so called _kernel function_ is defined as the inner product involved, i.e.
$$
K(x_j, x) = \langle \Phi(x_j), \Phi(x) \rangle.
$$
The optimization problem for $w$ no reads as
$$
\underset{\alpha, b}{\operatorname{argmin}} \sum_j H(y_j, \overline{y}_j) + \frac12\left\|\sum_{j=1} \alpha_j \Phi(x_j)\right\|^2 \quad \text{subject to}\quad \min_j|\langle x_j, w\rangle| = 1,
$$
with $\alpha$ representing the vector of all the $\alpha_j$. 
The important part here is that we now minimize of $\alpha$, which is easier.

The kernel function allow almost arbitrary number of observables as it, for example, can represent a Taylor series expansion.
Furthermore, it allows an implicit computation in higher dimensions by simply computing the inner product of differences between observations. 

One of these functions are so called _radial basis functions_ (RBF) with the simplest being a Gaussian kernel
$$
K(x_j, x) = \exp\left(-\gamma\|x_j - x\|^2\right).
$$

In `scikit-learn` this is supported via the [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) class.

Let us test this implementation with the help of our dogs and cats example.

```{python}
#| label: fig-clustering-sl-svm-rbf
#| fig-cap: "Training a SVM with an RBF kernel for the singular vectors 2 to 22. The picture shows the classification results projected for the principal components 2 and 4."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn import svm
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

CD = np.concatenate((dogs_w, cats_w), axis=1)
U, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)
v = VT.T

features = np.arange(1, 21)
xtrain = np.concatenate((v[:60, features], v[80:140, features]))
label = np.repeat(np.array([1, -1]), 60)
xtest = np.concatenate((v[60:80, features], v[140:160, features]))
truth = np.repeat(np.array([1, -1]), 20)

svc = svm.SVC(kernel="rbf", gamma="auto").fit(xtrain, label)
test_label = svc.predict(xtest)
train_label = svc.predict(xtrain)
cm = sklearn.metrics.confusion_matrix(test_label, truth)
plt.figure()
plt.scatter(xtrain[train_label == 1, 1], xtrain[train_label == 1, 3],
            alpha=0.5, color="C0", label="train_dogs")
plt.scatter(xtrain[train_label == -1, 1], xtrain[train_label == -1, 3],
            alpha=0.5, color="C1", label="train_cats")
plt.scatter(xtest[test_label == 1, 1], xtest[test_label == 1, 3],
            color="C0", label="test_dogs")
plt.scatter(xtest[test_label == -1, 1], xtest[test_label == -1, 3],
            color="C1", label="test_cats")
error = np.vstack((xtrain[label != train_label, :][:, [1,3]],
                   xtest[truth != test_label, :][:, [1, 3]]))
plt.scatter(error[:, 0], error[:, 1],
            color="k", marker="x", label="wrong classification")
plt.legend()
plt.ylim([-0.25, 0.2])
plt.xlim([-0.15, 0.25])
plt.xlabel(r"$PC_2$")
plt.ylabel(r"$PC_4$")
plt.show()
```
We get a confusion matrix for our test set as
```{python}
#| echo: false
df = pd.DataFrame(cm)
df.columns = ["PP", "PN"]
df["AC"] = ["P", "N"]
df.set_index("AC").rename_axis(None)
```
In @fig-clustering-sl-svm-rbf we can see the results of the classification for the entire set of observations, shaded for the training set, and crosses marking the wrongly classified data.
With `{python} error.shape[0]` wrongly classified images we have quite a good result, compared to LDA or QDA @fig-clustering-sl-lda3.
Note, the classification is hard to recognise for the two classes in the simple projection.
With the parameters `C` and `gamma` we can influence the classification.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-SVM-rbf}

## Nonlinear SVM with RBF

Recall the moons example from @sec-clustering-ul-dbscan and use a `SVC` classification to distinguish the clusters.
Look at four different results for $\gamma \in \{0.1, 5\}$ and $C \in \{0.001, 1000\}$, compare @Geron2022-xh.

In all of the four images plot the classification line in a projection onto the original 2D space.
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-SVM-regression}

## Nonlinear SVM for regression

We can use SVM for regression.
Have a look at [docs](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py) and use the findings to fit the following observations with various degrees and kernel functions.

```{python}
import matplotlib.pyplot as plt
import numpy as np
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

m = 100
x = 6 * np.random.rand(m) - 3
y = 1/2 * x ** 2 + x + 2 + np.random.randn(m)
fig = plt.figure()
plt.scatter(x, y, label="observations")
plt.show()
```

Compare [@Kandolf_GDM, Example 5.2] [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/regression/linear.html#exm-regression-linear-poly).
::::
:::

## Decision trees

Decision trees are widely used in data science for classification and regression.
The are a powerful class of algorithms that can fit not only numerical data.
Furthermore, the form the basis of _random forests_, one of the most powerful machine learning algorithms available to date.

The where not invented for machine learning but have been a staple in business for centuries.
The basic idea is that they establish an algorithmic flow chart for making decisions.
The criteria that creates the splits in each branch is related to a desired outcome and are therefore _important_.
Often experts are called upon creating such a decision tree.

The decision tree learning follows the same principals to create a predictive classification model based on the provided observations and labels.
Similar to DBSCAN they form a hierarchical structure that tries to split in an optimal way.
In this regard they are the counterpart to DBSCAN but they move from top to bottom and of course use the labels to guide the process. 

The following key feature make them wildly use:

1. The usually produce interpretable results (we can draw the graph)
1. The algorithm mimics human decision making, which helps for the interpretation
1. The can handle numerical and categorical data
1. They perform well with large sets of data
1. The reliability of the classification can be assessed with statistical validation

While there are a lot of different optimizations the base algorithm follows these steps:

1. Look through all components (features) of an observation $x_j$ that gives the best labeling prediction $y_j$
1. Compare the prediction accuracy over all observations, the best result is used
1. Proceed with the two new branches in the same fashion

Let us apply it to the Fischer Iris data set to better understand what is happening.

```{python}
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import sklearn
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from six import StringIO
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

iris = load_iris(as_frame=True)
X_iris = iris.data.values
y_iris = iris.target

decision_tree = DecisionTreeClassifier(max_depth=2, random_state=6020)
decision_tree.fit(X_iris, y_iris)

dot_data = "fischer_tree.dot"

sklearn.tree.export_graphviz(decision_tree, out_file=dot_data,  
                filled=True, rounded=True,
                feature_names=iris.feature_names,
                class_names=iris.target_names,
                special_characters=True)
```
```{dot}
//| label: fig-clustering-sl-tree-iris
//| fig-cap: "Decision tree for the Fischer iris data set and depth 2."
//|file: fischer_tree.dot
```
::: {.callout-important}

## Displaying `dot` files

In the code for @fig-clustering-sl-tree-iris we generate a `dot` file, that is interpreted with quarto to show the graph for a better integration.
In order to do this offline you need to install [`graphviz`](https://www.graphviz.org/download/) for the installation of `dot` and also install the python package `pydotplus`.

Than you should be able to use:
```{.python}
import pydotplus
from six import StringIO

dot_data = StringIO()
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.create_png()
```
:::

As we can see, for our tree with depth $2$ we only need to split along `petal length (cm)` and `petal width (cm)`, leaving the two other features untouched, compare @fig-clustering-iris. 

As we only have the splits happening in these two variables we can also visualize them easy.

```{python}
#| label: fig-clustering-sl-tree-iris-split
#| fig-cap: "Splits for the Fischer Iris data set with the first two split form the above tree and the third split would be the next step for a larger tree."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

iris.frame["target"] = iris.target_names[iris.target]
df = iris.frame
plt.figure()
for name in iris.target_names:
    index = df["target"] == name
    plt.scatter(df.iloc[:, 2][index], df.iloc[:, 3][index], label=name)

plt.xlim([0.5, 7.5])
plt.ylim([0, 2.6])
th = decision_tree.tree_.threshold[[0, 2, 3, 4]]
plt.plot([th[0], th[0]], plt.gca().get_ylim(), 'C1-.',
          linewidth=2, label="split 1")
plt.plot([th[0], plt.gca().get_xlim()[1]], [th[1], th[1]], 'C2:',
          linewidth=2, label="split 2")
plt.plot([4.95, 4.95], [0, th[1]], 'C3--',
          linewidth=2, label="(split 3)")
plt.xlabel(iris.feature_names[2])
plt.ylabel(iris.feature_names[3])
plt.legend()
plt.show()
```
In @fig-clustering-sl-tree-iris-split we can see the the first two splits and the next split if we would increase the tree.
With the first split, we immediately separate _setosa_ with $100\%$ accuracy.
The two other classes are a bit tricky and we can not classify everything correct right away.
In total $6$ out of $150$ observations are wrongly classified with this simple tree.

Let us also apply the tree classification to our cats and dogs example.

```{python}
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from six import StringIO
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

CD = np.concatenate((dogs_w, cats_w), axis=1)
U, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)
v = VT.T

features = np.arange(1, 21)
xtrain = np.concatenate((v[:60, features], v[80:140, features]))
label = np.repeat(np.array([1, -1]), 60)
xtest = np.concatenate((v[60:80, features], v[140:160, features]))
truth = np.repeat(np.array([1, -1]), 20)

decision_tree_cvd = DecisionTreeClassifier(max_depth=2).fit(xtrain, label)
test_label = decision_tree_cvd.predict(xtest)
cm = sklearn.metrics.confusion_matrix(test_label, truth)

test_label = svc.predict(xtest)
cm = sklearn.metrics.confusion_matrix(test_label, truth)

dot_data = "cvsd_tree.dot"

sklearn.tree.export_graphviz(decision_tree_cvd,
                out_file=dot_data,  
                filled=True, rounded=True,
                class_names=["dog", "cat"],
                special_characters=True)

score = decision_tree_cvd.score(xtest, truth)
```
```{dot}
//| label: fig-clustering-sl-tree-cvsd
//| fig-cap: "Decision tree for the Fischer iris data set and depth 2."
//| file: cvsd_tree.dot
```
```{python}
#| echo: false
df = pd.DataFrame(cm)
df.columns = ["PP", "PN"]
df["AC"] = ["P", "N"]
df.set_index("AC").rename_axis(None)
```
If we compare our confusion matrix for the test cases to the one of SVM we get comparable results.
In general, we can see that the first split is along $PC_2$ (note that we do not use the $PC_1$ in the code and therefore $x_0=PC_2$) and our second split is along $PC_4$.
We used these components before for our classification.
The third split is along $PC_5$ which we did not consider before hand. 
Overall the mean accuracy for out test set is `{python} float(np.round(score * 100, 2))`%.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-tree-moons}

## A tree on the moon

Recall the moons example from @sec-clustering-ul-dbscan and use a `DecisionTreeClassifier` classification to distinguish the clusters and plot the decision splits. 

Play around with the parameters, e.g. `min_samples_leaf = 5` and see how this influences the score for a test set.
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-tree-regression}

## Tree regression

We can use a decision tree for regression.
Have a look at [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and use the findings to fit the observations of @exr-nonlinear-SVM-regression with various `max_depth` values and no value here but limiting `min_samples_leaf=10`.

::::
:::

::: {.callout-note}
## Sensitivity to rotation and initial state

Due to the nature of decision trees and the way they split observations by lines, they become sensitive to rotation.
Furthermore, the tree construction is based on a random process as the feature for the split is chosen at random.

To illustrate this we use a slight adaptation of the [@Geron2022-xh, Figure 6-7].

```{python}
#| label: fig-clustering-sl-tree-rot
#| fig-cap: "Illustration of the sensitivity to rotation of decision trees. Note both trees split perfectly."
#| fig-subcap: 
#|   - "Clear split along the middle for a vertical split."
#|   - "More complicated structure for the rotated observation."
#|   - "Different initial random state for the rotated observations."
#|   - "Correction of the rotated observations via PCA and scaling, resulting in an easy split."
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

def plot_tree_bound(tree, X, y):
    plt.figure()
    plt.scatter(X[y==0, 0], X[y==0, 1])
    plt.scatter(X[y==1, 0], X[y==1, 1])
    DecisionBoundaryDisplay.from_estimator(
            tree,
            X,
            grid_resolution=2000,
            ax=plt.gca(),
            response_method="predict",
            plot_method="contour",
            alpha=1.0,
            levels=[0],
        )
    l = np.round(np.abs(X).max() + 0.1, 2)
    plt.xlim([-l, l])
    plt.ylim([-l, l])


X = np.random.rand(100, 2) - 0.5
y = (X[:, 0] > 0).astype(np.int32)

angle = np.pi / 4
rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],
                            [np.sin(angle), np.cos(angle)]])
X_rot = X @ rotation_matrix

tree_square = DecisionTreeClassifier(random_state=42).fit(X, y)
tree_square_rot = DecisionTreeClassifier(random_state=42).fit(X_rot, y)
tree_square_rot2 = DecisionTreeClassifier(random_state=6020).fit(X_rot, y)

pipeline = make_pipeline(StandardScaler(), PCA())
X_pca = pipeline.fit_transform(X_rot)
tree_square_pca = DecisionTreeClassifier(random_state=6020).fit(X_pca, y)

plot_tree_bound(tree_square, X, y)
plot_tree_bound(tree_square_rot, X_rot, y)
plot_tree_bound(tree_square_rot2, X_rot, y)
plot_tree_bound(tree_square_pca, X_pca, y)
plt.show()
```
:::

As we could see, trees are very sensitive to the observations and the state.
In order to counteract this phenomenon we can compute multiple trees and average the results.
This combinations of trees is called _ensemble_ and leads to the next topic.

## Ensemble learning and random forests

The notion of _wisdom of the crowd_ suggests, that the decision of multiple people averaged, results in a _better_ decision/judgment than the decisions of a single person.
Of course there is a lot of statistics going in, if we want to compute exactly how the result is influenced, or if we should use the same weights for each classification, or e.g. the notion of _experts_, and much more. 

Nevertheless, we can use this concept in our context to create so called _ensemble methods_, the process itself is called _ensemble learning_.
With this concept we can combine already good classification methods and get a better result than the best classification method included.

If we train a multitude of decision trees on various (random) subsets of our observations we can combine the predictions of the individual trees to an ensemble prediction. 
The resulting method is called a _random forest_.
This very simple process allows us to generate very powerful classification methods. 

There are some different approaches for the combination of such methods, we only discuss them briefly, see [@Geron2022-xh, Chapter 7] for a more detailed discussion.

::: {.callout-important}
All of the below discussed methods and approaches can be found in the [`sklearn.ensemble`](https://scikit-learn.org/stable/api/sklearn.ensemble.html) module.
:::

::: {.callout appearance="simple"}
:::: {#def-ensemble-voting}

## Voting classifiers

If we have a set of classifiers $C=\{c_1, \ldots, c_n\}$ of various kinds (even another ensemble classifier like a random forest is welcome), we can simple make a prediction with each resulting in $r_1, \ldots, r_n$.
Now we select the class witch occurs most often, i.e. the _mode_ of the predictions, we get a new classifier.
This is called _hard voting_.

If all our classifiers can not only produce a prediction but a probability for our prediction we can also create a _soft voting_ classifier. 
All we need to do is, average the probability of the predictions $p_1, \ldots, p_n$, and this will give us new probabilities for our ensemble classifier.

![Illustration of the difference between hard and soft voting for a ensemble method. For the three shown classifiers the class 0 is the most common. When moving to probabilities, the mean also predicts 0, where _more convinced_ classifiers get a higher weight.](../_assets/clustering/ensemble_voting){#fig-clustering-el-voting}

In `scikit-learn` this is can be found in the [`sklearn.ensemble.VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) class.
::::
:::

::: {.callout appearance="simple"}
:::: {#def-ensemble-baggingpasting}

## Bagging and Pasting

For _bagging_ and _pasting_ the idea is different.
Instead of influencing the output theses approaches influence how to manipulate the training of a set of (potentially equal) classifiers to achieve overall better results. 

_Bagging_ (bootstrap aggregating) uses sampling with replacement, i.e. the same observation can end up multiple times in the training set of the same classifier.
_Pasting_ uses sampling without replacement and therefore an observation can be in multiple classifiers but not more than once per classifier.

To predict, we can use hard or soft voting from above.

![Illustration of the random sampling for bagging and pasting in ensemble classifiers.](../_assets/clustering/ensemble_sampling){#fig-clustering-el-sampling}

With these sampling methods it is possible to use the _out-of-bag_ observations (everything that is not used for a particular training) for evaluation of the trained classifier.
This is called _out-of-bag evaluation_.

In `scikit-learn` this is can be found in the [`sklearn.ensemble.BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) class.
::::
:::

::: {.callout appearance="simple"}
:::: {#def-ensemble-randompatches}

## Random Patches and Random Subspaces

For bagging and pasting it is also possible to sample features and not only observations, i.e. what to look at.
This results in a random subset of input features for training for each classifier.
This is especially useful, for high dimensional data inputs such as images, as it can speed up the learning process.

We call such methods _random patches_ method if we sample both, training observations and training features. 

On the other hand, if we keep the training observations fix and only sample the training features the resulting method is called a _random subspace_ method.

In `scikit-learn` this is can be found in the achieved by manipulating the hyperparameter `max_features`, `bootstrap_features`, and `max_samples` in the `BagginClassifier` class.
::::
:::

::: {.callout appearance="simple"}
:::: {#def-ensemble-randomforest}

## Random Forest

An ensemble of decision tress, (usually) trained via bagging is called a _random forest_.

With a random forest it is quite easy to find out what features are important for the overall result.
In `scikit-learn` this is automatically computed for the [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) class.
::::
:::

::: {.callout appearance="simple"}
:::: {#def-ensemble-boosting}

## Boosting

_Boosting_ or sometimes _hypothesis boosting_ is the process of using ensemble methods to use slow learners to create a fast learner. 

The general idea is to train a sequence where the output of one is the input of the next classifier.
This _corrects_ the previous result and therefore helps to achieve overall better results.

The most common methods are called _AdaBoost_ (adaptive boosting) and _gradient boosting_.

In `scikit-learn` we can find this functionality in the classes [`sklearn.ensemble.AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) and [`sklearn.ensemble.GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier).

::::
:::

::: {.callout appearance="simple"}
:::: {#def-ensemble-stacking}

## Stacking

The general idea of _stacking_ is, to use a _blender_ for combining the results of our ensemble methods.
The blender is not just a linear combination like for soft/hard voting but another classifier/model to perform a hopefully better combination.
Of course we can stack this approach and produce multiple layers before we combine them into a single result. 

![Illustration of an $m$ layer stacking with various classifiers in each layer.](../_assets/clustering/ensemble_voting){#fig-clustering-el-stacking}

In `scikit-learn` this is can be found in the [`sklearn.ensemble.StackingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) class.
::::
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-ensemble}

## Ensemble methods

We follow the example of [@Geron2022-xh, Chapter 7] to explore the various possibilities for ensemble methods. 

For our data set we use the moons example:
```{.python}
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

X, y = make_moons(n_samples=500, noise=0.30, random_state=6020)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=6020)
```

1. Use the `VotingClassifier` class with an _LDA_, a _SVM_, and a _tree_ for classification (make sure to keep set `random_state` for each to have reproducible results.). Get the overall score for the test set, as well as the individual scores for the included classifiers. 
   ```{.python}
   for name, clf in voting.named_estimators_.items():
       print(f"{name}, =, {clf.score(X_test, y_test)}")
   ```

1. Switch to _soft voting_ for your ensemble classifier and see how this influences the results.

1. Create a `BaggingClassifier` with 500 trees and and an appropriate `max_samples` value. Report the score for this new classifier, also report the out-of-bag score via `.oob_score_`.

1. Create directly a `RandomForestClassifier` with 500 trees and appropriate value for `max_leaf_nodes`.

1. Create a `StackingClassifier` with an _LDA_, a _SVM_, and a _tree_ for classification and a _random forest_ as the final blending step and report the score of this method.

1. Train a random forest for the iris data set and check `.feature_importances_` to get an insight on the importance of each feature.
::::
:::

## Multiclass Classification

We mainly focused on the classification of two classes in the last sections but obviously not all problems only consist of two classes. 
Some of the methods discussed like random forests support multiple classes right away.
For others there are several approaches to create a multiclass classifier out of a binary classifier.

::: {.callout appearance="simple"}
:::: {#def-multiclass-ovr}

## One vs. the Rest (OvR) or One vs. All (OvA)

The _one versus the rest_ (OvR) or _one versus all_ (OvA) strategy is to train a binary classifier for each class and always interpret all other classes as the others or the _rest class_.
To classify an observation you get the scores for each classifier and select the one with the highest score.

For the Fischer Iris data set this would result in three classifiers (one for each iris), for the MNIST data set in ten (one for each number).

::::
:::

::: {.callout appearance="simple"}
:::: {#def-multiclass-ovo}

## One vs. One (OvO)

The _one versus one_ (OvO) strategy is to train a binary classifier for always two classes and build up a set of classifiers, for $n$ classes we get $\tfrac{n (n-1)}{2}$ classifiers. 
To classify an observation you get the result for each classifier and select the one class with the most duels won.

For the Fischer Iris data set this would result in three classifiers, for the MNIST data set in 45.

The advantage of OvO over OvR is that each only needs to be trained on the subset and not with the entire data set. 
This is especially useful for algorithms that do not scale well, like SVMs.
::::
:::

::: {.callout-tip}
In `scikit-learn` the framework automatically realizes that we train a binary classifier for multiple classes and it will select OvR or OvO automatically, depending on the algorithm.

Nevertheless, there exist dedicated classes for the task as well [`sklearn.multiclass.OneVsOneClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html) and [`sklearn.multiclass.OneVsRestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier).
:::

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-nonlinear-multi}

## Multiclass classification

1. Use SVM for the Fischer Iris data set and test it.
1. Compare with a random forest.
1. Create a convolution matrix for more than two classes with your results in interpret the results.
::::
:::
