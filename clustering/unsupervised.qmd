# Unsupervised learning {#sec-clustering-usl}

We start with unsupervised learning.
The goal of unsupervised learning is to discover clusters in the data that has no labels.
There are several algorithms to perform this task, the most prominent is the $k$-means clustering algorithm.

## $k$-Means Clustering {#sec-clustering-usl-kmeans}

The $k$-means algorithm tries to partition a set of $m$ (vector-valued) data observations into $k$ clusters.
Where in general the dimension of the data and the amount of observations is known, the number of clusters is often not known a priori.

The general idea is, to label each observation as belonging to a cluster with the nearest mean (the _representative_ of the cluster).
The resulting clusters are called Voronoi cells, see [Wikipedia - Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).

::: {.callout appearance="simple"}
:::: {#def-kmeans}

## $k$-Means Algorithm

For a given set of $m$ observations $(x_1, x_2, \ldots, x_m)$, with $x_i\in \mathrm{R}^n$ the algorithm strives to find $k$ sets $(S_1, S_2, \ldots, S_k)$ such that the variance inside the cluster is minimized, i.e.
$$
\underset{S}{\operatorname{argmin}} \sum_{i=1}^k \sum_{x\in S_i}\| x - \mu_i \|^2,
$$
where $\mu_i$ denotes the mean of $S_i$.

The algorithm itself is recursive for a given $k$

1. Randomly initialize $k$ points $\mu_1, \mu_2, \ldots, \mu_k$, as the cluster centers.
1. Label each observation $x_i$ by the nearest cluster center $\mu_j$, all points with the same label form the set $S_j$.
1. Compute the mean of each cluster $S_j$ and replace the current $\mu_j$ by it.
1. Repeat until the cluster centers stay stable up to some tolerance.

This algorithm was first introduced in @DBLP:journals/tit/Lloyd82 and is therefore often called Lloyd algorithm.

```{python}
#| code-fold: true
#| code-summary: "Show the code for Lloyd algorithm"
def lloyd(data, centers, steps):
    classes = np.zeros(data.shape[0])
    centers_ = np.copy(centers)
    for i in range(steps):
        for j in range(data.shape[0]):
            classes[j] = np.argmin(np.linalg.norm(centers_ - np.tile(data[j, :],
                                   (centers.shape[0], 1)), axis=1))
        for j in range(centers.shape[0]):
            centers_[j, :] = np.mean(data[classes == j, :], axis=0)
        
    return (classes, centers_)
```
::::
:::

::: {.callout-note}
In general @def-kmeans is NP-hard and therefore computationally not viable.
Nevertheless, a number of heuristic algorithms exist to provide a good performance, despite having no guarantee for global convergence.
:::

To illustrate the proceedings with the help of some artificial data and its clustering.

::: {.callout-important}
The following data set and the basic structure of the code is from [@Brunton2022, Code 5.5 - 5.6].
Also see [GitHub](https://github.com/dynamicslab/databook_python).
:::

```{python}
#| label: fig-clustering-unsupervised_lloyd
#| fig-cap: "Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution."
#| fig-subcap: 
#|   - "The original two classes and the initial guesses for the centers."
#|   - "Point association to the clusters in the first step."
#|   - "Point association to the clusters in the third step."
#|   - "Point association to the clusters in the sixth step."
#| code-fold: true
#| layout-ncol: 2
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

# Helper for plotting
def plot_lloyd(data, centers, classes, ms=500):
    plt.figure()
    for i in range(centers.shape[0]):
        plt.scatter(data[classes==i, 0], data[classes==i, 1])
        plt.scatter(centers[i, 0], centers[i, 1], c="k", s=ms, marker="*")
    plt.gca().set_aspect(1)

# Create data for illustration
n = 200
# Random ellipse centred in (0, 0) and axis (1, 0.5)
X = np.random.randn(n, 2) * np.array([1, 0.5])
# Random ellipse centred in (1, -2) and axis (1, 0.2)
X2 = (np.random.randn(n, 2) * np.array([1, 0.2])) + np.array([1, -2])
# Rotate ellipse 2 by theta
theta = np.pi / 4
X2 = X2 @ np.array([[np.cos(theta), -np.sin(theta)],
                  [np.sin(theta), np.cos(theta)]] )

centers = np.array([[0., -1.], [-1., 0.]])
data = np.concatenate((X, X2))
# Plot initial step with theoretical assignment
classes = np.concatenate((np.ones(X2.shape[0]), np.zeros(X.shape[0])))
plot_lloyd(data, centers, classes)

# Compute and plot consecutive steps of lloyds algorithm
for i in [1, 2, 3]:
    classes, centers_ = lloyd(data, centers, i)
    plot_lloyd(data, centers, classes)
    centers = centers_
plt.show()
```

In @fig-clustering-unsupervised_lloyd-1 we see the two _distinct_ clusters and the initial guesses for th centers.
In the successive plots we see how the centers move and converge to the _final_ position as seen in @fig-clustering-unsupervised_lloyd-4.
In this case the algorithm converges after the sixth step.

Of course the algorithm is sensitive to the initial guess and therefore modern versions provide strategies to determine the initial guess as well as the number of clusters.

We can see this in action in the [`sklearn.cluster.KMeans`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html) version.

```{python}
#| label: fig-clustering-unsupervised_lloyd2
#| fig-cap: "KMeans algorithm in action for a generic set of two datasets generated with a Gaussian distribution."
#| fig-subcap: 
#|   - "Seeking two clusters."
#|   - "Seeking three clusters."
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Show the code for the figure"
from sklearn.cluster import KMeans
np.random.seed(6020)

kmeans = KMeans(n_clusters=2, random_state=6020).fit(data)
plot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)
kmeans = KMeans(n_clusters=3, random_state=6020).fit(data)
plot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)
plt.show()
```

As can be seen in @fig-clustering-unsupervised_lloyd2-1 the algorithm comes up with the same split between the two sets.
If we try for three clusters @fig-clustering-unsupervised_lloyd2-2 the result is sensible as well.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-kmean}

## Apply the $k$-means algorithm to the Iris dataset

As an exercise to get some practice for using $k$-means apply the algorithm to the Iris data set to find a potential split for the three classes of flowers.

Try with only two dimensional data and with all four dimensions.
::::
:::

The major success, of this algorithm in general use, is based on the fast convergence, that requires no supervision.
We can also see that it is not very accurate, compare @fig-clustering-unsupervised_lloyd-1 and @fig-clustering-unsupervised_lloyd-4.
This is no surprise, as the algorithm has not _all information_ available.

How can we determine how _accurate_ the algorithm is?
If we have no labels this is of course not easy to do but cross-validation is a good tool.

In our case we can produce the labels and we can also split the data beforehand into a training set and a test set.
Usually a so called $80:20$ split is used, i.e. $80\%$ training data and $20\%$ test data.

```{python}
#| label: fig-clustering-unsupervised_lloyd3
#| fig-cap: "Validation against a test set."
#| code-fold: true
#| code-summary: "Show the code for the figure"
from sklearn.cluster import KMeans
np.random.seed(6020)

# Shuffle data
X_shuffle = X[np.random.permutation(X.shape[0]), :]
X2_shuffle = X2[np.random.permutation(X2.shape[0]), :]

# Split data into two parts
split = n // 5 * 4
data = np.concatenate((X_shuffle[:split, :], X2_shuffle[:split, :]))
test = np.concatenate((X_shuffle[split:, :], X2_shuffle[split:, :]))

# Create clustesr
kmeans = KMeans(n_clusters=2, random_state=6020).fit(data)
classes = kmeans.predict(data)
test_classes = kmeans.predict(test)

# Find wrong classifications
error = test_classes != np.concatenate((np.zeros(n-split), np.ones(n-split)))

# Plotting
plt.figure()
colour = ["tab:orange", "tab:blue"]
for i in range(2):
    plt.scatter(data[classes==i, 0], data[classes==i, 1],
                c=colour[i], alpha=0.5, label="train")
    plt.scatter(test[test_classes==i, 0], test[test_classes==i, 1],
                c=colour[i], label="test")

plt.scatter(test[error, 0], test[error, 1], marker="x", c="k", label="error")
plt.gca().set_aspect(1)
plt.legend()
plt.show()
```

The result of this can be seen in @fig-clustering-unsupervised_lloyd3 where we have two points wrongly classified to the opposite cluster.

There exist several extensions of the basic $k$-means algorithm to improve the results and overall performance.
Two such versions are the _accelerated $k$-means_, as well as _mini-batch $k$-means_.
Both can be found in @Geron2022-xh.

### Applications of $k$-means

In addition to the image segmentation illustrated above, the $k$-means algorithm is used in a multitude of applications, we list some here:

1. **Image segmentation**: To decompose an image into different regions we can use $k$-means. Applications range from robotics to surveillance where, where one or more objects are separated from the rest of the image.

1. **Customer segmentation/social network analysis**: To segment customers along e.g. their purchase history/behaviour, preferences, demographic data, amount spent, search queries, social media interactions, etc. $k$-means is used in marketing, retail, and advertising to personalize the experience.

1. **Text clustering**: In natural language processing (NLP) $k$-means is often used to cluster similar documents or text to make analysing large volumes of text data feasible.

1. **Fraud detection**: $k$-means is a crude tool for fraud detection in finance and banking.
Transactions are clustered according to similarities and anomalies are detected.
There exist more sophisticated methods in finance.

1. **Anomaly detection**: In medical (image) data $k$-means is often used to detect anomalies by finding points that _fall outside of clusters_. The same works for cybersecurity and e.g. network traffic.

1. **Recommendation systems**: By grouping users together it is easier to recommend them new songs, items for shopping and more. 

1. **Quality control**: By grouping similar products you can detect quality issues and defects in production processes. If an issue is found the part can be checked and the process adjusted.

1. **Traffic Analysis**: In transport and logistics you can analyze traffic patterns and use the information for optimization and similar trips, routes and vehicles.

We want to highlight _image segmentation_ as an application of $k$-means. 
The example found here is first shown in @Geron2022-xh.

The idea of image segmentation is to decompose an image into different segments.
The following variants exist:

- Colour segmentation - pixels with similar colour get assigned the same cluster. An application of this is image satellite postprocessing to find areas of forest or sea, or finding object in robotics applications, and organs in medical images. 

- Semantic segmentation - all pixels that are part of the same _object_ get assigned to the same segment. An application of this is in autonomous driving to find pedestrians - one segment containing all pedestrians.

- Instance segmentation - similar to semantic segmentation but individual objects are assigned to the same segment. In the autonomous driving application we would find individual pedestrians.

Here we show how to perform colour segmentation with $k$-means.

::: {.callout-important}
The following data set and the basic structure of the code is from @Geron2022-xh, see [GitHub](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb), 
:::

```{python}
#| label: fig-clustering-unsupervised-img-seg
#| fig-cap: "Colour segmentation for the image of a lady bug."
#| fig-subcap: 
#|   - "Original image."
#|   - "Segmentation by 8 colours."
#|   - "Segmentation by 4 colours."
#|   - "Segmentation by 2 colours."
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import imageio.v3 as iio
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

im = np.asarray(iio.imread(
        "https://github.com/ageron/handson-ml3/blob/main/images/"
        "unsupervised_learning/ladybug.png?raw=true"))
plt.figure()
plt.imshow(im)
plt.axis("off")

Z = im.reshape(-1, 3)
for colours in [8, 4, 2]:
    kmeans = KMeans(n_clusters=colours, n_init=10, random_state=6020).fit(Z)
    plt.figure()
    plt.imshow(kmeans.cluster_centers_[kmeans.labels_].reshape(im.shape) / 255)
    plt.axis("off")

plt.show()
```

## Unsupervised hierarchical clustering - Dendrogram

Similar to $k$-means a simple hierarchical algorithm is used to create a dendrogram.
The resulting tree allows you to easily see if data is clustered without the need of labeling or supervision.

We follow the example and discussion given in [@Brunton2022, Section 5.4]. 

There are two main approaches in creating the desired hierarchy, bottom-up often called _agglomerative_ and top-down often called _divisive_.

- For the agglomerative approach each observation $x_i$ is initially its own cluster and in each step points are combined according to their distance. Eventually everything ends up in a single cluster and the algorithm stops. 

- For the divisive approach we go the opposite direction and start with the super cluster containing all observations Â§x_i$ and we gradually split up into smaller and smaller clusters. The algorithm stops, when each observation is its own leave. 

Of course the norm[^norm] used has quite an influence as can be seen in [@Kandolf_GDM, Section 7.1] [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/regression/optimizers.html#over-determined-systems) where we compared LASSO and RIDGE algorithms for our optimization problem. 

To illustrate the agglomerative approach and the difference in norms we use four points and construct the hierarchy by combining the two closest points.

::: {#fig-clustering-unsupervised-dendrogram-explained}

![Use two norm or euclidean norm to compute the distance - $\| \cdot \|_2$](../_assets/clustering/dendrogram_euclidean){#fig-clustering-unsupervised-dendrogram-explained-1}

![Use two one or cityblock norm to compute the distance - $\| \cdot \|_1$](../_assets/clustering/dendrogram_city){#fig-clustering-unsupervised-dendrogram-explained-2}

We always combine the two nearest points and replace them with the point in the middle. This iteration continues until only one point is left.
The Dendrogram is build according to the order of points chosen.
:::

On a larger scale, with always the first 25 points of the two clusters above we get the results shown in @fig-clustering-unsupervised_dengrogram
```{python}
#| label: fig-clustering-unsupervised_dengrogram
#| fig-cap: "Construction a agglomerative hierarchy for our data set."
#| fig-subcap: 
#|   - "Dendrogram for euclidean norm."
#|   - "Histogram showing the clustering for the euclidean norm."
#|   - "Dendrogram for cityblock norm."
#|   - "Histogram showing the clustering for the cityblock norm."
#| code-fold: true
#| layout-ncol: 2
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import pdist
from scipy.cluster import hierarchy
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

XX = np.concatenate((X[:25, :], X2[:25, :]))
XX.shape
for metric in ["euclidean", "cityblock"]:
    plt.figure()
    Y = pdist(XX, metric=metric)
    Z = hierarchy.linkage(Y, method="average", metric=metric)
    thresh = 0.90 * np.max(Z[:, 2])
    dn = hierarchy.dendrogram(Z, p=100, color_threshold=thresh)

    plt.figure()
    plt.bar(range(XX.shape[0]), dn["leaves"])
    plt.plot(np.array([0, XX.shape[0]]),
             np.array([XX.shape[0] // 2, XX.shape[0] // 2]),
             "r:", linewidth=2)
    plt.plot(np.array([(XX.shape[0] // 2) + 1/2, (XX.shape[0] // 2) + 1/2]),
             np.array([0, XX.shape[0]]),
             'r:', linewidth=2)

plt.show()
```

The two dendrograms (@fig-clustering-unsupervised_dengrogram-1 @fig-clustering-unsupervised_dengrogram-3) show the hierarchical structure derived from the data set.
The number of clusters can be influenced by the `thresh` parameter and it is also used to label the observation accordingly.
It is quite similar to the number of clusters $k$ in the $k$-means algorithm.

The two bar graphs on the right (@fig-clustering-unsupervised_dengrogram-2 @fig-clustering-unsupervised_dengrogram-4) shows how the data is clustered in the dendrogram. 
The bars correspond to the distance metric produced by the algorithm.
The red lines indicate the region of a perfect split for the separation if the algorithm works perfectly and places every point in the original cluster.
If we recall, that the first 25 points are from set 1 and the next 25 from set 2 we can see that the euclidean distance generates the perfect split.
On the other hand, the _cityblock_ norm is placing one point in the wrong cluster.

::: {.callout-note}
The function [`scipy.cluste.hierarchy.linkage`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage) allows to specify the method of computing the distance between two clusters. 
The used `average` corresponds to unweighted pair group method with arithmetic mean [UPGMA](https://en.wikipedia.org/wiki/UPGMA) algorithm.
:::

## Density-based spatial clustering of applications with noise (DBSCAN) {#sec-clustering-ul-dbscan}

The algorithm was introduced in @DBSCAN and is an algorithm that finds areas of high density.
The main idea behind the algorithm is as follows.

Our observations (we will call them points here for consistency) form the basis and they can be in any space but as for all the algorithms presented here, there needs to exist some form to measure distance.
Furthermore, DBSCAN relies on two parameters, $\epsilon$ describing the radius of neighbourhood of a point and $minPts$ the minimum of points needed to form a cluster. 
With these parameters we perform the following steps:

1. A point $p$ is called a _core point_ if at least $minPts$ (including $p$) are within distance $\epsilon$ of $p$.
This region is called the $\epsilon$-neighbourhood of $p$.

1. A point $q$ is called _directly reachable_ form an core point $p$ iff[^iff] $q$ is in the $\epsilon$-neighbourhood of $p$.

1. A point $p$ is _reachable_ from $p$ if there exists a sequence of points (path) $p=p_1, p_2,  \ldots, p_n=q$ where each $p_{i+1}$ is directly reachable form $p_i$.
Consequently, all point on the path, except $p$, need to be core points.

1. A point that is not reachable from any other point is considered an _outlier_ or _noise point_. 

Therefore, we get our definition of a cluster: a point core point $p$ together with all points that are reachable from $p$. This includes core points (at least one) and none-core points (boundary points) on the boundary as they can not be used to reach more points.

In contrast to $k$-means the algorithm we can find none linear clusters.
We can illustrate this with the _moon data set_. 

::: {.callout-important}
For the plotting function please see [GitHub](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb), the accompanying repository to @Geron2022-xh, as it forms the basis.
:::


```{python}
#| label: fig-clustering-unsupervised_dbscan
#| fig-cap: "DBSCAN illustrated with the moons dataset."
#| fig-subcap: 
#|   - "Scikit-Learn's DBSCAN with `eps=0.05` and `min_samples=5`."
#|   - "Scikit-Learn's DBSCAN with `eps=0.2` and `min_samples=5`."
#| code-fold: true
#| layout-ncol: 2
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

def plot_dbscan(dbscan, X, size):
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    core_mask[dbscan.core_sample_indices_] = True
    anomalies_mask = dbscan.labels_ == -1
    non_core_mask = ~(core_mask | anomalies_mask)

    cores = dbscan.components_
    anomalies = X[anomalies_mask]
    non_cores = X[non_core_mask]
    
    # Plotting the clusters
    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask],
                marker="o", s=size, cmap="Paired")
    # Plotting the core points of the clusters
    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask],
                marker="*", s=20,)
    # Plotting the anomalies
    plt.scatter(anomalies[:, 0], anomalies[:, 1],
                c="r", marker="x", s=100)
    # Plotting the boundary points
    plt.scatter(non_cores[:, 0], non_cores[:, 1],
                c=dbscan.labels_[non_core_mask], marker=".")
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$", rotation=0)
    plt.gca().set_aspect(1)

np.random.seed(6020)
M, M_y = make_moons(n_samples=1000, noise=0.05, random_state=6020)

dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(M)
plt.figure()
plot_dbscan(dbscan, M, size=100)

dbscan = DBSCAN(eps=0.2, min_samples=5)
dbscan.fit(M)
plt.figure()
plot_dbscan(dbscan, M, size=600)
plt.show()
```

The algorithm works best for clusters well separated by low density regions.
As all algorithms depending on distance measures it suffers from the curse of dimensionality[^1].

As can be seen in @fig-clustering-unsupervised_dbscan-1 the algorithm produces quite a lot of clusters for a small $\epsilon=0.05$.
In total we get 10 clusters and quite a lot of outliers that are not all easy to retract by the naked eye.

Nevertheless, if we increase $\epsilon=0.2$ we can see that the two clusters are neatly reproduced, @fig-clustering-unsupervised_dbscan-2.

::: {.callout-note}

The `DBSCAN` class in Scikit-Learn does not provide a `.predict()` method like many other such classes. 
See @Geron2022-xh and  [GitHub](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb) for how to train a $k$-nearest neighbour (kNN) to perform this task. 
:::


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-dbscan}

## Application of DBSCAN

1. Apply the DBSCAN algorithm to our toy example used through this section to recover the two clusters as good as possible.
Try different $\epsilon$ values as well as different norms (`metric` parameter).

1. Additionally, for a higher dimensional problem, using DBSCAN split the Iris dataset into its three classes of flowers.
::::
:::

## Finite Mixtures Models

Finite mixture models assume that the observations $x_i$ are mixtures of $k$ processes which combine in the measurement.
Each mixture is defined via a probability model with unknown parameters.
The aim of the algorithm is to find the parameters such that the $k$ processes best describe the $x_i$ observations.
The basis of the algorithm is the so called _expectation-maximization_ (EM) algorithm of @dempster1977maximum, that is designed to find the _maximum-likelihood_ parameters of the defined statistical models.

The most common choice for the probability models is the Gaussian distribution[^gauss] and for this choice, the method is better known as _Gaussian mixture models_ (GMM).
This is often quite a sensible choice if nothing more is known about the observations and a Gaussian distribution is than always a save bet.
As a result each process can be described by two parameters, _mean_ and _variance_.
Each cluster in turn, will be described by an ellipsoidal shape where size, density, orientation and semi-axis vary. 

Like $k$-means, in the simplest version, GMM is provided with the number of clusters $k$ and starts from an initial guess of the means and corresponding variances.
The parameters are than iteratively updated to find a (local) maximum.
There are some problems with this approach, e.g. if you set one of the processes to have zero variance and mean equal to a point.
To avoid such problems _cross-validation_ can often help to avoid problems stemming from an initialization with a poor initial guess.

As mentioned, the main idea of the mixture models is that the observations are a linear combination of probability density functions (PDF)
$$
f(x_j, \Theta) = \sum_{p=1}^k \alpha_p f_p(x_j, \Theta_p),
$$
where $f$ denotes the observed/measured PDF with parameters $\Theta$, $f_p$ the PDF of mixture $p$ with parameters $\Theta_p$, and $k$ denotes the number of mixtures. 
The weights $\alpha_p$ fulfil $\sum_p \alpha_p = 1$.

Overall we therefore can formulate the algorithm as:

::: {.callout appearance="simple"}
:::: {#def-ma}

## Mixture Models Algorithm
Given the observed PDF $f(x_j, \Theta)$, estimate the mixture weights $\alpha_p$ and the parameters fo the distribution $\Theta_p$.

[see @Brunton2022]
::::
:::

Lets follow the derivative for the GMM method in [@Brunton2022, Section 5.5] to bet a better idea what is happening. 

For the GMM $f$ becomes
$$
f(x_j, \Theta) = \sum_{p=1}^k \alpha_p \mathcal{N}_p(x_j, \mu_p, \sigma_p)
$$
and for a given $k$ we need to find $\alpha_p, \mu_p, \sigma_p$.
We get $\Theta$ from the roots of 
$$
L(\Theta) = \sum_{j=1}^n\log f(x_j | \Theta),
$$
$L$ is called the _log-likelihood_ function and we sum over all observations $x_j$.
We transform it into a optimization problem by setting the derivative to zero
$$
\frac{\partial L(\Theta)}{\partial \Theta} = 0
$$
and solve it via the EM algorithm.
As the name suggests, there are two steps E and M.

The E step uses the following posterior to establish memberships. 
Therefore, we start of by assuming an initial guess of the vector $\Theta$ and this leads to the _posterior probability of distribution $p$ of $x_j$ _
$$
\tau_p(x_j, \Theta) = \frac{\alpha_p f_p(x_j, \Theta_p)}{f(x_j, \Theta)}.
$$
In other words, we try to figure out if $x_j$ is part of the $p$th mixture.
For the GMM this becomes, in the $l$ iteration:
$$
\tau_p^{(l)}(x_j) = \frac{\alpha_p^{(l)} \mathcal{N}_p(x_j, \mu_p^{(l)}, \sigma_p^{(l)})}{f(x_j, \Theta^{(l)})}.
$$

Now the M step starts to update the parameters and mixture weights as
$$
\begin{align}
\alpha_p^{(l+1)} &= \frac1n \sum_{j=1}^n \tau_p^{(l)}(x_j), \\
\mu_p^{(l+1)} &= \frac{\sum_{j=1}^n x_j\tau_p^{(l)}(x_j)}{\sum_{j=1}^n \tau_p^{(l)}(x_j)}, \\
\Sigma_p^{(l+1)} &= \frac{\sum_{j=1}^n \tau_p^{(l)}(x_j)(x_j - \mu_p^{(l+1)})(x_j - \mu_p^{(l+1)})^\mathrm{T}}{\sum_{j=1}^n \tau_p^{(l)}(x_j)},
\end{align}
$$
with $\Sigma_p^{(l+1)}$ denotes the covariance matrix.
The algorithm now alternates between E and M until convergence is reached.

Let us try it with the cats and dogs images in wavelet basis from @sec-clustering-cvd by fitting to the second and forth principal value of the SVD.
Note, we directly load the images in wavelet basis and do not recompute them.

::: {.callout-important}
The following data set and the basic structure of the code is from [@Brunton2022, Code 5.8].
Also see [GitHub](https://github.com/dynamicslab/databook_python).
:::


```{python}
#| label: fig-clustering-gmm
#| fig-cap: "Fitting of the second and fourth principal value of the cats and dogs images in wavelet basis via the GMM method. "
#| fig-subcap: 
#|   - "We can see a nice split along the tow animals via the Gaussian distributions."
#|   - "The PDFs of the fitted distributions, weighted via the GMM algorithm."
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import scipy
import requests
import io
from sklearn.mixture import GaussianMixture
from scipy.stats import multivariate_normal
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

CD = np.concatenate((dogs, cats), axis=1)
U,S,VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)
v = VT.T

dogcat = v[:,(1,3)]
GMModel = GaussianMixture(n_components=2, n_init=10).fit(dogcat)

plt.figure()
plt.scatter(v[:80,1], v[:80,3], label="dogs")
plt.scatter(v[80:,1], v[80:,3], label="cats")
plt.xlabel(r"$PC_1$")
plt.ylabel(r"$PC_4$")

x = np.linspace(-0.15, 0.25)
y = np.linspace(-0.25, 0.2)
X, Y = np.meshgrid(x, y)
XX = np.array([X.ravel(), Y.ravel()]).T
Z = np.exp(GMModel.score_samples(XX))
Z = Z.reshape(X.shape)

CS = plt.contour(X, Y, Z,
                levels=np.arange(3,32,4), colors='k', linestyles='solid')
plt.legend()
plt.gca().set_aspect(1)

plt.figure()
ax = plt.axes(projection='3d')
for i in range(GMModel.weights_.shape[0]):
    rv = multivariate_normal(GMModel.means_[i], GMModel.covariances_[i])
    z = GMModel.weights_[i] * rv.pdf(np.dstack((X, Y)))
    ax.plot_surface(X, Y, z, alpha=0.5)

ax.plot_wireframe(X, Y, Z, color='black', rstride=5, cstride=5)
ax.view_init(30, -80)
plt.xlabel(r"$PC_1$")
plt.ylabel(r"$PC_4$")
plt.legend([r"$\mathcal{N}(\mu_1, \sigma_1)$",
            r"$\mathcal{N}(\mu_2, \sigma_2)$"])
plt.show()
```
As can be seen in @fig-clustering-gmm-1 the two clusters are quite well captured by the GMM algorithm.
The contour shows the linear combination of the two Gaussians.
We see the two Gaussians on the right in @fig-clustering-gmm-2 where we use the same colours as for the cats and dogs respectively.
Note that the distributions are weight $\alpha_p$ from GMM but other than that there is no unit in the z direction.

As discussed above the algorithm is prone to problems during the initialization, therefore we use `n_init = 10` to initialize the algorithm 10 times and only keep the best result.

We can draw new samples from the resulting object `.sample()` and we can also fit data to it and we can also perform hard and soft clustering. 
For hard clustering the model assigns each instance to the most likely cluster (`.predict()`), where for soft clustering (`.predict_proba()`) we get the probability of _membership_ for each cluster. 

If the algorithm struggles to recover the clusters, we can halp it by imposing the shape of the cluster via restricting the covariance matrix $\Sigma_p$.
This can be done via the parameter `covariance_type`, see [docs](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-gmm}

## Application of GMM

1. Apply the GMM algorithm to our toy example used through this section to recover the two clusters as good as possible.
This should be straight forward, as we constructed it via Gaussian distributions.
Try constraining the algorithm by imposing the different values possible for `covariance_type`. 

1. Additionally, for a higher dimensional problem, using DBSCAN split the Iris dataset into its three classes of flowers.

1. Try to use GMM to get clusters for the two moon dataset used in @sec-clustering-ul-dbscan.
Also try with the `BayesianGaussianMixture` class, where no amount of clusters needs to be specified.
::::
:::

::: {.callout-tip}
GMM can also be used for anomaly detection. 
Simply assume every observation located in a low density region an anomaly.
For this the threshold needs to be defined in accordance to your expected anomalies. 

E.g. to get about $4\%$ anomalies you can use the following snippet togethere with the above code:
```{python}
#| label: fig-clustering-gmm-ano
#| fig-cap: "Fitting of the second and fourth principal value of the cats and dogs images in wavelet basis via the GMM method. "
#| code-fold: true
#| code-summary: "Show the code for the figure"
S = GMModel.score_samples(dogcat)
density_threshold = np.percentile(S, 2)
anomalies = dogcat[S < density_threshold]
plt.figure()
plt.scatter(dogcat[:80,0], dogcat[:80,1], alpha=0.5, label="dogs")
plt.scatter(dogcat[80:,0], dogcat[80:,1], alpha=0.5, label="cats")
plt.xlabel(r"$PC_1$")
plt.ylabel(r"$PC_4$")
plt.scatter(anomalies[:, 0], anomalies[:, 1], color="k", marker="x",
            label="anomaly")
plt.legend()
plt.show()
```

see @Geron2022-xh and the corresponding [GitHub repository](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb)
:::

## Motivational example {#sec-clustering-ul-me}

Before we make the leap to supervised learning we we introduce a new data set and see how our unsupervised approach works in this case.

We do this with the (in)famous MNIST data set of hand written digits. 
Lets start by looking at the data set and establishing some basic properties.

We load the data set and look at its description
```{python}
#| classes: styled-output
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', as_frame=False)
print(mnist.DESCR)
```

The images are stored in `.data` and the label in `.target`.
We can look at the first 100 digits

```{python}
#| label: fig-clustering-usl-mnist
#| fig-cap: "First 100 digits from the MNIST data set."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

mnist = fetch_openml('mnist_784', as_frame=False)

im = np.zeros((10 * 28, 10 * 28), int)
for i in range(10):
    for j in range(10):
        im[28*i:28*(i+1), 28*j:28*(j+1)] = mnist.data[i*10 + j].reshape(28, 28)
plt.figure()
plt.imshow(im, cmap="binary")
plt.axis("off")

plt.show()
```

As can be seen from @fig-clustering-usl-mnist, most digits are easy to distinguish for us but there are some nasty things included. 
Especially, there are $1$s with the english style (just one straight line) of writing (1st row, 7th column), but also as a diagonal line (1st row, 4th column), and finally also the german style (connecting line at base and top) of writing (3rd row, 5th column). 

Also the two digit comes in various forms, with loop (1st row, 6th column) and without (3rd row, 6th column).
Finally we all know that a $3$ can hide in an $8$, same as a $9$, a $1$ can hide in a $7$ and much more. 

Now, before we go any further in the investigation we split up the data set into a training and a test set, we will not use this here but we should establish some way of working right from the start. 

```{python}
X_train, X_test = mnist.data[:60000], mnist.data[60000:]
y_train, y_test = mnist.target[:60000], mnist.target[60000:]
```

Now let us try with $k$-means to find the ten digits by finding $10$ clusters.

```{python}
#| label: fig-clustering-usl-mnist2
#| fig-cap: "The cluster means of $k$-means for 10 clusters."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

k = 10
kmeans = KMeans(n_clusters=k, n_init=1, random_state=6020).fit(X_train)

im = np.zeros((1 * 28, 10 * 28), int)
for i in range(1):
    for j in range(10):
        im[28*i:28*(i+1), 28*j:28*(j+1)] = \
                kmeans.cluster_centers_[i*10 + j].reshape(28, 28)
plt.figure()
plt.imshow(im, cmap="binary")
plt.axis("off")

plt.show()
```

As can be seen from @fig-clustering-usl-mnist2, the cluster centers do not recover our ten digits.
In general they are, as expected smudged as the do not represent an actual image.
It looks like $0, 1, 2, 3, 6, 8, 9$ are _easy_ to discover but $4, 5, 7$ not.
If we look closely, we can see $4, 5, 7$ represented in our clusters but not as separate digits.
Obviously this is not a good way to proceed to find our digits.
We will discuss methods to perform this task in the next section @sec-clustering-sl.
Furthermore, keep this part in mind as we will come back to how this can still be helpful in a different aspect as semi supervised learning @sec-clustering-ssl.

[^norm]: see @Kandolf_GDM, Section 1.2 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/basics/linearalgebra.html#norms)

[^iff]: short for _if and only if_

[^1]: Problems occurring for higher dimensional data and distance measures, see [WikiPedia](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions)

[^gauss]: see @Kandolf_GDM, Example 14.6 or for a direct [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/statistics/bayesian.html#exm-statistics-normal_dist) 