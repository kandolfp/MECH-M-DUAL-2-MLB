# Unsupervised learning

We start with unsupervised learning.
The goal of unsupervised learning is to discover clusters in the data that has no labels.
There are several algorithms to perform this task, the most prominent is the $k$-means clustering algorithm.

## $k$-Means Clustering

The $k$-means algorithm tries to partition a set of $m$ (vector-valued) data observations into $k$ clusters.
Where in general the dimension of the data and the amount of observations is known, the number of clusters is often not known a priori.

The general idea is, to label each observation as belonging to a cluster with the nearest mean (the _representative_ of the cluster).
The resulting clusters are called Voronoi cells, see [Wikipedia - Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).

::: {.callout appearance="simple"}
:::: {#def-kmeans}

## $k$-Means Algorithm

For a given set of $m$ observations $(x_1, x_2, \ldots, x_m)$, with $x_i\in \mathrm{R}^n$ the algorithm strives to find $k$ sets $(S_1, S_2, \ldots, S_k)$ such that the variance inside the cluster is minimized, i.e.
$$
\underset{S}{\operatorname{argmin}} \sum_{i=1}^k \sum_{x\in S_i}\| x - \mu_i \|^2,
$$
where $\mu_i$ denotes the mean of $S_i$.

The algorithm itself is recursive for a given $k$

1. Randomly initialize $k$ points $\mu_1, \mu_2, \ldots, \mu_k$, as the cluster centers.
1. Label each observation $x_i$ by the nearest cluster center $\mu_j$, all points with the same label form the set $S_j$.
1. Compute the mean of each cluster $S_j$ and replace the current $\mu_j$ by it.
1. Repeat until the cluster centers stay stable up to some tolerance.

This algorithm was first introduced in @DBLP:journals/tit/Lloyd82 and is therefore often called Lloyd algorithm.

```{python}
#| code-fold: true
#| code-summary: "Show the code for Lloyd algorithm"
def lloyd(data, centers, steps):
    classes = np.zeros(data.shape[0])
    centers_ = np.copy(centers)
    for i in range(steps):
        for j in range(data.shape[0]):
            classes[j] = np.argmin(np.linalg.norm(centers_ - np.tile(data[j, :],
                                   (centers.shape[0], 1)), axis=1))
        for j in range(centers.shape[0]):
            centers_[j, :] = np.mean(data[classes == j, :], axis=0)
        
    return (classes, centers_)
```
::::
:::

::: {.callout-note}
In general @def-kmeans is NP-hard and therefore computationally not viable.
Nevertheless, a number of heuristic algorithms exist to provide a good performance, despite having no guarantee for global convergence.
:::

To illustrate the proceedings with the help of some artificial data and its clustering.

```{python}
#| label: fig-clustering-unsupervised_lloyd
#| fig-cap: "Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution."
#| fig-subcap: 
#|   - "The original two classes and the initial guesses for the centers."
#|   - "Point association to the clusters in the first step."
#|   - "Point association to the clusters in the third step."
#|   - "Point association to the clusters in the sixth step."
#| code-fold: true
#| layout-ncol: 2
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

# Helper for plotting
def plot_lloyd(data, centers, classes, ms=500):
    plt.figure()
    for i in range(centers.shape[0]):
        plt.scatter(data[classes==i, 0], data[classes==i, 1])
        plt.scatter(centers[i, 0], centers[i, 1], c="k", s=ms, marker="*")
    plt.gca().set_aspect(1)

# Create data for illustration
n = 200
# Random ellipse centred in (0, 0) and axis (1, 0.5)
X = np.random.randn(n, 2) * np.array([1, 0.5])
# Random ellipse centred in (1, -2) and axis (1, 0.2)
X2 = (np.random.randn(n, 2) * np.array([1, 0.2])) + np.array([1, -2])
# Rotate ellipse 2 by theta
theta = np.pi / 4
X2 = X2 @ np.array([[np.cos(theta), -np.sin(theta)],
                  [np.sin(theta), np.cos(theta)]] )

centers = np.array([[0., -1.], [-1., 0.]])
data = np.concatenate((X, X2))
# Plot initial step with theoretical assignment
classes = np.concatenate((np.ones(X2.shape[0]), np.zeros(X.shape[0])))
plot_lloyd(data, centers, classes)

# Compute and plot consecutive steps of lloyds algorithm
for i in [1, 2, 3]:
    classes, centers_ = lloyd(data, centers, i)
    plot_lloyd(data, centers, classes)
    centers = centers_
plt.show()
```

In @fig-clustering-unsupervised_lloyd-1 we see the two _distinct_ clusters and the initial guesses for th centers.
In the successive plots we see how the centers move and converge to the _final_ position as seen in @fig-clustering-unsupervised_lloyd-4.
In this case the algorithm converges after the sixth step.

Of course the algorithm is sensitive to the initial guess and therefore modern versions provide strategies to determine the initial guess as well as the number of clusters.

We can see this in action in the [`sklearn.cluster.KMeans`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html) version.

```{python}
#| label: fig-clustering-unsupervised_lloyd2
#| fig-cap: "KMeans algorithm in action for a generic set of two datasets generated with a Gaussian distribution."
#| fig-subcap: 
#|   - "Seeking two clusters."
#|   - "Seeking three clusters."
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Show the code for the figure"
from sklearn.cluster import KMeans
np.random.seed(6020)

kmeans = KMeans(n_clusters=2, random_state=6020).fit(data)
plot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)
kmeans = KMeans(n_clusters=3, random_state=6020).fit(data)
plot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)
plt.show()
```

As can be seen in @fig-clustering-unsupervised_lloyd2-1 the algorithm comes up with the same split between the two sets.
If we try for three clusters @fig-clustering-unsupervised_lloyd2-2 the result is sensible as well.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-kmean}

## Apply the $k$-means algorithm to the Iris dataset

As an exercise to get some practice for using $k$-means apply the algorithm to the Iris data set to find a potential split for the three classes of flowers.

Try with only two dimensional data and with all four dimensions.
::::
:::

The major success, of this algorithm in general use, is based on the fast convergence, that requires no supervision.
We can also see that it is not very accurate, compare @fig-clustering-unsupervised_lloyd-1 and @fig-clustering-unsupervised_lloyd-4.
This is no surprise, as the algorithm has not _all information_ available.

How can we determine how _accurate_ the algorithm is?
If we have no labels this is of course not easy to do but cross-validation is a good tool.

In our case we can produce the labels and we can also split the data beforehand into a training set and a test set.
Usually a so called $80:20$ split is used, i.e. $80\%$ training data and $20\%$ test data.

```{python}
#| label: fig-clustering-unsupervised_lloyd3
#| fig-cap: "Validation against a test set."
#| code-fold: true
#| code-summary: "Show the code for the figure"
from sklearn.cluster import KMeans
np.random.seed(6020)

# Shuffle data
X_shuffle = X[np.random.permutation(X.shape[0]), :]
X2_shuffle = X2[np.random.permutation(X2.shape[0]), :]

# Split data into two parts
split = n // 5 * 4
data = np.concatenate((X_shuffle[:split, :], X2_shuffle[:split, :]))
test = np.concatenate((X_shuffle[split:, :], X2_shuffle[split:, :]))

# Create clustesr
kmeans = KMeans(n_clusters=2, random_state=6020).fit(data)
classes = kmeans.predict(data)
test_classes = kmeans.predict(test)

# Find wrong classifications
error = test_classes != np.concatenate((np.zeros(n-split), np.ones(n-split)))

# Plotting
plt.figure()
colour = ["tab:orange", "tab:blue"]
for i in range(2):
    plt.scatter(data[classes==i, 0], data[classes==i, 1],
                c=colour[i], alpha=0.5, label="train")
    plt.scatter(test[test_classes==i, 0], test[test_classes==i, 1],
                c=colour[i], label="test")

plt.scatter(test[error, 0], test[error, 1], marker="x", c="k", label="error")
plt.gca().set_aspect(1)
plt.legend()
plt.show()
```

The result of this can be seen in @fig-clustering-unsupervised_lloyd3 where we have two points wrongly classified to the opposite cluster.

There exist several extensions of the basic $k$-means algorithm to improve the results and overall performance.
Two such versions are the _accelerated $k$-means_, as well as _mini-batch $k$-means_.
Both can be found in @Geron2022-xh.

We want to highlight an application of $k$-means for image segmentation that can also be found in @Geron2022-xh.

### Image Segmentation with $k$-means

The idea of image segmentation is to decompose an image into different segments.
The following variants exist:

- Colour segmentation - pixels with similar colour get assigned the same cluster. An application of this is image satellite postprocessing to find areas of forest or sea.

- Semantic segmentation - all pixels that are part of the same _object_ get assigned to the same segment. An application of this is in autonomous driving to find pedestrians - one segment containing all pedestrians.

- Instance segmentation - similar to semantic segmentation but individual objects are assigned to the same segment. In the autonomous driving application we would find individual pedestrians.

Here we show how to perform colour segmentation with $k$-means.
The following code is based on notebook at [GitHub](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb), the accompanying repository to @Geron2022-xh.

```{python}
#| label: fig-clustering-unsupervised-img-seg
#| fig-cap: "Colour segmentation for the image of a lady bug."
#| fig-subcap: 
#|   - "Original image."
#|   - "Segmentation by 8 colours."
#|   - "Segmentation by 4 colours."
#|   - "Segmentation by 2 colours."
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import imageio.v3 as iio
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

im = np.asarray(iio.imread(
        "https://github.com/ageron/handson-ml3/blob/main/images/"
        "unsupervised_learning/ladybug.png?raw=true"))
plt.figure()
plt.imshow(im)
plt.axis("off")

Z = im.reshape(-1, 3)
for colours in [8, 4, 2]:
    kmeans = KMeans(n_clusters=colours, n_init=10, random_state=6020).fit(Z)
    plt.figure()
    plt.imshow(kmeans.cluster_centers_[kmeans.labels_].reshape(im.shape) / 255)
    plt.axis("off")

plt.show()
```

### Further applications of $k$-means

In addition to the image segmentation illustrated above, the $k$-means algorithm is used in a multitude of applications, we list some here:

1. **Customer segmentation/social network analysis**: To segment customers along e.g. their purchase history/behaviour, preferences, demographic data, amount spent, search queries, social media interactions, etc. $k$-means is used in marketing, retail, and advertising to personalize the experience.

1. **Text clustering**: In natural language processing (NLP) $k$-means is often used to cluster similar documents or text to make analysing large volumes of text data feasible.

1. **Fraud detection**: $k$-means is a crude tool for fraud detection in finance and banking.
Transactions are clustered according to similarities and anomalies are detected.
There exist more sophisticated methods in finance.

1. **Anomaly detection**: In medical (image) data $k$-means is often used to detect anomalies by finding points that _fall outside of clusters_. The same works for cybersecurity and e.g. network traffic.

1. **Recommendation systems**: By grouping users together it is easier to recommend them new songs, items for shopping and more. 

1. **Quality control**: By grouping similar products you can detect quality issues and defects in production processes. If an issue is found the part can be checked and the process adjusted.

1. **Traffic Analysis**: In transport and logistics you can analyze traffic patterns and use the information for optimization and similar trips, routes and vehicles.

## Unsupervised hierarchical clustering - Dendrogram

Similar to $k$-means a simple hierarchical algorithm is used to create a dendrogram.
The resulting tree allows you to easily see if data is clustered without the need of labeling or supervision.

We follow the example and discussion given in [@Brunton2022, Section 5.4]. 

There are two main approaches in creating the desired hierarchy, bottom-up often called _agglomerative_ and top-down often called _divisive_.

- For the agglomerative approach each observation $x_i$ is initially its own cluster and in each step points are combined according to their distance. Eventually everything ends up in a single cluster and the algorithm stops. 

- For the divisive approach we go the opposite direction and start with the super cluster containing all observations §x_i$ and we gradually split up into smaller and smaller clusters. The algorithm stops, when each observation is its own leave. 

Of course the norm[^norm] used has quite an influence as can be seen in [@Kandolf_GDM, Section 7.1] [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/regression/optimizers.html#over-determined-systems) where we compared LASSO and RIDGE algorithms for our optimization problem. 

To illustrate the agglomerative approach and the difference in norms we use four points and construct the hierarchy by combining the two closest points.

::: {#fig-clustering-unsupervised-dendrogram-explained}

![Use two norm or euclidean norm to compute the distance - $\| \cdot \|_2$](../_assets/clustering/dendrogram_euclidean){#fig-clustering-unsupervised-dendrogram-explained-1}

![Use two one or cityblock norm to compute the distance - $\| \cdot \|_1$](../_assets/clustering/dendrogram_city){#fig-clustering-unsupervised-dendrogram-explained-2}

We always combine the two nearest points and replace them with the point in the middle. This iteration continues until only one point is left.
The Dendrogram is build according to the order of points chosen.
:::

On a larger scale, with always the first 25 points of the two clusters above we get the results shown in @fig-clustering-unsupervised_dengrogram
```{python}
#| label: fig-clustering-unsupervised_dengrogram
#| fig-cap: "Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution."
#| fig-subcap: 
#|   - "Dendrogram for euclidean norm."
#|   - "Histogram showing the clustering for the euclidean norm."
#|   - "Dendrogram for cityblock norm."
#|   - "Histogram showing the clustering for the cityblock norm."
#| code-fold: true
#| layout-ncol: 2
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import pdist
from scipy.cluster import hierarchy
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

XX = np.concatenate((X[:25, :], X2[:25, :]))
XX.shape
for metric in ["euclidean", "cityblock"]:
    plt.figure()
    Y = pdist(XX, metric=metric)
    Z = hierarchy.linkage(Y, method="average", metric=metric)
    thresh = 0.90 * np.max(Z[:, 2])
    dn = hierarchy.dendrogram(Z, p=100, color_threshold=thresh)

    plt.figure()
    plt.bar(range(XX.shape[0]), dn["leaves"])
    plt.plot(np.array([0, XX.shape[0]]),
             np.array([XX.shape[0] // 2, XX.shape[0] // 2]),
             "r:", linewidth=2)
    plt.plot(np.array([(XX.shape[0] // 2) + 1/2, (XX.shape[0] // 2) + 1/2]),
             np.array([0, XX.shape[0]]),
             'r:', linewidth=2)

plt.show()
```

The two dendrograms (@fig-clustering-unsupervised_dengrogram-1 @fig-clustering-unsupervised_dengrogram-3) show the hierarchical structure derived from the data set.
The number of clusters can be influenced by the `thresh` parameter and it is also used to label the observation accordingly.
It is quite similar to the number of clusters $k$ in the $k$-means algorithm.

The two bar graphs on the right (@fig-clustering-unsupervised_dengrogram-2 @fig-clustering-unsupervised_dengrogram-4) shows how the data is clustered in the dendrogram. 
The bars correspond to the distance metric produced by the algorithm.
The red lines indicate the region of a perfect split for the separation if the algorithm works perfectly and places every point in the original cluster.
If we recall, that the first 25 points are from set 1 and the next 25 from set 2 we can see that the euclidean distance generates the perfect split.
On the other hand, the _cityblock_ norm is placing one point in the wrong cluster.

::: {.callout-note}
The function [`scipy.cluste.hierarchy.linkage`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage) allows to specify the method of computing the distance between two clusters. 
The used `average` corresponds to unweighted pair group method with arithmetic mean [UPGMA](https://en.wikipedia.org/wiki/UPGMA) algorithm.
:::


[^norm]: [see @Kandolf_GDM, Section 1.2] [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/basics/linearalgebra.html#norms)