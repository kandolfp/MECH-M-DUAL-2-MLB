# Unsupervised learning {#sec-clustering-usl}

We start with unsupervised learning.
The goal of unsupervised learning is to discover clusters in the data that has no labels.
There are several algorithms to perform this task, the most prominent is the $k$-means clustering algorithm.

## $k$-Means Clustering {#sec-clustering-usl-kmeans}

The $k$-means algorithm tries to partition a set of $m$ (vector-valued) data observations into $k$ clusters.
Where in general the dimension of the data and the amount of observations is known, the number of clusters is often not known a priori.

The general idea is, to label each observation as belonging to a cluster with the nearest mean (the _representative_ of the cluster).
The resulting clusters are called Voronoi cells, see [Wikipedia - Voronoi diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).

::: {.callout appearance="simple"}
:::: {#def-kmeans}

## $k$-Means Algorithm

For a given set of $m$ observations $(x_1, x_2, \ldots, x_m)$, with $x_i\in \mathrm{R}^n$ the algorithm strives to find $k$ sets $(S_1, S_2, \ldots, S_k)$ such that the variance inside the cluster is minimized, i.e.
$$
\underset{S}{\operatorname{argmin}} \sum_{i=1}^k \sum_{x\in S_i}\| x - \mu_i \|^2,
$$
where $\mu_i$ denotes the mean of $S_i$.

The algorithm itself is recursive for a given $k$

1. Randomly initialize $k$ points $\mu_1, \mu_2, \ldots, \mu_k$, as the cluster centers.
1. Label each observation $x_i$ by the nearest cluster center $\mu_j$, all points with the same label form the set $S_j$.
1. Compute the mean of each cluster $S_j$ and replace the current $\mu_j$ by it.
1. Repeat until the cluster centers stay stable up to some tolerance.

This algorithm was first introduced in @DBLP:journals/tit/Lloyd82 and is therefore often called Lloyd algorithm.

```{python}
#| code-fold: true
#| code-summary: "Show the code for Lloyd algorithm"
def lloyd(data, centers, steps):
    classes = np.zeros(data.shape[0])
    centers_ = np.copy(centers)
    for i in range(steps):
        for j in range(data.shape[0]):
            classes[j] = np.argmin(np.linalg.norm(centers_ - np.tile(data[j, :],
                                   (centers.shape[0], 1)), axis=1))
        for j in range(centers.shape[0]):
            centers_[j, :] = np.mean(data[classes == j, :], axis=0)
        
    return (classes, centers_)
```
::::
:::

::: {.callout-note}
In general @def-kmeans is NP-hard and therefore computationally not viable.
Nevertheless, a number of heuristic algorithms exist to provide a good performance, despite having no guarantee for global convergence.
:::

To illustrate the proceedings with the help of some artificial data and its clustering.

```{python}
#| label: fig-clustering-unsupervised_lloyd
#| fig-cap: "Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution."
#| fig-subcap: 
#|   - "The original two classes and the initial guesses for the centers."
#|   - "Point association to the clusters in the first step."
#|   - "Point association to the clusters in the third step."
#|   - "Point association to the clusters in the sixth step."
#| code-fold: true
#| layout-ncol: 2
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

# Helper for plotting
def plot_lloyd(data, centers, classes, ms=500):
    plt.figure()
    for i in range(centers.shape[0]):
        plt.scatter(data[classes==i, 0], data[classes==i, 1])
        plt.scatter(centers[i, 0], centers[i, 1], c="k", s=ms, marker="*")
    plt.gca().set_aspect(1)

# Create data for illustration
n = 200
# Random ellipse centred in (0, 0) and axis (1, 0.5)
X = np.random.randn(n, 2) * np.array([1, 0.5])
# Random ellipse centred in (1, -2) and axis (1, 0.2)
X2 = (np.random.randn(n, 2) * np.array([1, 0.2])) + np.array([1, -2])
# Rotate ellipse 2 by theta
theta = np.pi / 4
X2 = X2 @ np.array([[np.cos(theta), -np.sin(theta)],
                  [np.sin(theta), np.cos(theta)]] )

centers = np.array([[0., -1.], [-1., 0.]])
data = np.concatenate((X, X2))
# Plot initial step with theoretical assignment
classes = np.concatenate((np.ones(X2.shape[0]), np.zeros(X.shape[0])))
plot_lloyd(data, centers, classes)

# Compute and plot consecutive steps of lloyds algorithm
for i in [1, 2, 3]:
    classes, centers_ = lloyd(data, centers, i)
    plot_lloyd(data, centers, classes)
    centers = centers_
plt.show()
```

In @fig-clustering-unsupervised_lloyd-1 we see the two _distinct_ clusters and the initial guesses for th centers.
In the successive plots we see how the centers move and converge to the _final_ position as seen in @fig-clustering-unsupervised_lloyd-4.
In this case the algorithm converges after the sixth step.

Of course the algorithm is sensitive to the initial guess and therefore modern versions provide strategies to determine the initial guess as well as the number of clusters.

We can see this in action in the [`sklearn.cluster.KMeans`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html) version.

```{python}
#| label: fig-clustering-unsupervised_lloyd2
#| fig-cap: "KMeans algorithm in action for a generic set of two datasets generated with a Gaussian distribution."
#| fig-subcap: 
#|   - "Seeking two clusters."
#|   - "Seeking three clusters."
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Show the code for the figure"
from sklearn.cluster import KMeans
np.random.seed(6020)

kmeans = KMeans(n_clusters=2, random_state=6020).fit(data)
plot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)
kmeans = KMeans(n_clusters=3, random_state=6020).fit(data)
plot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)
plt.show()
```

As can be seen in @fig-clustering-unsupervised_lloyd2-1 the algorithm comes up with the same split between the two sets.
If we try for three clusters @fig-clustering-unsupervised_lloyd2-2 the result is sensible as well.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-kmean}

## Apply the $k$-means algorithm to the Iris dataset

As an exercise to get some practice for using $k$-means apply the algorithm to the Iris data set to find a potential split for the three classes of flowers.

Try with only two dimensional data and with all four dimensions.
::::
:::

The major success, of this algorithm in general use, is based on the fast convergence, that requires no supervision.
We can also see that it is not very accurate, compare @fig-clustering-unsupervised_lloyd-1 and @fig-clustering-unsupervised_lloyd-4.
This is no surprise, as the algorithm has not _all information_ available.

How can we determine how _accurate_ the algorithm is?
If we have no labels this is of course not easy to do but cross-validation is a good tool.

In our case we can produce the labels and we can also split the data beforehand into a training set and a test set.
Usually a so called $80:20$ split is used, i.e. $80\%$ training data and $20\%$ test data.

```{python}
#| label: fig-clustering-unsupervised_lloyd3
#| fig-cap: "Validation against a test set."
#| code-fold: true
#| code-summary: "Show the code for the figure"
from sklearn.cluster import KMeans
np.random.seed(6020)

# Shuffle data
X_shuffle = X[np.random.permutation(X.shape[0]), :]
X2_shuffle = X2[np.random.permutation(X2.shape[0]), :]

# Split data into two parts
split = n // 5 * 4
data = np.concatenate((X_shuffle[:split, :], X2_shuffle[:split, :]))
test = np.concatenate((X_shuffle[split:, :], X2_shuffle[split:, :]))

# Create clustesr
kmeans = KMeans(n_clusters=2, random_state=6020).fit(data)
classes = kmeans.predict(data)
test_classes = kmeans.predict(test)

# Find wrong classifications
error = test_classes != np.concatenate((np.zeros(n-split), np.ones(n-split)))

# Plotting
plt.figure()
colour = ["tab:orange", "tab:blue"]
for i in range(2):
    plt.scatter(data[classes==i, 0], data[classes==i, 1],
                c=colour[i], alpha=0.5, label="train")
    plt.scatter(test[test_classes==i, 0], test[test_classes==i, 1],
                c=colour[i], label="test")

plt.scatter(test[error, 0], test[error, 1], marker="x", c="k", label="error")
plt.gca().set_aspect(1)
plt.legend()
plt.show()
```

The result of this can be seen in @fig-clustering-unsupervised_lloyd3 where we have two points wrongly classified to the opposite cluster.

There exist several extensions of the basic $k$-means algorithm to improve the results and overall performance.
Two such versions are the _accelerated $k$-means_, as well as _mini-batch $k$-means_.
Both can be found in @Geron2022-xh.

### Applications of $k$-means

In addition to the image segmentation illustrated above, the $k$-means algorithm is used in a multitude of applications, we list some here:

1. **Image segmentation**: To decompose an image into different regions we can use $k$-means. Applications range from robotics to surveillance where, where one or more objects are separated from the rest of the image.

1. **Customer segmentation/social network analysis**: To segment customers along e.g. their purchase history/behaviour, preferences, demographic data, amount spent, search queries, social media interactions, etc. $k$-means is used in marketing, retail, and advertising to personalize the experience.

1. **Text clustering**: In natural language processing (NLP) $k$-means is often used to cluster similar documents or text to make analysing large volumes of text data feasible.

1. **Fraud detection**: $k$-means is a crude tool for fraud detection in finance and banking.
Transactions are clustered according to similarities and anomalies are detected.
There exist more sophisticated methods in finance.

1. **Anomaly detection**: In medical (image) data $k$-means is often used to detect anomalies by finding points that _fall outside of clusters_. The same works for cybersecurity and e.g. network traffic.

1. **Recommendation systems**: By grouping users together it is easier to recommend them new songs, items for shopping and more. 

1. **Quality control**: By grouping similar products you can detect quality issues and defects in production processes. If an issue is found the part can be checked and the process adjusted.

1. **Traffic Analysis**: In transport and logistics you can analyze traffic patterns and use the information for optimization and similar trips, routes and vehicles.

We want to highlight _image segmentation_ as an application of $k$-means. 
The example found here is first shown in @Geron2022-xh.

The idea of image segmentation is to decompose an image into different segments.
The following variants exist:

- Colour segmentation - pixels with similar colour get assigned the same cluster. An application of this is image satellite postprocessing to find areas of forest or sea, or finding object in robotics applications, and organs in medical images. 

- Semantic segmentation - all pixels that are part of the same _object_ get assigned to the same segment. An application of this is in autonomous driving to find pedestrians - one segment containing all pedestrians.

- Instance segmentation - similar to semantic segmentation but individual objects are assigned to the same segment. In the autonomous driving application we would find individual pedestrians.

Here we show how to perform colour segmentation with $k$-means.

::: {.callout-important}
The following data set and the basic structure of the code is from @Geron2022-xh, see [GitHub](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb), 
:::

```{python}
#| label: fig-clustering-unsupervised-img-seg
#| fig-cap: "Colour segmentation for the image of a lady bug."
#| fig-subcap: 
#|   - "Original image."
#|   - "Segmentation by 8 colours."
#|   - "Segmentation by 4 colours."
#|   - "Segmentation by 2 colours."
#| layout-ncol: 2
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import imageio.v3 as iio
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

im = np.asarray(iio.imread(
        "https://github.com/ageron/handson-ml3/blob/main/images/"
        "unsupervised_learning/ladybug.png?raw=true"))
plt.figure()
plt.imshow(im)
plt.axis("off")

Z = im.reshape(-1, 3)
for colours in [8, 4, 2]:
    kmeans = KMeans(n_clusters=colours, n_init=10, random_state=6020).fit(Z)
    plt.figure()
    plt.imshow(kmeans.cluster_centers_[kmeans.labels_].reshape(im.shape) / 255)
    plt.axis("off")

plt.show()
```

## Unsupervised hierarchical clustering - Dendrogram

Similar to $k$-means a simple hierarchical algorithm is used to create a dendrogram.
The resulting tree allows you to easily see if data is clustered without the need of labeling or supervision.

We follow the example and discussion given in [@Brunton2022, Section 5.4]. 

There are two main approaches in creating the desired hierarchy, bottom-up often called _agglomerative_ and top-down often called _divisive_.

- For the agglomerative approach each observation $x_i$ is initially its own cluster and in each step points are combined according to their distance. Eventually everything ends up in a single cluster and the algorithm stops. 

- For the divisive approach we go the opposite direction and start with the super cluster containing all observations §x_i$ and we gradually split up into smaller and smaller clusters. The algorithm stops, when each observation is its own leave. 

Of course the norm[^norm] used has quite an influence as can be seen in [@Kandolf_GDM, Section 7.1] [Link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/regression/optimizers.html#over-determined-systems) where we compared LASSO and RIDGE algorithms for our optimization problem. 

To illustrate the agglomerative approach and the difference in norms we use four points and construct the hierarchy by combining the two closest points.

::: {#fig-clustering-unsupervised-dendrogram-explained}

![Use two norm or euclidean norm to compute the distance - $\| \cdot \|_2$](../_assets/clustering/dendrogram_euclidean){#fig-clustering-unsupervised-dendrogram-explained-1}

![Use two one or cityblock norm to compute the distance - $\| \cdot \|_1$](../_assets/clustering/dendrogram_city){#fig-clustering-unsupervised-dendrogram-explained-2}

We always combine the two nearest points and replace them with the point in the middle. This iteration continues until only one point is left.
The Dendrogram is build according to the order of points chosen.
:::

On a larger scale, with always the first 25 points of the two clusters above we get the results shown in @fig-clustering-unsupervised_dengrogram
```{python}
#| label: fig-clustering-unsupervised_dengrogram
#| fig-cap: "Construction a agglomerative hierarchy for our data set."
#| fig-subcap: 
#|   - "Dendrogram for euclidean norm."
#|   - "Histogram showing the clustering for the euclidean norm."
#|   - "Dendrogram for cityblock norm."
#|   - "Histogram showing the clustering for the cityblock norm."
#| code-fold: true
#| layout-ncol: 2
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import pdist
from scipy.cluster import hierarchy
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

XX = np.concatenate((X[:25, :], X2[:25, :]))
XX.shape
for metric in ["euclidean", "cityblock"]:
    plt.figure()
    Y = pdist(XX, metric=metric)
    Z = hierarchy.linkage(Y, method="average", metric=metric)
    thresh = 0.90 * np.max(Z[:, 2])
    dn = hierarchy.dendrogram(Z, p=100, color_threshold=thresh)

    plt.figure()
    plt.bar(range(XX.shape[0]), dn["leaves"])
    plt.plot(np.array([0, XX.shape[0]]),
             np.array([XX.shape[0] // 2, XX.shape[0] // 2]),
             "r:", linewidth=2)
    plt.plot(np.array([(XX.shape[0] // 2) + 1/2, (XX.shape[0] // 2) + 1/2]),
             np.array([0, XX.shape[0]]),
             'r:', linewidth=2)

plt.show()
```

The two dendrograms (@fig-clustering-unsupervised_dengrogram-1 @fig-clustering-unsupervised_dengrogram-3) show the hierarchical structure derived from the data set.
The number of clusters can be influenced by the `thresh` parameter and it is also used to label the observation accordingly.
It is quite similar to the number of clusters $k$ in the $k$-means algorithm.

The two bar graphs on the right (@fig-clustering-unsupervised_dengrogram-2 @fig-clustering-unsupervised_dengrogram-4) shows how the data is clustered in the dendrogram. 
The bars correspond to the distance metric produced by the algorithm.
The red lines indicate the region of a perfect split for the separation if the algorithm works perfectly and places every point in the original cluster.
If we recall, that the first 25 points are from set 1 and the next 25 from set 2 we can see that the euclidean distance generates the perfect split.
On the other hand, the _cityblock_ norm is placing one point in the wrong cluster.

::: {.callout-note}
The function [`scipy.cluste.hierarchy.linkage`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage) allows to specify the method of computing the distance between two clusters. 
The used `average` corresponds to unweighted pair group method with arithmetic mean [UPGMA](https://en.wikipedia.org/wiki/UPGMA) algorithm.
:::

## Density-based spatial clustering of applications with noise (DBSCAN) {#sec-clustering-ul-dbscan}

The algorithm was introduced in @DBSCAN and is an algorithm that finds areas of high density.
The main idea behind the algorithm is as follows.

Our observations (we will call them points here for consistency) form the basis and they can be in any space but as for all the algorithms presented here, there needs to exist some form to measure distance.
Furthermore, DBSCAN relies on two parameters, $\epsilon$ describing the radius of neighbourhood of a point and $minPts$ the minimum of points needed to form a cluster. 
With these parameters we perform the following steps:

1. A point $p$ is called a _core point_ if at least $minPts$ (including $p$) are within distance $\epsilon$ of $p$.
This region is called the $\epsilon$-neighbourhood of $p$.

1. A point $q$ is called _directly reachable_ form an core point $p$ iff[^iff] $q$ is in the $\epsilon$-neighbourhood of $p$.

1. A point $p$ is _reachable_ from $p$ if there exists a sequence of points (path) $p=p_1, p_2,  \ldots, p_n=q$ where each $p_{i+1}$ is directly reachable form $p_i$.
Consequently, all point on the path, except $p$, need to be core points.

1. A point that is not reachable from any other point is considered an _outlier_ or _noise point_. 

Therefore, we get our definition of a cluster: a point core point $p$ together with all points that are reachable from $p$. This includes core points (at least one) and none-core points (boundary points) on the boundary as they can not be used to reach more points.

In contrast to $k$-means the algorithm we can find none linear clusters.
We can illustrate this with the _moon data set_. 

::: {.callout-important}
For the plotting function please see [GitHub](https://github.com/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb), the accompanying repository to @Geron2022-xh, as it forms the basis.
:::


```{python}
#| label: fig-clustering-unsupervised_dbscan
#| fig-cap: "DBSCAN illustrated with the moons dataset."
#| fig-subcap: 
#|   - "Scikit-Learn's DBSCAN with `eps=0.05` and `min_samples=5`."
#|   - "Scikit-Learn's DBSCAN with `eps=0.2` and `min_samples=5`."
#| code-fold: true
#| layout-ncol: 2
#| code-summary: "Show the code for the figure"
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

def plot_dbscan(dbscan, X, size):
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    core_mask[dbscan.core_sample_indices_] = True
    anomalies_mask = dbscan.labels_ == -1
    non_core_mask = ~(core_mask | anomalies_mask)

    cores = dbscan.components_
    anomalies = X[anomalies_mask]
    non_cores = X[non_core_mask]
    
    # Plotting the clusters
    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask],
                marker="o", s=size, cmap="Paired")
    # Plotting the core points of the clusters
    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask],
                marker="*", s=20,)
    # Plotting the anomalies
    plt.scatter(anomalies[:, 0], anomalies[:, 1],
                c="r", marker="x", s=100)
    # Plotting the boundary points
    plt.scatter(non_cores[:, 0], non_cores[:, 1],
                c=dbscan.labels_[non_core_mask], marker=".")
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$", rotation=0)
    plt.gca().set_aspect(1)

np.random.seed(6020)
M, M_y = make_moons(n_samples=1000, noise=0.05, random_state=6020)

dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(M)
plt.figure()
plot_dbscan(dbscan, M, size=100)

dbscan = DBSCAN(eps=0.2, min_samples=5)
dbscan.fit(M)
plt.figure()
plot_dbscan(dbscan, M, size=600)
plt.show()
```

The algorithm works best for clusters well separated by low density regions.
As all algorithms depending on distance measures it suffers from the curse of dimensionality[^1].

As can be seen in @fig-clustering-unsupervised_dbscan-1 the algorithm produces quite a lot of clusters for a small $\epsilon=0.05$.
In total we get 10 clusters and quite a lot of outliers that are not all easy to retract by the naked eye.

Nevertheless, if we increase $\epsilon=0.2$ we can see that the two clusters are neatly reproduced, @fig-clustering-unsupervised_dbscan-2.


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-dbscan}

## Application of DBSCAN

1. Apply the DBSCAN algorithm to our toy example used throught this section to recover the two clusters as good as possible.
Try different $\epsilon$ values as well as different norms (`metric` parameter).

1. Additionally, for a higher dimensional problem, using DBSCAN split the Iris dataset into its three classes of flowers.
::::
:::


[^norm]: see @Kandolf_GDM, Section 1.2 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/basics/linearalgebra.html#norms)

[îff]: short for _if and only if_

[^1]: Problems occurring for higher dimensional data and distance measures, see [WikiPedia](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions)