[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "",
    "text": "Preface\nThese are the lecture notes for the Maschinelles Lernen in der industriellen Bildverarbeitung class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the summer term 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nThese notes are built with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basic concepts behind modern day Data Science techniques for industrial image processing. We will always try to not only discuss the theory but also use Python to illustrate the content programmatically.\nThe plan for this notes it to cover not only the basics of modern methods for machine learning in image processing but also topics in the near vicinity like\n\nData management, labeling and preprocessing\nLife-cycle management of models\nmachine learning operations (MLOps)\ncurrent and research topics\n\nThese notes are intended for engineering students and therefore the mathematical concepts will rarely include rigorous proofs.\n\n\n\n\n\n\nNote\n\n\n\nThe sklearn package has all the tools to perform a PCA, see Link for an example using the Iris dataset same as in the beginning of this section.\n\n\nThe following books are cover large sections of the content and are a good read to get further into the topics:\n\nBrunton and Kutz (2022)\nGeron (2022)\nGoodfellow, Bengio, and Courville (2016)\nLandup (2022)\nHuyen (2022)\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems. Sebastopol, CA: O’Reilly Media.\n\n\nLandup, David. 2022. Practical Deep Learning for Computer Vision with Python.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "clustering/index.html",
    "href": "clustering/index.html",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Feature selection and generation of a feature space\nOne aspect if machine learning is binning data into meaningful and distinct categories that can be used for decision making. This is often called clustering and classification. To achieve this task the main modus of operation is to find a low-rank feature space that is informative and interpretable.\nOne way is to extract the dominate features in a data set are for example the singular value decomposition (SVD) or principal component analysis, see (Kandolf 2025). The idea is to not work in the heigh dimensional measurement space, but instead in the low rank feature space. This allows us to significantly reduce the cost by performing the our analysis in this low dimensional space.\nIn order to find the feature space there are two main paths for machine learning:\nAs the names suggest in unsupervised learning no labels are given to the algorithm and it must find patterns in the data to generate clusters and labels such that predictions can be made. This is often used to discover previously unknown patterns in the (low-rank) subspace of the data and leads to feature engineering or feature extraction. These features can than be used for building a model.\nSupervised learning, on the other hand, uses labelled data sets. The training data is labelled for cross-validated by a teacher or expert. I.e. the input and output for a model is explicitly provided and regression methods are used to find the best model for the given labelled data. There are several different form of supervised learning like reinforcement learning, semi-supervised learning, or active learning.\nThe main idea behind machine learning is to construct or exploit the intrinsic low-rank feature space of a given data set, how to find these is determined by the algorithm.\nBefore we go into specific algorithms lets have a general look at what we are going to talk about. We will follow (Brunton and Kutz 2022, sec. 5.1) for this introduction.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "href": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Fischer Iris data set\nFirst we look into one of the standard data sets well known in the community, the so called Fischer Iris data set Fisher (1936).\n\n\n\n\n\n\nNote\n\n\n\nLuckily for us there are several data packages available for general use. One of them is the Fisher Iris data set.\n\n\n\n\n\n\nFigure 1: Illustration of the recorded features and structure of the iris dataset from the Medium article - Exploring the Iris flower dataset.\n\n\n\nWe use the possibilities of sklearn 1 to access the dataset in Python.\n\n\n\n\nShow the code for the figure\nimport plotly.express as px\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nfig = px.scatter_3d(df, x='sepal length (cm)', y='sepal width (cm)',\n                    z='petal width (cm)', color='target')\ncamera = dict(\n    eye=dict(x=2, y=2, z=0.1)\n)\nfig.update_layout(scene_camera=camera)\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a scatter 3D plot along the features sepal length, sepal width, and petal width, the forth feature petal width is not shown.\n\n\n\n\nAs can be seen in Figure 2 the three features are enough to easily separate Sentosa and to a very good degree Versicolor from Virginica, where the last two have a small overlap in the samples provided.\nThe petal length seems to be the best feature for the classification and no highly sophisticated machine learning algorithms are required. We can see the this even better if we use a so called Pair plot from the seaborn package as seen in Figure 3.\n\n\nShow the code for the figure\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\n_ = sns.pairplot(iris.frame, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\nFigure 3: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a pair grid plot. The diagonal shows the univariate distribution of the feature.\n\n\n\n\n\nHere a grid with plots is created where each (numeric) feature is shared across a row and a column. Therefore, we can see the combination of two and in the diagonal we see the univariate distribution of the values. After a close inspection we can see that petal width provides us with the best distinction feature and sepal width with the worst.\nEven though we already have enough to give a good classification for this dataset it is worth to investigate a bit further in order to illustrate how, for example the principal component analysis (PCA) (see Kandolf 2025, sec. 4.2) Link can be applied here.\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nA = df.iloc[: , :4].to_numpy()\n\nXavg = A.mean(0)\nB = A - np.tile(Xavg, (150, 1))\nU, S, VT = np.linalg.svd(B, full_matrices=False)\nC = B @ VT[:2, :].T\nq = np.linalg.norm(S[:2])**2 / np.linalg.norm(B, \"fro\")**2\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(C[index, 0], C[index, 1], label=name)\nplt.legend()\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_2$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Feature reduction with PCA for the Iris dataset. We show the first two components principal components.\n\n\n\n\n\nAs can be seen in the Figure 4 the first two components cover about 97.77% of the total variation in the samples and in accordance with this result a very good separation of the three species.\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 1 (Explained Variance) The explained variance is a measure of how much of the total variance in the data is explained via the principal component. It is equivalent to the singular value associated with the component so in the above example it is nothing else than \\[\n\\rho_i = \\frac{\\sigma_i^2}{\\|\\Sigma\\|^2}.\n\\]\nThe explained variance can be used to select the amount of principal values you use for a classification.\n\n\n\n\n\n\n\n\n\nDimensionality Reduction\n\n\n\nAs we can see with the Iris data set we often have a problem of dimensionality if we try to visualize something. The data has four dimensions but it is very hard for us to perceive, let alone visualize, four dimensions (if the forth dimension is not time).\nTherefore, dimensional reduction techniques are often applied. One of these is PCA others are\n\nMultidimensional scaling (MDS) - preserve the distance between observations while doing the reduction.\nIsomap - create a graph by connecting each observation to its neighbors and reduce the dimensions by keeping the geodesic distances, i.e. the number of nodes on the graph between observations\n\\(t\\)-distributed stochastic neighbor embedding (\\(t\\)-SNE) - try to reduce dimensions while keeping similar observations close and dissimilar observations apart.\nLinear discriminant analysis (LDA) - projects data onto a hyperplane where the hyperplane is chosen that is most discriminative for the observations.\n\n\n\nNow let us take a look at a more complicated example from Brunton and Kutz (2022).\n\n\nDogs and Cats\nIn this data set we explore how distinguish between dogs and cats. The data set contains \\(80\\) images of dogs and \\(80\\) images of cats, each with \\(64 \\times 64\\) pixels and therefore a total of \\(4096\\) feature measurements per image.\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.2 - 5.4). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef createImage(data, x, y, width=64, length=64):\n    image = np.zeros((width * x, length * y))\n    counter = 0\n    for i in range(x):\n        for j in range(y):\n            img = np.flipud(np.reshape(data[:, counter], (width, length)))\n            image[length * i: length * (i + 1), width * j: width * (j + 1)] \\\n                    = img.T\n            counter += 1\n    return image\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(createImage(cats, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog\"]\nplt.figure()\nplt.imshow(createImage(dogs, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\nFigure 5: The first 16 cats from the data set.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The first 16 dogs from the data set.\n\n\n\n\n\n\nAgain, we use PCA to reduce the feature space of our data.\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nCD = np.concatenate((dogs, cats), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=False)\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U[:, j], (64, 64)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 7: First four modes of the PCA of the 160 images.\n\n\n\nIn Figure 7 we can see the first four modes of the PCA. While mode 2 (Figure 7 (b)) especially features the pointy ears common in cats, mode 3 (Figure 7 (c)) is more dog like. Consequently, will the two different animals have either add or subtract this feature form the mean.\nOf course this is not the only possible representation of the initial data. As discussed in (Kandolf 2025, sec. 10) Link we can also use the Wavelet transform.\nThe multi resolution analysis features of the Wavelet transformation can be used to transform the image in such a way that the resulting principal components yield a better separation of the two classes.\n\n\n\n\n\n\nNote\n\n\n\nWe use the following workflow to get from the original image \\(A_0\\) to the \\(\\tilde{A}\\) image in a sort of Wavelet basis.\n\n\n\n\n\n\nFigure 8: Workflow to get from the original image to the wavelet transformed version.\n\n\n\nIn short, we combine the vertical and horizontal rescaled feature images, as we are most interested in edge detection for this example.\nNote that the image has now only a quarter of pixels as the original image.\nFor a more detailed explanation see @#appendix-dvc.\n\n\nShow code required for the transformation.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\n\nimport pywt\nimport math\n\ndef img2wave(data):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\n\n\n\n\nLet us try how this changes our first four modes.\n\nShow the code for the figures\nCD_w = img2wave(CD)\nU_w, S_w, VT_w = np.linalg.svd(CD_w - np.mean(CD_w), full_matrices=False)\nl = math.isqrt(CD_w[:, 0].shape[0])\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U_w[:, j], (l, l)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 9: First four modes of the PCA of the 160 images in our wavelet basis.\n\n\n\nRight away we can see that the ears and eyes of the cats show up more pronounced in the second mode Figure 9 (b).\nNow how does this influence our classification problem? For this let us first see how easy it is to distinguish the two classes in a scatter plot for the first three modes.\n\nShow the code for the figure\nimport seaborn as sns\nimport pandas as pd\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\ntarget = [\"cat\"] * 80 + [\"dog\"] * 80\nC = U[:, :4].T @ (CD - np.mean(CD))\ndf = pd.DataFrame(C.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf[\"target\"] = target\n\nC_w = U_w[:, :4].T @ (CD_w - np.mean(CD))\ndf_w = pd.DataFrame(C_w.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf_w[\"target\"] = target\n\n_ = sns.pairplot(df, hue=\"target\", height=1.45)\n_ = sns.pairplot(df_w, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\n\n\nFigure 10: First four modes of the raw images for cats and dogs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: First four modes of the wavelet images for cats and dogs.\n\n\n\n\n\n\nThe two figures above give us a good idea what the different principal values can tell us. It is quite clear, that the first mode is not of much use for differentiation of the two classes. In general the second mode seems to work best (the ears), in contrast to the raw image we can also see that the wavelet basis is slightly better for the separation.\n\n\nShow the code for the figures\nimport plotly.express as px\n\n\nfig = px.scatter_3d(df, x='PV2', y='PV3', z='PV4', color=\"target\")\nfig.show()\nfig = px.scatter_3d(df_w, x=\"PV2\", y=\"PV3\", z=\"PV4\", color=\"target\")\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 12: Scatter plots of modes 2 to 4 for the raw images.\n\n\n\n\n\n\n                                                \n\n\nFigure 13: Scatter plots of modes 2 to 4 for the wavelet images.\n\n\n\n\nNow that we have an idea what we are dealing with we can start looking into the two previously described paths in machine learning. The difference between supervised and unsupervised learning can be summarized by the following two images.\n\nShow the code for the figure\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(df.iloc[:, 0][index], df.iloc[:, 2][index], label=name)\n\nplt.figure()\nplt.scatter(df.iloc[:, 0], df.iloc[:, 2], color=\"gray\")\n\n\n\n\n\n\n\n\n\n\nFigure 14: Labelled data for supervised learning.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Unlabelled data for unsupervised learning.\n\n\n\n\n\n\nYou either provide/have labels or not.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nFisher, R. A. 1936. “THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS.” Annals of Eugenics 7 (2): 179–88. https://doi.org/https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#footnotes",
    "href": "clustering/index.html#footnotes",
    "title": "Clustering and Classification",
    "section": "",
    "text": "To add it to pdm us pdm add scikit-learn.↩︎",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html",
    "href": "clustering/unsupervised.html",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "1.1 \\(k\\)-Means Clustering\nWe start with unsupervised learning. The goal of unsupervised learning is to discover clusters in the data that has no labels. There are several algorithms to perform this task, the most prominent is the \\(k\\)-means clustering algorithm.\nThe \\(k\\)-means algorithm tries to partition a set of \\(m\\) (vector-valued) data observations into \\(k\\) clusters. Where in general the dimension of the data and the amount of observations is known, the number of clusters is often not known a priori.\nThe general idea is, to label each observation as belonging to a cluster with the nearest mean (the representative of the cluster). The resulting clusters are called Voronoi cells, see Wikipedia - Voronoi diagram.\nTo illustrate the proceedings with the help of some artificial data and its clustering.\nIn Figure 1.1 (a) we see the two distinct clusters and the initial guesses for th centers. In the successive plots we see how the centers move and converge to the final position as seen in Figure 1.1 (d). In this case the algorithm converges after the sixth step.\nOf course the algorithm is sensitive to the initial guess and therefore modern versions provide strategies to determine the initial guess as well as the number of clusters.\nWe can see this in action in the sklearn.cluster.KMeans version.\nAs can be seen in Figure 1.2 (a) the algorithm comes up with the same split between the two sets. If we try for three clusters Figure 1.2 (b) the result is sensible as well.\nThe major success, of this algorithm in general use, is based on the fast convergence, that requires no supervision. We can also see that it is not very accurate, compare Figure 1.1 (a) and Figure 1.1 (d). This is no surprise, as the algorithm has not all information available.\nHow can we determine how accurate the algorithm is? If we have no labels this is of course not easy to do but cross-validation is a good tool.\nIn our case we can produce the labels and we can also split the data beforehand into a training set and a test set. Usually a so called \\(80:20\\) split is used, i.e. \\(80\\%\\) training data and \\(20\\%\\) test data.\nShow the code for the figure\nfrom sklearn.cluster import KMeans\nnp.random.seed(6020)\n\n# Shuffle data\nX_shuffle = X[np.random.permutation(X.shape[0]), :]\nX2_shuffle = X2[np.random.permutation(X2.shape[0]), :]\n\n# Split data into two parts\nsplit = n // 5 * 4\ndata = np.concatenate((X_shuffle[:split, :], X2_shuffle[:split, :]))\ntest = np.concatenate((X_shuffle[split:, :], X2_shuffle[split:, :]))\n\n# Create clustesr\nkmeans = KMeans(n_clusters=2, random_state=6020).fit(data)\nclasses = kmeans.predict(data)\ntest_classes = kmeans.predict(test)\n\n# Find wrong classifications\nerror = test_classes != np.concatenate((np.zeros(n-split), np.ones(n-split)))\n\n# Plotting\nplt.figure()\ncolour = [\"tab:orange\", \"tab:blue\"]\nfor i in range(2):\n    plt.scatter(data[classes==i, 0], data[classes==i, 1],\n                c=colour[i], alpha=0.5, label=\"train\")\n    plt.scatter(test[test_classes==i, 0], test[test_classes==i, 1],\n                c=colour[i], label=\"test\")\n\nplt.scatter(test[error, 0], test[error, 1], marker=\"x\", c=\"k\", label=\"error\")\nplt.gca().set_aspect(1)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.3: Validation against a test set.\nThe result of this can be seen in Figure 1.3 where we have two points wrongly classified to the opposite cluster.\nThere exist several extensions of the basic \\(k\\)-means algorithm to improve the results and overall performance. Two such versions are the accelerated \\(k\\)-means, as well as mini-batch \\(k\\)-means. Both can be found in Geron (2022).\nWe want to highlight an application of \\(k\\)-means for image segmentation that can also be found in Geron (2022).",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#k-means-clustering",
    "href": "clustering/unsupervised.html#k-means-clustering",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "Definition 1.1 (\\(k\\)-Means Algorithm) For a given set of \\(m\\) observations \\((x_1, x_2, \\ldots, x_m)\\), with \\(x_i\\in \\mathrm{R}^n\\) the algorithm strives to find \\(k\\) sets \\((S_1, S_2, \\ldots, S_k)\\) such that the variance inside the cluster is minimized, i.e. \\[\n\\underset{S}{\\operatorname{argmin}} \\sum_{i=1}^k \\sum_{x\\in S_i}\\| x - \\mu_i \\|^2,\n\\] where \\(\\mu_i\\) denotes the mean of \\(S_i\\).\nThe algorithm itself is recursive for a given \\(k\\)\n\nRandomly initialize \\(k\\) points \\(\\mu_1, \\mu_2, \\ldots, \\mu_k\\), as the cluster centers.\nLabel each observation \\(x_i\\) by the nearest cluster center \\(\\mu_j\\), all points with the same label form the set \\(S_j\\).\nCompute the mean of each cluster \\(S_j\\) and replace the current \\(\\mu_j\\) by it.\nRepeat until the cluster centers stay stable up to some tolerance.\n\nThis algorithm was first introduced in Lloyd (1982) and is therefore often called Lloyd algorithm.\n\n\nShow the code for Lloyd algorithm\ndef lloyd(data, centers, steps):\n    classes = np.zeros(data.shape[0])\n    centers_ = np.copy(centers)\n    for i in range(steps):\n        for j in range(data.shape[0]):\n            classes[j] = np.argmin(np.linalg.norm(centers_ - np.tile(data[j, :],\n                                   (centers.shape[0], 1)), axis=1))\n        for j in range(centers.shape[0]):\n            centers_[j, :] = np.mean(data[classes == j, :], axis=0)\n        \n    return (classes, centers_)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn general Definition 1.1 is NP-hard and therefore computationally not viable. Nevertheless, a number of heuristic algorithms exist to provide a good performance, despite having no guarantee for global convergence.\n\n\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\n# Helper for plotting\ndef plot_lloyd(data, centers, classes, ms=500):\n    plt.figure()\n    for i in range(centers.shape[0]):\n        plt.scatter(data[classes==i, 0], data[classes==i, 1])\n        plt.scatter(centers[i, 0], centers[i, 1], c=\"k\", s=ms, marker=\"*\")\n    plt.gca().set_aspect(1)\n\n# Create data for illustration\nn = 200\n# Random ellipse centred in (0, 0) and axis (1, 0.5)\nX = np.random.randn(n, 2) * np.array([1, 0.5])\n# Random ellipse centred in (1, -2) and axis (1, 0.2)\nX2 = (np.random.randn(n, 2) * np.array([1, 0.2])) + np.array([1, -2])\n# Rotate ellipse 2 by theta\ntheta = np.pi / 4\nX2 = X2 @ np.array([[np.cos(theta), -np.sin(theta)],\n                  [np.sin(theta), np.cos(theta)]] )\n\ncenters = np.array([[0., -1.], [-1., 0.]])\ndata = np.concatenate((X, X2))\n# Plot initial step with theoretical assignment\nclasses = np.concatenate((np.ones(X2.shape[0]), np.zeros(X.shape[0])))\nplot_lloyd(data, centers, classes)\n\n# Compute and plot consecutive steps of lloyds algorithm\nfor i in [1, 2, 3]:\n    classes, centers_ = lloyd(data, centers, i)\n    plot_lloyd(data, centers, classes)\n    centers = centers_\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The original two classes and the initial guesses for the centers.\n\n\n\n\n\n\n\n\n\n\n\n(b) Point association to the clusters in the first step.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Point association to the clusters in the third step.\n\n\n\n\n\n\n\n\n\n\n\n(d) Point association to the clusters in the sixth step.\n\n\n\n\n\n\n\nFigure 1.1: Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\n\n\n\n\nShow the code for the figure\nfrom sklearn.cluster import KMeans\nnp.random.seed(6020)\n\nkmeans = KMeans(n_clusters=2, random_state=6020).fit(data)\nplot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)\nkmeans = KMeans(n_clusters=3, random_state=6020).fit(data)\nplot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Seeking two clusters.\n\n\n\n\n\n\n\n\n\n\n\n(b) Seeking three clusters.\n\n\n\n\n\n\n\nFigure 1.2: KMeans algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.1 (Apply the \\(k\\)-means algorithm to the Iris dataset) As an exercise to get some practice for using \\(k\\)-means apply the algorithm to the Iris data set to find a potential split for the three classes of flowers.\nTry with only two dimensional data and with all four dimensions.\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 Image Segmentation with \\(k\\)-means\nThe idea of image segmentation is to decompose an image into different segments. The following variants exist:\n\nColour segmentation - pixels with similar colour get assigned the same cluster. An application of this is image satellite postprocessing to find areas of forest or sea, or finding object in robotics applications, and organs in medical images.\nSemantic segmentation - all pixels that are part of the same object get assigned to the same segment. An application of this is in autonomous driving to find pedestrians - one segment containing all pedestrians.\nInstance segmentation - similar to semantic segmentation but individual objects are assigned to the same segment. In the autonomous driving application we would find individual pedestrians.\n\nHere we show how to perform colour segmentation with \\(k\\)-means. The following code is based on notebook at GitHub, the accompanying repository to Geron (2022).\n\nShow the code for the figure\nimport numpy as np\nimport imageio.v3 as iio\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nim = np.asarray(iio.imread(\n        \"https://github.com/ageron/handson-ml3/blob/main/images/\"\n        \"unsupervised_learning/ladybug.png?raw=true\"))\nplt.figure()\nplt.imshow(im)\nplt.axis(\"off\")\n\nZ = im.reshape(-1, 3)\nfor colours in [8, 4, 2]:\n    kmeans = KMeans(n_clusters=colours, n_init=10, random_state=6020).fit(Z)\n    plt.figure()\n    plt.imshow(kmeans.cluster_centers_[kmeans.labels_].reshape(im.shape) / 255)\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Segmentation by 8 colours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Segmentation by 4 colours.\n\n\n\n\n\n\n\n\n\n\n\n(d) Segmentation by 2 colours.\n\n\n\n\n\n\n\nFigure 1.4: Colour segmentation for the image of a lady bug.\n\n\n\n\n\n1.1.2 Further applications of \\(k\\)-means\nIn addition to the image segmentation illustrated above, the \\(k\\)-means algorithm is used in a multitude of applications, we list some here:\n\nCustomer segmentation/social network analysis: To segment customers along e.g. their purchase history/behaviour, preferences, demographic data, amount spent, search queries, social media interactions, etc. \\(k\\)-means is used in marketing, retail, and advertising to personalize the experience.\nText clustering: In natural language processing (NLP) \\(k\\)-means is often used to cluster similar documents or text to make analysing large volumes of text data feasible.\nFraud detection: \\(k\\)-means is a crude tool for fraud detection in finance and banking. Transactions are clustered according to similarities and anomalies are detected. There exist more sophisticated methods in finance.\nAnomaly detection: In medical (image) data \\(k\\)-means is often used to detect anomalies by finding points that fall outside of clusters. The same works for cybersecurity and e.g. network traffic.\nRecommendation systems: By grouping users together it is easier to recommend them new songs, items for shopping and more.\nQuality control: By grouping similar products you can detect quality issues and defects in production processes. If an issue is found the part can be checked and the process adjusted.\nTraffic Analysis: In transport and logistics you can analyze traffic patterns and use the information for optimization and similar trips, routes and vehicles.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#unsupervised-hierarchical-clustering---dendrogram",
    "href": "clustering/unsupervised.html#unsupervised-hierarchical-clustering---dendrogram",
    "title": "1  Unsupervised learning",
    "section": "1.2 Unsupervised hierarchical clustering - Dendrogram",
    "text": "1.2 Unsupervised hierarchical clustering - Dendrogram\nSimilar to \\(k\\)-means a simple hierarchical algorithm is used to create a dendrogram. The resulting tree allows you to easily see if data is clustered without the need of labeling or supervision.\nWe follow the example and discussion given in (Brunton and Kutz 2022, sec. 5.4).\nThere are two main approaches in creating the desired hierarchy, bottom-up often called agglomerative and top-down often called divisive.\n\nFor the agglomerative approach each observation \\(x_i\\) is initially its own cluster and in each step points are combined according to their distance. Eventually everything ends up in a single cluster and the algorithm stops.\nFor the divisive approach we go the opposite direction and start with the super cluster containing all observations §x_i$ and we gradually split up into smaller and smaller clusters. The algorithm stops, when each observation is its own leave.\n\nOf course the norm1 used has quite an influence as can be seen in (Kandolf 2025, sec. 7.1) Link where we compared LASSO and RIDGE algorithms for our optimization problem.\nTo illustrate the agglomerative approach and the difference in norms we use four points and construct the hierarchy by combining the two closest points.\n\n\n\n\n\n\n\n\n\n(a) Use two norm or euclidean norm to compute the distance - \\(\\| \\cdot \\|_2\\)\n\n\n\n\n\n\n\n\n\n(b) Use two one or cityblock norm to compute the distance - \\(\\| \\cdot \\|_1\\)\n\n\n\n\n\nFigure 1.5: We always combine the two nearest points and replace them with the point in the middle. This iteration continues until only one point is left. The Dendrogram is build according to the order of points chosen.\n\n\n\nOn a larger scale, with always the first 25 points of the two clusters above we get the results shown in Figure 1.6\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster import hierarchy\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nXX = np.concatenate((X[:25, :], X2[:25, :]))\nXX.shape\nfor metric in [\"euclidean\", \"cityblock\"]:\n    plt.figure()\n    Y = pdist(XX, metric=metric)\n    Z = hierarchy.linkage(Y, method=\"average\", metric=metric)\n    thresh = 0.90 * np.max(Z[:, 2])\n    dn = hierarchy.dendrogram(Z, p=100, color_threshold=thresh)\n\n    plt.figure()\n    plt.bar(range(XX.shape[0]), dn[\"leaves\"])\n    plt.plot(np.array([0, XX.shape[0]]),\n             np.array([XX.shape[0] // 2, XX.shape[0] // 2]),\n             \"r:\", linewidth=2)\n    plt.plot(np.array([(XX.shape[0] // 2) + 1/2, (XX.shape[0] // 2) + 1/2]),\n             np.array([0, XX.shape[0]]),\n             'r:', linewidth=2)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Dendrogram for euclidean norm.\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram showing the clustering for the euclidean norm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Dendrogram for cityblock norm.\n\n\n\n\n\n\n\n\n\n\n\n(d) Histogram showing the clustering for the cityblock norm.\n\n\n\n\n\n\n\nFigure 1.6: Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\nThe two dendrograms (Figure 1.6 (a) Figure 1.6 (c)) show the hierarchical structure derived from the data set. The number of clusters can be influenced by the thresh parameter and it is also used to label the observation accordingly. It is quite similar to the number of clusters \\(k\\) in the \\(k\\)-means algorithm.\nThe two bar graphs on the right (Figure 1.6 (b) Figure 1.6 (d)) shows how the data is clustered in the dendrogram. The bars correspond to the distance metric produced by the algorithm. The red lines indicate the region of a perfect split for the separation if the algorithm works perfectly and places every point in the original cluster. If we recall, that the first 25 points are from set 1 and the next 25 from set 2 we can see that the euclidean distance generates the perfect split. On the other hand, the cityblock norm is placing one point in the wrong cluster.\n\n\n\n\n\n\nNote\n\n\n\nThe function scipy.cluste.hierarchy.linkage allows to specify the method of computing the distance between two clusters. The used average corresponds to unweighted pair group method with arithmetic mean UPGMA algorithm.\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.\n\n\nLloyd, Stuart P. 1982. “Least Squares Quantization in PCM.” IEEE Trans. Inf. Theory 28 (2): 129–36. https://doi.org/10.1109/TIT.1982.1056489.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#footnotes",
    "href": "clustering/unsupervised.html#footnotes",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "(see Kandolf 2025, sec. 1.2) Link↩︎",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "T.B.D.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nFisher, R. A. 1936. “THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC\nPROBLEMS.” Annals of Eugenics 7 (2): 179–88.\nhttps://doi.org/https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow 3e. 3rd ed. Sebastopol, CA:\nO’Reilly Media.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems.\nSebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen\nDatenbasierter Methoden.” Management Center Innsbruck, Course\nMaterial. https://doi.org/10.5281/zenodo.14671708.\n\n\nLandup, David. 2022. Practical Deep Learning for Computer Vision\nwith Python.\n\n\nLloyd, Stuart P. 1982. “Least Squares Quantization in\nPCM.” IEEE Trans. Inf. Theory\n28 (2): 129–36. https://doi.org/10.1109/TIT.1982.1056489.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendices/explanations.html",
    "href": "appendices/explanations.html",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "A.1 Wavelet decomposition for cats and dogs\nIn Section 1.2 we discuss using the wavelet transformation to transform the image into a different basis. Here are the details of how this is performed with cat zero as example.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(np.reshape(cats[:, 0], (64, 64)).T, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\nFigure A.1: The original cat.\nWe use the Haar-Wavelet and we only need to do one level of transformation. As per usual we get four images, each half the resolution, that represent the decomposition. The images are, a downsampled version of the original image, one highlighting the vertical features, one highlighting the horizontal features, and one highlighting the diagonal features.\nFigure A.2: Wavelet transformation of the cat.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.4\n\n\n\n\n\n\n\n\n\n\n\nFigure A.5\nFor our purposes only the vertical and horizontal feature are of interest and we combine these two images. In order to make sure the features are highlighted optimal we need to rescale the images before the combination. For this we use a similar function like the MATLAB wcodemat function.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\nFigure A.6: Combination of vertical and horizontal features unaltered.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.7: Combination of vertical and horizontal features rescaled.\nIn total this leads to the following function to transform a list of the images, given as row vectors.\nimport pywt\nimport math\n\ndef img2wave(images):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\nNote that the resulting image has only one forth of the pixels as the original image. We can also visualize the transformation steps as follows in Figure A.8.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#appendix-dvc",
    "href": "appendices/explanations.html#appendix-dvc",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "Show the code for the figure\nimport pywt\n\n[A_1, (cH1, cV1, cD1)] = pywt.wavedec2(np.reshape(cats[:, 0], (64, 64)).T,\n                                       wavelet=\"haar\", level=1)\nplt.figure()\nplt.imshow(A_1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cH1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cD1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\nShow the code for the figure\nimport pywt\n\nplt.figure()\nplt.imshow(cH1 + cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(rescale(cH1, 256) + rescale(cV1, 256), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\nFigure A.8: Workflow to get from the original image to the wavelet transformed version.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  }
]