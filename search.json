[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "",
    "text": "Preface\nThese are the lecture notes for the Maschinelles Lernen in der industriellen Bildverarbeitung class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the summer term 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nThese notes are built with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basic concepts behind modern day Data Science techniques for industrial image processing. We will always try to not only discuss the theory but also use Python to illustrate the content programmatically.\nThe plan for this notes it to cover not only the basics of modern methods for machine learning in image processing but also topics in the near vicinity like\n\nData management, labeling and preprocessing\nLife-cycle management of models\nmachine learning operations (MLOps)\ncurrent and research topics\n\nThese notes are intended for engineering students and therefore the mathematical concepts will rarely include rigorous proofs.\n\n\n\n\n\n\nNote\n\n\n\nThe sklearn package has all the tools to perform a PCA, see Link for an example using the Iris dataset same as in the beginning of this section.\n\n\nThe following books are cover large sections of the content and are a good read to get further into the topics:\n\nBrunton and Kutz (2022)\nGeron (2022)\nGoodfellow, Bengio, and Courville (2016)\nLandup (2022)\nHuyen (2022)\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems. Sebastopol, CA: O’Reilly Media.\n\n\nLandup, David. 2022. Practical Deep Learning for Computer Vision with Python.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "clustering/index.html",
    "href": "clustering/index.html",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Feature selection and generation of a feature space\nOne aspect if machine learning is binning data into meaningful and distinct categories that can be used for decision making. This is often called clustering and classification. To achieve this task the main modus of operation is to find a low-rank feature space that is informative and interpretable.\nOne way is to extract the dominate features in a data set are for example the singular value decomposition (SVD) or principal component analysis, see (Kandolf 2025). The idea is to not work in the heigh dimensional measurement space, but instead in the low rank feature space. This allows us to significantly reduce the cost by performing the our analysis in this low dimensional space.\nIn order to find the feature space there are two main paths for machine learning:\nAs the names suggest in unsupervised learning no labels are given to the algorithm and it must find patterns in the data to generate clusters and labels such that predictions can be made. This is often used to discover previously unknown patterns in the (low-rank) subspace of the data and leads to feature engineering or feature extraction. These features can than be used for building a model.\nSupervised learning, on the other hand, uses labelled data sets. The training data is labelled for cross-validated by a teacher or expert. I.e. the input and output for a model is explicitly provided and regression methods are used to find the best model for the given labelled data. There are several different form of supervised learning like reinforcement learning, semi-supervised learning, or active learning.\nThe main idea behind machine learning is to construct or exploit the intrinsic low-rank feature space of a given data set, how to find these is determined by the algorithm.\nBefore we go into specific algorithms lets have a general look at what we are going to talk about. We will follow (Brunton and Kutz 2022, sec. 5.1) for this introduction.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "href": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Fischer Iris data set\nFirst we look into one of the standard data sets well known in the community, the so called Fischer Iris data set Fisher (1936).\n\n\n\n\n\n\nNote\n\n\n\nLuckily for us there are several data packages available for general use. One of them is the Fisher Iris data set.\n\n\n\n\n\n\nFigure 1: Illustration of the recorded features and structure of the iris dataset from the Medium article - Exploring the Iris flower dataset.\n\n\n\nWe use the possibilities of sklearn 1 to access the dataset in Python.\n\n\n\n\nShow the code for the figure\nimport plotly.express as px\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nfig = px.scatter_3d(df, x='sepal length (cm)', y='sepal width (cm)',\n                    z='petal width (cm)', color='target')\ncamera = dict(\n    eye=dict(x=2, y=2, z=0.1)\n)\nfig.update_layout(scene_camera=camera)\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a scatter 3D plot along the features sepal length, sepal width, and petal width, the forth feature petal width is not shown.\n\n\n\n\nAs can be seen in Figure 2 the three features are enough to easily separate Sentosa and to a very good degree Versicolor from Virginica, where the last two have a small overlap in the samples provided.\nThe petal length seems to be the best feature for the classification and no highly sophisticated machine learning algorithms are required. We can see the this even better if we use a so called Pair plot from the seaborn package as seen in Figure 3.\n\n\nShow the code for the figure\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\n_ = sns.pairplot(iris.frame, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\nFigure 3: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a pair grid plot. The diagonal shows the univariate distribution of the feature.\n\n\n\n\n\nHere a grid with plots is created where each (numeric) feature is shared across a row and a column. Therefore, we can see the combination of two and in the diagonal we see the univariate distribution of the values. After a close inspection we can see that petal width provides us with the best distinction feature and sepal width with the worst.\nEven though we already have enough to give a good classification for this dataset it is worth to investigate a bit further in order to illustrate how, for example the principal component analysis (PCA) (see Kandolf 2025, sec. 4.2) Link can be applied here.\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nA = df.iloc[: , :4].to_numpy()\n\nXavg = A.mean(0)\nB = A - np.tile(Xavg, (150, 1))\nU, S, VT = np.linalg.svd(B, full_matrices=False)\nC = B @ VT[:2, :].T\nq = np.linalg.norm(S[:2])**2 / np.linalg.norm(B, \"fro\")**2\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(C[index, 0], C[index, 1], label=name)\nplt.legend()\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_2$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Feature reduction with PCA for the Iris dataset. We show the first two components principal components.\n\n\n\n\n\nAs can be seen in the Figure 4 the first two components cover about 97.77% of the total variation in the samples and in accordance with this result a very good separation of the three species.\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 1 (Explained Variance) The explained variance is a measure of how much of the total variance in the data is explained via the principal component. It is equivalent to the singular value associated with the component so in the above example it is nothing else than \\[\n\\rho_i = \\frac{\\sigma_i^2}{\\|\\Sigma\\|^2}.\n\\]\nThe explained variance can be used to select the amount of principal values you use for a classification.\n\n\n\n\n\n\n\n\n\nDimensionality Reduction\n\n\n\nAs we can see with the Iris data set we often have a problem of dimensionality if we try to visualize something. The data has four dimensions but it is very hard for us to perceive, let alone visualize, four dimensions (if the forth dimension is not time).\nTherefore, dimensional reduction techniques are often applied. One of these is PCA others are\n\nMultidimensional scaling (MDS) - preserve the distance between observations while doing the reduction.\nIsomap - create a graph by connecting each observation to its neighbors and reduce the dimensions by keeping the geodesic distances, i.e. the number of nodes on the graph between observations\n\\(t\\)-distributed stochastic neighbor embedding (\\(t\\)-SNE) - try to reduce dimensions while keeping similar observations close and dissimilar observations apart.\nLinear discriminant analysis (LDA) - projects data onto a hyperplane where the hyperplane is chosen that is most discriminative for the observations.\n\n\n\nNow let us take a look at a more complicated example from Brunton and Kutz (2022).\n\n\nDogs and Cats\nIn this data set we explore how distinguish between dogs and cats. The data set contains \\(80\\) images of dogs and \\(80\\) images of cats, each with \\(64 \\times 64\\) pixels and therefore a total of \\(4096\\) feature measurements per image.\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.2 - 5.4). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef createImage(data, x, y, width=64, length=64):\n    image = np.zeros((width * x, length * y))\n    counter = 0\n    for i in range(x):\n        for j in range(y):\n            img = np.flipud(np.reshape(data[:, counter], (width, length)))\n            image[length * i: length * (i + 1), width * j: width * (j + 1)] \\\n                    = img.T\n            counter += 1\n    return image\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(createImage(cats, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog\"]\nplt.figure()\nplt.imshow(createImage(dogs, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\nFigure 5: The first 16 cats from the data set.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The first 16 dogs from the data set.\n\n\n\n\n\n\nAgain, we use PCA to reduce the feature space of our data.\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nCD = np.concatenate((dogs, cats), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=False)\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U[:, j], (64, 64)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 7: First four modes of the PCA of the 160 images.\n\n\n\nIn Figure 7 we can see the first four modes of the PCA. While mode 2 (Figure 7 (b)) especially features the pointy ears common in cats, mode 3 (Figure 7 (c)) is more dog like. Consequently, will the two different animals have either add or subtract this feature form the mean.\nOf course this is not the only possible representation of the initial data. As discussed in (Kandolf 2025, sec. 10) Link we can also use the Wavelet transform.\nThe multi resolution analysis features of the Wavelet transformation can be used to transform the image in such a way that the resulting principal components yield a better separation of the two classes.\n\n\n\n\n\n\nNote\n\n\n\nWe use the following workflow to get from the original image \\(A_0\\) to the \\(\\tilde{A}\\) image in a sort of Wavelet basis.\n\n\n\n\n\n\nFigure 8: Workflow to get from the original image to the wavelet transformed version.\n\n\n\nIn short, we combine the vertical and horizontal rescaled feature images, as we are most interested in edge detection for this example.\nNote that the image has now only a quarter of pixels as the original image.\nFor a more detailed explanation see @#appendix-dvc.\n\n\nShow code required for the transformation.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\n\nimport pywt\nimport math\n\ndef img2wave(data):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\n\n\n\n\nLet us try how this changes our first four modes.\n\nShow the code for the figures\nCD_w = img2wave(CD)\nU_w, S_w, VT_w = np.linalg.svd(CD_w - np.mean(CD_w), full_matrices=False)\nl = math.isqrt(CD_w[:, 0].shape[0])\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U_w[:, j], (l, l)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 9: First four modes of the PCA of the 160 images in our wavelet basis.\n\n\n\nRight away we can see that the ears and eyes of the cats show up more pronounced in the second mode Figure 9 (b).\nNow how does this influence our classification problem? For this let us first see how easy it is to distinguish the two classes in a scatter plot for the first three modes.\n\nShow the code for the figure\nimport seaborn as sns\nimport pandas as pd\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\ntarget = [\"cat\"] * 80 + [\"dog\"] * 80\nC = U[:, :4].T @ (CD - np.mean(CD))\ndf = pd.DataFrame(C.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf[\"target\"] = target\n\nC_w = U_w[:, :4].T @ (CD_w - np.mean(CD))\ndf_w = pd.DataFrame(C_w.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf_w[\"target\"] = target\n\n_ = sns.pairplot(df, hue=\"target\", height=1.45)\n_ = sns.pairplot(df_w, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\n\n\nFigure 10: First four modes of the raw images for cats and dogs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: First four modes of the wavelet images for cats and dogs.\n\n\n\n\n\n\nThe two figures above give us a good idea what the different principal values can tell us. It is quite clear, that the first mode is not of much use for differentiation of the two classes. In general the second mode seems to work best (the ears), in contrast to the raw image we can also see that the wavelet basis is slightly better for the separation.\n\n\nShow the code for the figures\nimport plotly.express as px\n\n\nfig = px.scatter_3d(df, x='PV2', y='PV3', z='PV4', color=\"target\")\nfig.show()\nfig = px.scatter_3d(df_w, x=\"PV2\", y=\"PV3\", z=\"PV4\", color=\"target\")\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 12: Scatter plots of modes 2 to 4 for the raw images.\n\n\n\n\n\n\n                                                \n\n\nFigure 13: Scatter plots of modes 2 to 4 for the wavelet images.\n\n\n\n\nNow that we have an idea what we are dealing with we can start looking into the two previously described paths in machine learning. The difference between supervised and unsupervised learning can be summarized by the following two images.\n\nShow the code for the figure\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(df.iloc[:, 0][index], df.iloc[:, 2][index], label=name)\n\nplt.figure()\nplt.scatter(df.iloc[:, 0], df.iloc[:, 2], color=\"gray\")\n\n\n\n\n\n\n\n\n\n\nFigure 14: Labelled data for supervised learning.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Unlabelled data for unsupervised learning.\n\n\n\n\n\n\nYou either provide/have labels or not.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#footnotes",
    "href": "clustering/index.html#footnotes",
    "title": "Clustering and Classification",
    "section": "",
    "text": "To add it to pdm us pdm add scikit-learn.↩︎",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html",
    "href": "clustering/unsupervised.html",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "1.1 \\(k\\)-Means Clustering\nWe start with unsupervised learning. The goal of unsupervised learning is to discover clusters in the data that has no labels. There are several algorithms to perform this task, the most prominent is the \\(k\\)-means clustering algorithm.\nThe \\(k\\)-means algorithm tries to partition a set of \\(m\\) (vector-valued) data observations into \\(k\\) clusters. Where in general the dimension of the data and the amount of observations is known, the number of clusters is often not known a priori.\nThe general idea is, to label each observation as belonging to a cluster with the nearest mean (the representative of the cluster). The resulting clusters are called Voronoi cells, see Wikipedia - Voronoi diagram.\nTo illustrate the proceedings with the help of some artificial data and its clustering.\nIn Figure 1.1 (a) we see the two distinct clusters and the initial guesses for th centers. In the successive plots we see how the centers move and converge to the final position as seen in Figure 1.1 (d). In this case the algorithm converges after the sixth step.\nOf course the algorithm is sensitive to the initial guess and therefore modern versions provide strategies to determine the initial guess as well as the number of clusters.\nWe can see this in action in the sklearn.cluster.KMeans version.\nAs can be seen in Figure 1.2 (a) the algorithm comes up with the same split between the two sets. If we try for three clusters Figure 1.2 (b) the result is sensible as well.\nThe major success, of this algorithm in general use, is based on the fast convergence, that requires no supervision. We can also see that it is not very accurate, compare Figure 1.1 (a) and Figure 1.1 (d). This is no surprise, as the algorithm has not all information available.\nHow can we determine how accurate the algorithm is? If we have no labels this is of course not easy to do but cross-validation is a good tool.\nIn our case we can produce the labels and we can also split the data beforehand into a training set and a test set. Usually a so called \\(80:20\\) split is used, i.e. \\(80\\%\\) training data and \\(20\\%\\) test data.\nShow the code for the figure\nfrom sklearn.cluster import KMeans\nnp.random.seed(6020)\n\n# Shuffle data\nX_shuffle = X[np.random.permutation(X.shape[0]), :]\nX2_shuffle = X2[np.random.permutation(X2.shape[0]), :]\n\n# Split data into two parts\nsplit = n // 5 * 4\ndata = np.concatenate((X_shuffle[:split, :], X2_shuffle[:split, :]))\ntest = np.concatenate((X_shuffle[split:, :], X2_shuffle[split:, :]))\n\n# Create clustesr\nkmeans = KMeans(n_clusters=2, random_state=6020).fit(data)\nclasses = kmeans.predict(data)\ntest_classes = kmeans.predict(test)\n\n# Find wrong classifications\nerror = test_classes != np.concatenate((np.zeros(n-split), np.ones(n-split)))\n\n# Plotting\nplt.figure()\ncolour = [\"tab:orange\", \"tab:blue\"]\nfor i in range(2):\n    plt.scatter(data[classes==i, 0], data[classes==i, 1],\n                c=colour[i], alpha=0.5, label=\"train\")\n    plt.scatter(test[test_classes==i, 0], test[test_classes==i, 1],\n                c=colour[i], label=\"test\")\n\nplt.scatter(test[error, 0], test[error, 1], marker=\"x\", c=\"k\", label=\"error\")\nplt.gca().set_aspect(1)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.3: Validation against a test set.\nThe result of this can be seen in Figure 1.3 where we have two points wrongly classified to the opposite cluster.\nThere exist several extensions of the basic \\(k\\)-means algorithm to improve the results and overall performance. Two such versions are the accelerated \\(k\\)-means, as well as mini-batch \\(k\\)-means. Both can be found in Geron (2022).",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#sec-clustering-usl-kmeans",
    "href": "clustering/unsupervised.html#sec-clustering-usl-kmeans",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "Definition 1.1 (\\(k\\)-Means Algorithm) For a given set of \\(m\\) observations \\((x_1, x_2, \\ldots, x_m)\\), with \\(x_i\\in \\mathrm{R}^n\\) the algorithm strives to find \\(k\\) sets \\((S_1, S_2, \\ldots, S_k)\\) such that the variance inside the cluster is minimized, i.e. \\[\n\\underset{S}{\\operatorname{argmin}} \\sum_{i=1}^k \\sum_{x\\in S_i}\\| x - \\mu_i \\|^2,\n\\] where \\(\\mu_i\\) denotes the mean of \\(S_i\\).\nThe algorithm itself is recursive for a given \\(k\\)\n\nRandomly initialize \\(k\\) points \\(\\mu_1, \\mu_2, \\ldots, \\mu_k\\), as the cluster centers.\nLabel each observation \\(x_i\\) by the nearest cluster center \\(\\mu_j\\), all points with the same label form the set \\(S_j\\).\nCompute the mean of each cluster \\(S_j\\) and replace the current \\(\\mu_j\\) by it.\nRepeat until the cluster centers stay stable up to some tolerance.\n\nThis algorithm was first introduced in Lloyd (1982) and is therefore often called Lloyd algorithm.\n\n\nShow the code for Lloyd algorithm\ndef lloyd(data, centers, steps):\n    classes = np.zeros(data.shape[0])\n    centers_ = np.copy(centers)\n    for i in range(steps):\n        for j in range(data.shape[0]):\n            classes[j] = np.argmin(np.linalg.norm(centers_ - np.tile(data[j, :],\n                                   (centers.shape[0], 1)), axis=1))\n        for j in range(centers.shape[0]):\n            centers_[j, :] = np.mean(data[classes == j, :], axis=0)\n        \n    return (classes, centers_)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn general Definition 1.1 is NP-hard and therefore computationally not viable. Nevertheless, a number of heuristic algorithms exist to provide a good performance, despite having no guarantee for global convergence.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.5 - 5.6). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\n# Helper for plotting\ndef plot_lloyd(data, centers, classes, ms=500):\n    plt.figure()\n    for i in range(centers.shape[0]):\n        plt.scatter(data[classes==i, 0], data[classes==i, 1])\n        plt.scatter(centers[i, 0], centers[i, 1], c=\"k\", s=ms, marker=\"*\")\n    plt.gca().set_aspect(1)\n\n# Create data for illustration\nn = 200\n# Random ellipse centred in (0, 0) and axis (1, 0.5)\nX = np.random.randn(n, 2) * np.array([1, 0.5])\n# Random ellipse centred in (1, -2) and axis (1, 0.2)\nX2 = (np.random.randn(n, 2) * np.array([1, 0.2])) + np.array([1, -2])\n# Rotate ellipse 2 by theta\ntheta = np.pi / 4\nX2 = X2 @ np.array([[np.cos(theta), -np.sin(theta)],\n                  [np.sin(theta), np.cos(theta)]] )\n\ncenters = np.array([[0., -1.], [-1., 0.]])\ndata = np.concatenate((X, X2))\n# Plot initial step with theoretical assignment\nclasses = np.concatenate((np.ones(X2.shape[0]), np.zeros(X.shape[0])))\nplot_lloyd(data, centers, classes)\n\n# Compute and plot consecutive steps of lloyds algorithm\nfor i in [1, 2, 3]:\n    classes, centers_ = lloyd(data, centers, i)\n    plot_lloyd(data, centers, classes)\n    centers = centers_\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The original two classes and the initial guesses for the centers.\n\n\n\n\n\n\n\n\n\n\n\n(b) Point association to the clusters in the first step.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Point association to the clusters in the third step.\n\n\n\n\n\n\n\n\n\n\n\n(d) Point association to the clusters in the sixth step.\n\n\n\n\n\n\n\nFigure 1.1: Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\n\n\n\n\nShow the code for the figure\nfrom sklearn.cluster import KMeans\nnp.random.seed(6020)\n\nkmeans = KMeans(n_clusters=2, random_state=6020).fit(data)\nplot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)\nkmeans = KMeans(n_clusters=3, random_state=6020).fit(data)\nplot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Seeking two clusters.\n\n\n\n\n\n\n\n\n\n\n\n(b) Seeking three clusters.\n\n\n\n\n\n\n\nFigure 1.2: KMeans algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.1 (Apply the \\(k\\)-means algorithm to the Iris dataset) As an exercise to get some practice for using \\(k\\)-means apply the algorithm to the Iris data set to find a potential split for the three classes of flowers.\nTry with only two dimensional data and with all four dimensions.\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 Applications of \\(k\\)-means\nIn addition to the image segmentation illustrated above, the \\(k\\)-means algorithm is used in a multitude of applications, we list some here:\n\nImage segmentation: To decompose an image into different regions we can use \\(k\\)-means. Applications range from robotics to surveillance where, where one or more objects are separated from the rest of the image.\nCustomer segmentation/social network analysis: To segment customers along e.g. their purchase history/behaviour, preferences, demographic data, amount spent, search queries, social media interactions, etc. \\(k\\)-means is used in marketing, retail, and advertising to personalize the experience.\nText clustering: In natural language processing (NLP) \\(k\\)-means is often used to cluster similar documents or text to make analysing large volumes of text data feasible.\nFraud detection: \\(k\\)-means is a crude tool for fraud detection in finance and banking. Transactions are clustered according to similarities and anomalies are detected. There exist more sophisticated methods in finance.\nAnomaly detection: In medical (image) data \\(k\\)-means is often used to detect anomalies by finding points that fall outside of clusters. The same works for cybersecurity and e.g. network traffic.\nRecommendation systems: By grouping users together it is easier to recommend them new songs, items for shopping and more.\nQuality control: By grouping similar products you can detect quality issues and defects in production processes. If an issue is found the part can be checked and the process adjusted.\nTraffic Analysis: In transport and logistics you can analyze traffic patterns and use the information for optimization and similar trips, routes and vehicles.\n\nWe want to highlight image segmentation as an application of \\(k\\)-means. The example found here is first shown in Geron (2022).\nThe idea of image segmentation is to decompose an image into different segments. The following variants exist:\n\nColour segmentation - pixels with similar colour get assigned the same cluster. An application of this is image satellite postprocessing to find areas of forest or sea, or finding object in robotics applications, and organs in medical images.\nSemantic segmentation - all pixels that are part of the same object get assigned to the same segment. An application of this is in autonomous driving to find pedestrians - one segment containing all pedestrians.\nInstance segmentation - similar to semantic segmentation but individual objects are assigned to the same segment. In the autonomous driving application we would find individual pedestrians.\n\nHere we show how to perform colour segmentation with \\(k\\)-means.\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from Geron (2022), see GitHub,\n\n\n\nShow the code for the figure\nimport numpy as np\nimport imageio.v3 as iio\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nim = np.asarray(iio.imread(\n        \"https://github.com/ageron/handson-ml3/blob/main/images/\"\n        \"unsupervised_learning/ladybug.png?raw=true\"))\nplt.figure()\nplt.imshow(im)\nplt.axis(\"off\")\n\nZ = im.reshape(-1, 3)\nfor colours in [8, 4, 2]:\n    kmeans = KMeans(n_clusters=colours, n_init=10, random_state=6020).fit(Z)\n    plt.figure()\n    plt.imshow(kmeans.cluster_centers_[kmeans.labels_].reshape(im.shape) / 255)\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Segmentation by 8 colours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Segmentation by 4 colours.\n\n\n\n\n\n\n\n\n\n\n\n(d) Segmentation by 2 colours.\n\n\n\n\n\n\n\nFigure 1.4: Colour segmentation for the image of a lady bug.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#unsupervised-hierarchical-clustering---dendrogram",
    "href": "clustering/unsupervised.html#unsupervised-hierarchical-clustering---dendrogram",
    "title": "1  Unsupervised learning",
    "section": "1.2 Unsupervised hierarchical clustering - Dendrogram",
    "text": "1.2 Unsupervised hierarchical clustering - Dendrogram\nSimilar to \\(k\\)-means a simple hierarchical algorithm is used to create a dendrogram. The resulting tree allows you to easily see if data is clustered without the need of labeling or supervision.\nWe follow the example and discussion given in (Brunton and Kutz 2022, sec. 5.4).\nThere are two main approaches in creating the desired hierarchy, bottom-up often called agglomerative and top-down often called divisive.\n\nFor the agglomerative approach each observation \\(x_i\\) is initially its own cluster and in each step points are combined according to their distance. Eventually everything ends up in a single cluster and the algorithm stops.\nFor the divisive approach we go the opposite direction and start with the super cluster containing all observations §x_i$ and we gradually split up into smaller and smaller clusters. The algorithm stops, when each observation is its own leave.\n\nOf course the norm1 used has quite an influence as can be seen in (Kandolf 2025, sec. 7.1) Link where we compared LASSO and RIDGE algorithms for our optimization problem.\nTo illustrate the agglomerative approach and the difference in norms we use four points and construct the hierarchy by combining the two closest points.\n\n\n\n\n\n\n\n\n\n(a) Use two norm or euclidean norm to compute the distance - \\(\\| \\cdot \\|_2\\)\n\n\n\n\n\n\n\n\n\n(b) Use two one or cityblock norm to compute the distance - \\(\\| \\cdot \\|_1\\)\n\n\n\n\n\nFigure 1.5: We always combine the two nearest points and replace them with the point in the middle. This iteration continues until only one point is left. The Dendrogram is build according to the order of points chosen.\n\n\n\nOn a larger scale, with always the first 25 points of the two clusters above we get the results shown in Figure 1.6\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster import hierarchy\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nXX = np.concatenate((X[:25, :], X2[:25, :]))\nXX.shape\nfor metric in [\"euclidean\", \"cityblock\"]:\n    plt.figure()\n    Y = pdist(XX, metric=metric)\n    Z = hierarchy.linkage(Y, method=\"average\", metric=metric)\n    thresh = 0.90 * np.max(Z[:, 2])\n    dn = hierarchy.dendrogram(Z, p=100, color_threshold=thresh)\n\n    plt.figure()\n    plt.bar(range(XX.shape[0]), dn[\"leaves\"])\n    plt.plot(np.array([0, XX.shape[0]]),\n             np.array([XX.shape[0] // 2, XX.shape[0] // 2]),\n             \"r:\", linewidth=2)\n    plt.plot(np.array([(XX.shape[0] // 2) + 1/2, (XX.shape[0] // 2) + 1/2]),\n             np.array([0, XX.shape[0]]),\n             'r:', linewidth=2)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Dendrogram for euclidean norm.\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram showing the clustering for the euclidean norm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Dendrogram for cityblock norm.\n\n\n\n\n\n\n\n\n\n\n\n(d) Histogram showing the clustering for the cityblock norm.\n\n\n\n\n\n\n\nFigure 1.6: Construction a agglomerative hierarchy for our data set.\n\n\n\nThe two dendrograms (Figure 1.6 (a) Figure 1.6 (c)) show the hierarchical structure derived from the data set. The number of clusters can be influenced by the thresh parameter and it is also used to label the observation accordingly. It is quite similar to the number of clusters \\(k\\) in the \\(k\\)-means algorithm.\nThe two bar graphs on the right (Figure 1.6 (b) Figure 1.6 (d)) shows how the data is clustered in the dendrogram. The bars correspond to the distance metric produced by the algorithm. The red lines indicate the region of a perfect split for the separation if the algorithm works perfectly and places every point in the original cluster. If we recall, that the first 25 points are from set 1 and the next 25 from set 2 we can see that the euclidean distance generates the perfect split. On the other hand, the cityblock norm is placing one point in the wrong cluster.\n\n\n\n\n\n\nNote\n\n\n\nThe function scipy.cluste.hierarchy.linkage allows to specify the method of computing the distance between two clusters. The used average corresponds to unweighted pair group method with arithmetic mean UPGMA algorithm.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#sec-clustering-ul-dbscan",
    "href": "clustering/unsupervised.html#sec-clustering-ul-dbscan",
    "title": "1  Unsupervised learning",
    "section": "1.3 Density-based spatial clustering of applications with noise (DBSCAN)",
    "text": "1.3 Density-based spatial clustering of applications with noise (DBSCAN)\nThe algorithm was introduced in Ester et al. (1996) and is an algorithm that finds areas of high density. The main idea behind the algorithm is as follows.\nOur observations (we will call them points here for consistency) form the basis and they can be in any space but as for all the algorithms presented here, there needs to exist some form to measure distance. Furthermore, DBSCAN relies on two parameters, \\(\\epsilon\\) describing the radius of neighbourhood of a point and \\(minPts\\) the minimum of points needed to form a cluster. With these parameters we perform the following steps:\n\nA point \\(p\\) is called a core point if at least \\(minPts\\) (including \\(p\\)) are within distance \\(\\epsilon\\) of \\(p\\). This region is called the \\(\\epsilon\\)-neighbourhood of \\(p\\).\nA point \\(q\\) is called directly reachable form an core point \\(p\\) iff2 \\(q\\) is in the \\(\\epsilon\\)-neighbourhood of \\(p\\).\nA point \\(p\\) is reachable from \\(p\\) if there exists a sequence of points (path) \\(p=p_1, p_2,  \\ldots, p_n=q\\) where each \\(p_{i+1}\\) is directly reachable form \\(p_i\\). Consequently, all point on the path, except \\(p\\), need to be core points.\nA point that is not reachable from any other point is considered an outlier or noise point.\n\nTherefore, we get our definition of a cluster: a point core point \\(p\\) together with all points that are reachable from \\(p\\). This includes core points (at least one) and none-core points (boundary points) on the boundary as they can not be used to reach more points.\nIn contrast to \\(k\\)-means the algorithm we can find none linear clusters. We can illustrate this with the moon data set.\n\n\n\n\n\n\nImportant\n\n\n\nFor the plotting function please see GitHub, the accompanying repository to Geron (2022), as it forms the basis.\n\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef plot_dbscan(dbscan, X, size):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    # Plotting the clusters\n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask],\n                marker=\"o\", s=size, cmap=\"Paired\")\n    # Plotting the core points of the clusters\n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask],\n                marker=\"*\", s=20,)\n    # Plotting the anomalies\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    # Plotting the boundary points\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n    plt.gca().set_aspect(1)\n\nnp.random.seed(6020)\nM, M_y = make_moons(n_samples=1000, noise=0.05, random_state=6020)\n\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(M)\nplt.figure()\nplot_dbscan(dbscan, M, size=100)\n\ndbscan = DBSCAN(eps=0.2, min_samples=5)\ndbscan.fit(M)\nplt.figure()\nplot_dbscan(dbscan, M, size=600)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scikit-Learn’s DBSCAN with eps=0.05 and min_samples=5.\n\n\n\n\n\n\n\n\n\n\n\n(b) Scikit-Learn’s DBSCAN with eps=0.2 and min_samples=5.\n\n\n\n\n\n\n\nFigure 1.7: DBSCAN illustrated with the moons dataset.\n\n\n\nThe algorithm works best for clusters well separated by low density regions. As all algorithms depending on distance measures it suffers from the curse of dimensionality3.\nAs can be seen in Figure 1.7 (a) the algorithm produces quite a lot of clusters for a small \\(\\epsilon=0.05\\). In total we get 10 clusters and quite a lot of outliers that are not all easy to retract by the naked eye.\nNevertheless, if we increase \\(\\epsilon=0.2\\) we can see that the two clusters are neatly reproduced, Figure 1.7 (b).\n\n\n\n\n\n\nNote\n\n\n\nThe DBSCAN class in Scikit-Learn does not provide a .predict() method like many other such classes. See Geron (2022) and GitHub for how to train a \\(k\\)-nearest neighbour (kNN) to perform this task.\n\n\n\n\n\n\n\n\n\nExercise 1.2 (Application of DBSCAN)  \n\nApply the DBSCAN algorithm to our toy example used through this section to recover the two clusters as good as possible. Try different \\(\\epsilon\\) values as well as different norms (metric parameter).\nAdditionally, for a higher dimensional problem, using DBSCAN split the Iris dataset into its three classes of flowers.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#finite-mixtures-models",
    "href": "clustering/unsupervised.html#finite-mixtures-models",
    "title": "1  Unsupervised learning",
    "section": "1.4 Finite Mixtures Models",
    "text": "1.4 Finite Mixtures Models\nFinite mixture models assume that the observations \\(x_i\\) are mixtures of \\(k\\) processes which combine in the measurement. Each mixture is defined via a probability model with unknown parameters. The aim of the algorithm is to find the parameters such that the \\(k\\) processes best describe the \\(x_i\\) observations. The basis of the algorithm is the so called expectation-maximization (EM) algorithm of Dempster, Laird, and Rubin (1977), that is designed to find the maximum-likelihood parameters of the defined statistical models.\nThe most common choice for the probability models is the Gaussian distribution4 and for this choice, the method is better known as Gaussian mixture models (GMM). This is often quite a sensible choice if nothing more is known about the observations and a Gaussian distribution is than always a save bet. As a result each process can be described by two parameters, mean and variance. Each cluster in turn, will be described by an ellipsoidal shape where size, density, orientation and semi-axis vary.\nLike \\(k\\)-means, in the simplest version, GMM is provided with the number of clusters \\(k\\) and starts from an initial guess of the means and corresponding variances. The parameters are than iteratively updated to find a (local) maximum. There are some problems with this approach, e.g. if you set one of the processes to have zero variance and mean equal to a point. To avoid such problems cross-validation can often help to avoid problems stemming from an initialization with a poor initial guess.\nAs mentioned, the main idea of the mixture models is that the observations are a linear combination of probability density functions (PDF) \\[\nf(x_j, \\Theta) = \\sum_{p=1}^k \\alpha_p f_p(x_j, \\Theta_p),\n\\] where \\(f\\) denotes the observed/measured PDF with parameters \\(\\Theta\\), \\(f_p\\) the PDF of mixture \\(p\\) with parameters \\(\\Theta_p\\), and \\(k\\) denotes the number of mixtures. The weights \\(\\alpha_p\\) fulfil \\(\\sum_p \\alpha_p = 1\\).\nOverall we therefore can formulate the algorithm as:\n\n\n\n\n\n\n\nDefinition 1.2 (Mixture Models Algorithm) Given the observed PDF \\(f(x_j, \\Theta)\\), estimate the mixture weights \\(\\alpha_p\\) and the parameters fo the distribution \\(\\Theta_p\\).\n(see Brunton and Kutz 2022)\n\n\n\n\nLets follow the derivative for the GMM method in (Brunton and Kutz 2022, sec. 5.5) to bet a better idea what is happening.\nFor the GMM \\(f\\) becomes \\[\nf(x_j, \\Theta) = \\sum_{p=1}^k \\alpha_p \\mathcal{N}_p(x_j, \\mu_p, \\sigma_p)\n\\] and for a given \\(k\\) we need to find \\(\\alpha_p, \\mu_p, \\sigma_p\\). We get \\(\\Theta\\) from the roots of \\[\nL(\\Theta) = \\sum_{j=1}^n\\log f(x_j | \\Theta),\n\\] \\(L\\) is called the log-likelihood function and we sum over all observations \\(x_j\\). We transform it into a optimization problem by setting the derivative to zero \\[\n\\frac{\\partial L(\\Theta)}{\\partial \\Theta} = 0\n\\] and solve it via the EM algorithm. As the name suggests, there are two steps E and M.\nThe E step uses the following posterior to establish memberships. Therefore, we start of by assuming an initial guess of the vector \\(\\Theta\\) and this leads to the posterior probability of distribution \\(p\\) of \\(x_j\\)  \\[\n\\tau_p(x_j, \\Theta) = \\frac{\\alpha_p f_p(x_j, \\Theta_p)}{f(x_j, \\Theta)}.\n\\] In other words, we try to figure out if \\(x_j\\) is part of the \\(p\\)th mixture. For the GMM this becomes, in the \\(l\\) iteration: \\[\n\\tau_p^{(l)}(x_j) = \\frac{\\alpha_p^{(l)} \\mathcal{N}_p(x_j, \\mu_p^{(l)}, \\sigma_p^{(l)})}{f(x_j, \\Theta^{(l)})}.\n\\]\nNow the M step starts to update the parameters and mixture weights as \\[\n\\begin{align}\n\\alpha_p^{(l+1)} &= \\frac1n \\sum_{j=1}^n \\tau_p^{(l)}(x_j), \\\\\n\\mu_p^{(l+1)} &= \\frac{\\sum_{j=1}^n x_j\\tau_p^{(l)}(x_j)}{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)}, \\\\\n\\Sigma_p^{(l+1)} &= \\frac{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)(x_j - \\mu_p^{(l+1)})(x_j - \\mu_p^{(l+1)})^\\mathrm{T}}{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)},\n\\end{align}\n\\] with \\(\\Sigma_p^{(l+1)}\\) denotes the covariance matrix. The algorithm now alternates between E and M until convergence is reached.\nLet us try it with the cats and dogs images in wavelet basis from Section 1.2 by fitting to the second and forth principal value of the SVD. Note, we directly load the images in wavelet basis and do not recompute them.\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.8). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.mixture import GaussianMixture\nfrom scipy.stats import multivariate_normal\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\nCD = np.concatenate((dogs, cats), axis=1)\nU,S,VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\nv = VT.T\n\ndogcat = v[:,(1,3)]\nGMModel = GaussianMixture(n_components=2, n_init=10).fit(dogcat)\n\nplt.figure()\nplt.scatter(v[:80,1], v[:80,3], label=\"dogs\")\nplt.scatter(v[80:,1], v[80:,3], label=\"cats\")\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\n\nx = np.linspace(-0.15, 0.25)\ny = np.linspace(-0.25, 0.2)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = np.exp(GMModel.score_samples(XX))\nZ = Z.reshape(X.shape)\n\nCS = plt.contour(X, Y, Z,\n                levels=np.arange(3,32,4), colors='k', linestyles='solid')\nplt.legend()\nplt.gca().set_aspect(1)\n\nplt.figure()\nax = plt.axes(projection='3d')\nfor i in range(GMModel.weights_.shape[0]):\n    rv = multivariate_normal(GMModel.means_[i], GMModel.covariances_[i])\n    z = GMModel.weights_[i] * rv.pdf(np.dstack((X, Y)))\n    ax.plot_surface(X, Y, z, alpha=0.5)\n\nax.plot_wireframe(X, Y, Z, color='black', rstride=5, cstride=5)\nax.view_init(30, -80)\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\nplt.legend([r\"$\\mathcal{N}(\\mu_1, \\sigma_1)$\",\n            r\"$\\mathcal{N}(\\mu_2, \\sigma_2)$\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) We can see a nice split along the tow animals via the Gaussian distributions.\n\n\n\n\n\n\n\n\n\n\n\n(b) The PDFs of the fitted distributions, weighted via the GMM algorithm.\n\n\n\n\n\n\n\nFigure 1.8: Fitting of the second and fourth principal value of the cats and dogs images in wavelet basis via the GMM method.\n\n\n\nAs can be seen in Figure 1.8 (a) the two clusters are quite well captured by the GMM algorithm. The contour shows the linear combination of the two Gaussians. We see the two Gaussians on the right in Figure 1.8 (b) where we use the same colours as for the cats and dogs respectively. Note that the distributions are weight \\(\\alpha_p\\) from GMM but other than that there is no unit in the z direction.\nAs discussed above the algorithm is prone to problems during the initialization, therefore we use n_init = 10 to initialize the algorithm 10 times and only keep the best result.\nWe can draw new samples from the resulting object .sample() and we can also fit data to it and we can also perform hard and soft clustering. For hard clustering the model assigns each instance to the most likely cluster (.predict()), where for soft clustering (.predict_proba()) we get the probability of membership for each cluster.\nIf the algorithm struggles to recover the clusters, we can halp it by imposing the shape of the cluster via restricting the covariance matrix \\(\\Sigma_p\\). This can be done via the parameter covariance_type, see docs.\n\n\n\n\n\n\n\nExercise 1.3 (Application of GMM)  \n\nApply the GMM algorithm to our toy example used through this section to recover the two clusters as good as possible. This should be straight forward, as we constructed it via Gaussian distributions. Try constraining the algorithm by imposing the different values possible for covariance_type.\nAdditionally, for a higher dimensional problem, using DBSCAN split the Iris dataset into its three classes of flowers.\nTry to use GMM to get clusters for the two moon dataset used in Section 1.3. Also try with the BayesianGaussianMixture class, where no amount of clusters needs to be specified.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGMM can also be used for anomaly detection. Simply assume every observation located in a low density region an anomaly. For this the threshold needs to be defined in accordance to your expected anomalies.\nE.g. to get about \\(4\\%\\) anomalies you can use the following snippet togethere with the above code:\n\n\nShow the code for the figure\nS = GMModel.score_samples(dogcat)\ndensity_threshold = np.percentile(S, 2)\nanomalies = dogcat[S &lt; density_threshold]\nplt.figure()\nplt.scatter(dogcat[:80,0], dogcat[:80,1], alpha=0.5, label=\"dogs\")\nplt.scatter(dogcat[80:,0], dogcat[80:,1], alpha=0.5, label=\"cats\")\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color=\"k\", marker=\"x\",\n            label=\"anomaly\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.9: Fitting of the second and fourth principal value of the cats and dogs images in wavelet basis via the GMM method.\n\n\n\n\n\nsee Geron (2022) and the corresponding GitHub repository",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#motivational-example",
    "href": "clustering/unsupervised.html#motivational-example",
    "title": "1  Unsupervised learning",
    "section": "1.5 Motivational example",
    "text": "1.5 Motivational example\nBefore we make the leap to supervised learning we we introduce a new data set and see how our unsupervised approach works in this case.\nWe do this with the (in)famous MNIST data set of hand written digits. Lets start by looking at the data set and establishing some basic properties.\nWe load the data set and look at its description\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)\nprint(mnist.DESCR)\n\n**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n**Please cite**:  \n\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n\nDownloaded from openml.org.\n\n\nThe images are stored in .data and the label in .target. We can look at the first 100 digits\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nmnist = fetch_openml('mnist_784', as_frame=False)\n\nim = np.zeros((10 * 28, 10 * 28), int)\nfor i in range(10):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = mnist.data[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.10: First 100 digits from the MNIST data set.\n\n\n\n\n\nAs can be seen from Figure 1.10, most digits are easy to distinguish for us but there are some nasty things included. Especially, there are \\(1\\)s with the english style (just one straight line) of writing (1st row, 7th column), but also as a diagonal line (1st row, 4th column), and finally also the german style (connecting line at base and top) of writing (3rd row, 5th column).\nAlso the two digit comes in various forms, with loop (1st row, 6th column) and without (3rd row, 6th column). Finally we all know that a \\(3\\) can hide in an \\(8\\), same as a \\(9\\), a \\(1\\) can hide in a \\(7\\) and much more.\nNow, before we go any further in the investigation we split up the data set into a training and a test set, we will not use this here but we should establish some way of working right from the start.\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\nNow let us try with \\(k\\)-means to find the ten digits by finding \\(10\\) clusters.\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nk = 10\nkmeans = KMeans(n_clusters=k, n_init=1, random_state=6020).fit(X_train)\n\nim = np.zeros((1 * 28, 10 * 28), int)\nfor i in range(1):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = \\\n                kmeans.cluster_centers_[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.11: The cluster means of \\(k\\)-means for 10 clusters.\n\n\n\n\n\nAs can be seen from Figure 1.11, the cluster centers do not recover our ten digits. In general they are, as expected smudged as the do not represent an actual image. It looks like \\(0, 1, 2, 3, 6, 8, 9\\) are easy to discover but \\(4, 5, 7\\) not. If we look closely, we can see \\(4, 5, 7\\) represented in our clusters but not as separate digits. Obviously this is not a good way to proceed to find our digits. We will discuss methods to perform this task in the next section Chapter 2. Furthermore, keep this part in mind as we will come back to how this can still be helpful in a different aspect as semi supervised learning ?sec-clustering-ssl.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B (Methodological) 39 (1): 1–22.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. “A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.” In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 226–31. KDD’96. Portland, Oregon: AAAI Press.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.\n\n\nLloyd, Stuart P. 1982. “Least Squares Quantization in PCM.” IEEE Trans. Inf. Theory 28 (2): 129–36. https://doi.org/10.1109/TIT.1982.1056489.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#footnotes",
    "href": "clustering/unsupervised.html#footnotes",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "see Kandolf (2025), Section 1.2 or follow the direct link↩︎\nshort for if and only if↩︎\nProblems occurring for higher dimensional data and distance measures, see WikiPedia↩︎\nsee Kandolf (2025), Example 14.6 or for a direct Link↩︎",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html",
    "href": "clustering/supervised.html",
    "title": "2  Supervised learning",
    "section": "",
    "text": "If we recall Fisher (1936) from the Iris data set, we can also find one of the first supervised learning methods in this paper. The introduced linear discriminant analysis (LDA) has survived over time and is still one of the standard techniques for classification, even though we use a more generalized and improved method nowadays.\nThe idea if LDA is to find a linear combination of features that optimally separates two or more classes. The crucial part in the algorithm is that it is guided by labelled data. At its core, the algorithm aims to solve an optimization problem: find an optimal low-dimensional embedding of the data that shows a clear separation between their point distribution, or maximize the distance between the inter-class data and minimize the intra-class data distance.\nThe main idea of LDA is to use projection. For a two-class LDA this becomes \\[\nw = \\operatorname{arg\\, max}_w \\frac{w^\\mathrm{T}S_B w}{w^\\mathrm{T}S_W w},\n\\tag{2.1}\\] (the generalized Rayleigh quotient) where \\(w\\) is our thought after projection and the two scatter matrices \\[\nS_B = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^\\mathrm{T}\n\\] for between-class relation as well as \\[\nS_W = \\sum_{j=1}^2 \\sum_{x\\in\\mathcal{D}_j} (x - \\mu_j)(x - \\mu_j)^\\mathrm{T}\n\\] for within-class data. The set \\(\\mathcal{D}_j\\) denotes the subdomain of the data associated with cluster \\(j\\). The two matrices measure the variance of the data set as well as the means. To solve Equation 2.1 we need to solve the generalized eigenvalue problem1 \\[\nS_B w = \\lambda S_w w\n\\] where the maximal eigenvalue and the corresponding eigenvector are our solution.\nWe try this with the cats and dogs data set in both basis.\nIf we use our raw data set for the classification we get an overall accuracy of 67.5% with \\(\\tfrac{4}{20}\\) wrongly labelled dogs and \\(\\tfrac{9}{20}\\) wrongly labelled cats. We can increase this to an accuracy of 82.5% with \\(\\tfrac{5}{20}\\) wrongly labelled dogs and \\(\\tfrac{2}{20}\\) wrongly labelled cats.\nThis could be expected, see Figure 11 for the separation of the principal values for the two basis.\nOf course we only have very limited data with only 80 images for each of the species. In this case we should do a cross-validation and we have not shuffled the data.\nLet us see how selecting different test and training sets influence the behaviour.\nWith a maximal accuracy of 90.0% and a minimal accuracy of 65.0% our initial result with 82.5% was quite good and above average (77.1%).\nInstead of a linear discriminants, we can also use quadratic discriminants. To show the difference let us look at the classification line of the two methods for our data in wavelet basis\nThe quadratic discriminant classification can be quite beneficial in cases, so be aware of its existence.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#footnotes",
    "href": "clustering/supervised.html#footnotes",
    "title": "2  Supervised learning",
    "section": "",
    "text": "see (Kandolf 2025, Definition 3.5) or the direct link Link↩︎",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "T.B.D.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977.\n“Maximum Likelihood from Incomplete Data via the EM\nAlgorithm.” Journal of the Royal Statistical Society: Series\nB (Methodological) 39 (1): 1–22.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996.\n“A Density-Based Algorithm for Discovering Clusters in Large\nSpatial Databases with Noise.” In Proceedings of the Second\nInternational Conference on Knowledge Discovery and Data Mining,\n226–31. KDD’96. Portland, Oregon: AAAI Press.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic\nProblems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow 3e. 3rd ed. Sebastopol, CA:\nO’Reilly Media.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems.\nSebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen\nDatenbasierter Methoden.” Management Center Innsbruck, Course\nMaterial. https://doi.org/10.5281/zenodo.14671708.\n\n\nLandup, David. 2022. Practical Deep Learning for Computer Vision\nwith Python.\n\n\nLloyd, Stuart P. 1982. “Least Squares Quantization in\nPCM.” IEEE Trans. Inf. Theory\n28 (2): 129–36. https://doi.org/10.1109/TIT.1982.1056489.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendices/explanations.html",
    "href": "appendices/explanations.html",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "A.1 Wavelet decomposition for cats and dogs\nIn Section 1.2 we discuss using the wavelet transformation to transform the image into a different basis. Here are the details of how this is performed with cat zero as example.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(np.reshape(cats[:, 0], (64, 64)).T, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\nFigure A.1: The original cat.\nWe use the Haar-Wavelet and we only need to do one level of transformation. As per usual we get four images, each half the resolution, that represent the decomposition. The images are, a downsampled version of the original image, one highlighting the vertical features, one highlighting the horizontal features, and one highlighting the diagonal features.\nFigure A.2: Wavelet transformation of the cat.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.4\n\n\n\n\n\n\n\n\n\n\n\nFigure A.5\nFor our purposes only the vertical and horizontal feature are of interest and we combine these two images. In order to make sure the features are highlighted optimal we need to rescale the images before the combination. For this we use a similar function like the MATLAB wcodemat function.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\nFigure A.6: Combination of vertical and horizontal features unaltered.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.7: Combination of vertical and horizontal features rescaled.\nIn total this leads to the following function to transform a list of the images, given as row vectors.\nimport pywt\nimport math\n\ndef img2wave(images):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\nNote that the resulting image has only one forth of the pixels as the original image. We can also visualize the transformation steps as follows in Figure A.8.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#appendix-dvc",
    "href": "appendices/explanations.html#appendix-dvc",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "Show the code for the figure\nimport pywt\n\n[A_1, (cH1, cV1, cD1)] = pywt.wavedec2(np.reshape(cats[:, 0], (64, 64)).T,\n                                       wavelet=\"haar\", level=1)\nplt.figure()\nplt.imshow(A_1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cH1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cD1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\nShow the code for the figure\nimport pywt\n\nplt.figure()\nplt.imshow(cH1 + cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(rescale(cH1, 256) + rescale(cV1, 256), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\nFigure A.8: Workflow to get from the original image to the wavelet transformed version.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  }
]