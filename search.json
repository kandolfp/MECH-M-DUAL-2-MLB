[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "",
    "text": "Preface\nThese are the lecture notes for the Maschinelles Lernen in der industriellen Bildverarbeitung class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the summer term 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe want to thank the open source community for providing excellent tutorials and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout the document at hand.\nThese notes are built with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basic concepts behind modern day Data Science techniques for industrial image processing. We will always try to not only discuss the theory but also use Python to illustrate the content programmatically.\nThe plan for these notes is to cover not only the basics of modern methods for machine learning in image processing but also topics in the near vicinity like\n\ndata management, labeling and preprocessing\nlife-cycle management of models\nmachine learning operations (MLOps)\ncurrent and research topics\n\nThese notes are intended for engineering students and therefore the mathematical concepts will rarely include rigorous proofs.\n\n\n\n\n\n\nNote\n\n\n\nThe sklearn package has all the tools to perform a PCA, see Link for an example using the Iris dataset same as in the beginning of this section.\n\n\nThe following books cover large sections of the content and are a good read to get further into the topics:\n\nBrunton and Kutz (2022)\nGeron (2022)\nGoodfellow, Bengio, and Courville (2016)\nLandup (2022)\nHuyen (2022)\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems. Sebastopol, CA: O’Reilly Media.\n\n\nLandup, David. 2022. Practical Deep Learning for Computer Vision with Python.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "clustering/index.html",
    "href": "clustering/index.html",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Feature selection and generation of a feature space\nOne aspect of machine learning is binning data into meaningful and distinct categories that can be used for decision-making, amongst other things. This process is referred to as clustering and classification. To achieve this task the main modus of operation is to find a low-rank feature space that is informative and interpretable.\nOne such way is to extract the dominate features in a data set is for example the singular value decomposition (SVD) or principal component analysis, see (Kandolf 2025). The main idea is to not work in the heigh dimensional measurement space, but instead in the low rank feature space. This allows us to significantly reduce the cost (of computation or learning) by performing our clustering in this low dimensional space.\nIn order to find the feature space there are two main paths for machine learning:\nAs the names suggest, in unsupervised learning, no labels are given to the algorithm and it must find patterns in the data to generate clusters and labels such that predictions can be made. This is often used to discover previously unknown patterns in the (low-rank) subspace of the data and leads to feature engineering or feature extraction. These features can than be used for building a model, e.g. a classifier.\nSupervised learning, on the other hand, uses labelled data sets. The training data is labelled for cross-validation by a teacher or expert. I.e. the input and output for a model is explicitly provided and regression methods are used to find the best model for the given labelled data. There are several different forms of supervised learning like reinforcement learning, semi-supervised learning, or active learning.\nThe main idea behind machine learning is to construct or exploit the intrinsic low-rank feature space of a given data set, how to find these is determined by the algorithm.\nBefore we go into specific algorithms lets have a general look at what we are going to talk about. We will follow (Brunton and Kutz 2022, sec. 5.1) for this introduction.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "href": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Fischer Iris data set\nFirst we look into one of the standard data sets well known in the community, the so called Fischer Iris data set Fisher (1936).\n\n\n\n\n\n\nNote\n\n\n\nLuckily for us there are several data packages available for general use. One of them is the Fisher Iris data set.\n\n\n\n\n\n\nFigure 1: Illustration of the recorded features and structure of the iris dataset from the Medium article - Exploring the Iris flower dataset.\n\n\n\nWe use the possibilities of sklearn 1 to access the dataset in Python.\n\n\nLet us start right away by loading and visualizing the data in a 3D scatter plot (there are 4 features present so one is not shown).\n\n\nShow the code for the figure\nimport plotly.express as px\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nfig = px.scatter_3d(df, x='sepal length (cm)', y='sepal width (cm)',\n                    z='petal width (cm)', color='target')\ncamera = dict(\n    eye=dict(x=2, y=2, z=0.1)\n)\nfig.update_layout(scene_camera=camera)\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Iris data set with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a scatter 3D plot along the features sepal length, sepal width, and petal width, the forth feature petal width is not shown.\n\n\n\n\nAs can be seen in Figure 2 the three features are enough to easily separate Sentosa and to a very good degree Versicolor from Virginica, where the last two have a small overlap in the samples provided.\nThe petal width seems to be the best feature for the classification and no highly sophisticated machine learning algorithms are required. We can see this even better if we use a so called Pair plot from the seaborn package as seen in Figure 3.\n\n\nShow the code for the figure\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\n_ = sns.pairplot(iris.frame, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\nFigure 3: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a pair grid plot. The diagonal shows the univariate distribution of the feature.\n\n\n\n\n\nHere a grid with plots is created where each (numeric) feature is shared across a row and a column. Therefore, in a single plot we can see the combination of two, and in the diagonal we see the univariate distribution of the values. After a close inspection, we can also see here that petal width provides us with the best distinction feature and sepal width with the worst.\nEven though we already have enough to give a good classification for this dataset, it is worth to investigate a bit further. We illustrate how the principal component analysis (PCA)2 can be applied here.\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nA = df.iloc[: , :4].to_numpy()\n\nXavg = A.mean(0)\nB = A - np.tile(Xavg, (150, 1))\nU, S, VT = np.linalg.svd(B, full_matrices=False)\nC = B @ VT[:2, :].T\nq = np.linalg.norm(S[:2])**2 / np.linalg.norm(B, \"fro\")**2\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(C[index, 0], C[index, 1], label=name)\nplt.legend()\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_2$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Feature reduction with PCA for the Iris dataset. We show the first two components principal components.\n\n\n\n\n\nAs can be seen in the Figure 4 the first two components cover about 97.77% of the total variation in the samples and in accordance with these results a very good separation of the three species is possible with them\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 1 (Explained Variance) The explained variance is a measure of how much of the total variance in the data is explained via the principal component. It is equivalent to the singular value associated with the component so in the above example it is nothing else than \\[\n\\rho_i = \\frac{\\sigma_i^2}{\\|\\Sigma\\|^2}.\n\\]\nThe explained variance can be helpful to select the amount of principal values you use for a classification. But be aware, as we will see later, the largest principal value might not be the best to use.\n\n\n\n\n\n\n\n\n\nDimensionality Reduction\n\n\n\nAs we can see with the Fischer Iris data set, we often have a problem of dimensionality, if we try to visualize something. The data has four dimensions but it is very hard for us to perceive, let alone visualize, four dimensions (if the forth dimension is not time).\nTherefore, dimensional reduction techniques are often applied. One of these is PCA others are:\n\nMultidimensional scaling (MDS) - preserve the distance between observations while doing the reduction.\nIsomap - create a graph by connecting each observation to its neighbors and reduce the dimensions by keeping the geodesic distances, i.e. the number of nodes on the graph between observations\n\\(t\\)-distributed stochastic neighbor embedding (\\(t\\)-SNE) - try to reduce dimensions while keeping similar observations close and dissimilar observations apart.\nLinear discriminant analysis (LDA) - projects data onto a hyperplane where the hyperplane is chosen in such a way that it is most discriminative for the observations see 2.1 Linear Discriminant Analysis (LDA).\n\n\n\nNow let us take a look at a more complicated example from Brunton and Kutz (2022).\n\n\nDogs and Cats\nIn this data set we explore how to distinguish between images dogs and cats, more particular the head only. The data set contains \\(80\\) images of dogs and \\(80\\) images of cats, each with \\(64 \\times 64\\) pixels and therefore a total of \\(4096\\) feature measurements per image.\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.2 - 5.4). Also see GitHub.\n\n\nAs before, a general inspection of the data set is always a good idea. With image we usually look at some to get an overview.\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef createImage(data, x, y, width=64, length=64):\n    image = np.zeros((width * x, length * y))\n    counter = 0\n    for i in range(x):\n        for j in range(y):\n            img = np.flipud(np.reshape(data[:, counter], (width, length)))\n            image[length * i: length * (i + 1), width * j: width * (j + 1)] \\\n                    = img.T\n            counter += 1\n    return image\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(createImage(cats, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog\"]\nplt.figure()\nplt.imshow(createImage(dogs, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\nFigure 5: The first 16 cats from the data set.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The first 16 dogs from the data set.\n\n\n\n\n\n\nAgain, we use PCA to reduce the feature space of our data.\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nCD = np.concatenate((dogs, cats), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=False)\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U[:, j], (64, 64)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 7: First four modes of the PCA of the 160 images.\n\n\n\nIn Figure 7 we can see the first four modes of the PCA. While mode 2 (Figure 7 (b)) highlights the pointy ears common in cats, mode 3 (Figure 7 (c)) is recovers more dog like features. Consequently, these two features make a good choice for classification.\nOf course this is not the only possible representation of the initial data. We can also use the Wavelet transform3.\nThe multi resolution analysis features of the Wavelet transformation can be used to transform the image in such a way that the resulting principal components yield a better separation of the two classes.\n\n\n\n\n\n\nNote\n\n\n\nWe use the following workflow to get from the original image \\(A_0\\) to the \\(\\tilde{A}\\) image in a sort of Wavelet basis.\n\n\n\n\n\n\nFigure 8: Workflow to get from the original image to the wavelet transformed version.\n\n\n\nIn short, we combine the vertical and horizontal rescaled feature images, as we are most interested in edge detection for this example.\nNote that the image has now only a quarter of the pixels of the original image.\nFor a more detailed explanation see A.1 Wavelet decomposition for cats and dogs.\n\n\nShow code required for the transformation.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\n\nimport pywt\nimport math\n\ndef img2wave(data):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\n\n\n\n\nLet us try how this changes our first four modes.\n\nShow the code for the figures\nCD_w = img2wave(CD)\nU_w, S_w, VT_w = np.linalg.svd(CD_w - np.mean(CD_w), full_matrices=False)\nl = math.isqrt(CD_w[:, 0].shape[0])\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U_w[:, j], (l, l)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 9: First four modes of the PCA of the 160 images in our wavelet basis.\n\n\n\nRight away we can see that the ears and eyes of the cats show up more pronounced in the second mode Figure 9 (b).\nNow how does this influence our classification problem? For this let us first see how easy it is to distinguish the two classes in a scatter plot for the first four modes.\n\nShow the code for the figure\nimport seaborn as sns\nimport pandas as pd\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\ntarget = [\"cat\"] * 80 + [\"dog\"] * 80\nC = U[:, :4].T @ (CD - np.mean(CD))\ndf = pd.DataFrame(C.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf[\"target\"] = target\n\nC_w = U_w[:, :4].T @ (CD_w - np.mean(CD))\ndf_w = pd.DataFrame(C_w.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf_w[\"target\"] = target\n\n_ = sns.pairplot(df, hue=\"target\", height=1.45)\n_ = sns.pairplot(df_w, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\n\n\nFigure 10: First four modes of the raw images for cats and dogs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: First four modes of the wavelet images for cats and dogs.\n\n\n\n\n\n\nThe two figures above give us a good idea what the different principal values can tell us. It is quite clear, that the first mode is not of much use for differentiation of the two classes. In general the second mode seems to work best (the ears), in contrast to the raw image we can also see that the wavelet basis is slightly better for the separation.\n\n\nShow the code for the figures\nimport plotly.express as px\n\n\nfig = px.scatter_3d(df, x='PV2', y='PV3', z='PV4', color=\"target\")\nfig.show()\nfig = px.scatter_3d(df_w, x=\"PV2\", y=\"PV3\", z=\"PV4\", color=\"target\")\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 12: Scatter plots of modes 2 to 4 for the raw images.\n\n\n\n\n\n\n                                                \n\n\nFigure 13: Scatter plots of modes 2 to 4 for the wavelet images.\n\n\n\n\nNow that we have an idea what we are dealing with, we can start looking into the two previously described paths in machine learning. The difference between supervised and unsupervised learning can be summarized by the following two images.\n\nShow the code for the figure\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(df.iloc[:, 0][index], df.iloc[:, 2][index], label=name)\n\nplt.figure()\nplt.scatter(df.iloc[:, 0], df.iloc[:, 2], color=\"gray\")\n\n\n\n\n\n\n\n\n\n\nFigure 14: Labelled data for supervised learning.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Unlabelled data for unsupervised learning.\n\n\n\n\n\n\nYou either provide/have labels or not.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#footnotes",
    "href": "clustering/index.html#footnotes",
    "title": "Clustering and Classification",
    "section": "",
    "text": "To add it to pdm us pdm add scikit-learn.↩︎\nsee Kandolf (2025), Section 4.2 or follow the direct link↩︎\nsee Kandolf (2025), Section 10 or follow the direct link↩︎",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html",
    "href": "clustering/unsupervised.html",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "1.1 \\(k\\)-Means Clustering\nWe start with unsupervised learning. The goal of unsupervised learning is to discover clusters in the data of observations that have no labels, i.e. we have nothing to look at as reference. There are several algorithms to perform this task, the most prominent is the \\(k\\)-means clustering algorithm.\nThe \\(k\\)-means algorithm tries to partition a set of \\(m\\) (vector-valued) data observations into \\(k\\) clusters. Where in general, the dimension of the data and the amount of observations is known, the number of clusters is often not known a priori.\nThe general idea is, to label each observation as belonging to a cluster with the nearest mean (the representative of the cluster). The resulting clusters are called Voronoi cells, see Wikipedia - Voronoi diagram.\nWe illustrate the proceedings with the help of some artificial observations in two dimensional space, and show how the clustering takes place.\nIn Figure 1.1 (a) we see the two distinct clusters and the initial guesses for the centers. In the successive plots, we see how the centers move and converge to the final position, as seen in Figure 1.1 (d). In this case the algorithm converges after the sixth step.\nOf course the algorithm is sensitive to the initial guess and therefore modern versions provide strategies to determine the initial guess, as well as the number of clusters.\nWe can see this in action in the sklearn.cluster.KMeans version.\nAs can be seen in Figure 1.2 (a) the algorithm comes up with (almost) the same split between the two sets as our crude implementation. If we try for three clusters Figure 1.2 (b) the result is sensible as well.\nThe major success, of this algorithm in general use, is based on the fast convergence, that requires no supervision. We can also see that it is not very accurate, compare Figure 1.1 (a) and Figure 1.1 (d). This is no surprise, as the algorithm has not all information available.\nHow can we determine how accurate the algorithm is? If we have no labels this is of course not easy to do but cross-validation is a good tool.\nIn our case we can produce the labels and we can also split the data beforehand into a training set and a test set. Usually a so called \\(80:20\\) split is used, i.e. \\(80\\%\\) training data and \\(20\\%\\) test data.\nShow the code for the figure\nfrom sklearn.cluster import KMeans\nnp.random.seed(6020)\n\n# Shuffle data\nX_shuffle = X[np.random.permutation(X.shape[0]), :]\nX2_shuffle = X2[np.random.permutation(X2.shape[0]), :]\n\n# Split data into two parts\nsplit = n // 5 * 4\ndata = np.concatenate((X_shuffle[:split, :], X2_shuffle[:split, :]))\ntest = np.concatenate((X_shuffle[split:, :], X2_shuffle[split:, :]))\n\n# Create clustesr\nkmeans = KMeans(n_clusters=2, random_state=6020).fit(data)\nclasses = kmeans.predict(data)\ntest_classes = kmeans.predict(test)\n\n# Find wrong classifications\nerror = test_classes != np.concatenate((np.zeros(n-split), np.ones(n-split)))\n\n# Plotting\nplt.figure()\ncolour = [\"tab:orange\", \"tab:blue\"]\nfor i in range(2):\n    plt.scatter(data[classes==i, 0], data[classes==i, 1],\n                c=colour[i], alpha=0.5, label=\"train\")\n    plt.scatter(test[test_classes==i, 0], test[test_classes==i, 1],\n                c=colour[i], label=\"test\")\n\nplt.scatter(test[error, 0], test[error, 1], marker=\"x\", c=\"k\", label=\"error\")\nplt.gca().set_aspect(1)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.3: Validation against a test set.\nThe result of this can be seen in Figure 1.3 where we have two points wrongly classified to the opposite cluster.\nThere exist several extensions of the basic \\(k\\)-means algorithm to improve the results and overall performance. Two such versions are the accelerated \\(k\\)-means, as well as mini-batch \\(k\\)-means. Both can be found in Geron (2022).",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#sec-clustering-usl-kmeans",
    "href": "clustering/unsupervised.html#sec-clustering-usl-kmeans",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "Definition 1.1 (\\(k\\)-Means Algorithm) For a given set of \\(m\\) observations \\((x_1, x_2, \\ldots, x_m)\\), with \\(x_i\\in \\mathrm{R}^n\\) the algorithm strives to find \\(k\\) sets \\((S_1, S_2, \\ldots, S_k)\\) such that the variance inside the cluster is minimized, i.e. \\[\n\\underset{S}{\\operatorname{argmin}} \\sum_{i=1}^k \\sum_{x\\in S_i}\\| x - \\mu_i \\|^2,\n\\] where \\(\\mu_i\\) denotes the mean of \\(S_i\\).\nThe algorithm itself is recursive, for a given \\(k\\)\n\nRandomly initialize \\(k\\) points \\(\\mu_1, \\mu_2, \\ldots, \\mu_k\\), as the cluster centers,\nLabel each observation \\(x_i\\) by the nearest cluster center \\(\\mu_j\\), all points with the same label form the set \\(S_j\\).\nCompute the mean of each cluster \\(S_j\\) and replace the current \\(\\mu_j\\) by it.\nRepeat, starting from step 2, until the cluster centers stay stable up to some tolerance.\n\nThis algorithm was first introduced in Lloyd (1982) and is therefore often called Lloyd algorithm.\n\n\nShow the code for Lloyd algorithm\ndef lloyd(data, centers, steps):\n    classes = np.zeros(data.shape[0])\n    centers_ = np.copy(centers)\n    for i in range(steps):\n        for j in range(data.shape[0]):\n            classes[j] = np.argmin(np.linalg.norm(centers_ - np.tile(data[j, :],\n                                   (centers.shape[0], 1)), axis=1))\n        for j in range(centers.shape[0]):\n            centers_[j, :] = np.mean(data[classes == j, :], axis=0)\n        \n    return (classes, centers_)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn general Definition 1.1 is NP-hard and therefore computationally not viable. Nevertheless, a number of heuristic algorithms exist to provide a good performance, despite having no guarantee for global convergence.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.5 - 5.6). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\n# Helper for plotting\ndef plot_lloyd(data, centers, classes, ms=500):\n    plt.figure()\n    for i in range(centers.shape[0]):\n        plt.scatter(data[classes==i, 0], data[classes==i, 1])\n        plt.scatter(centers[i, 0], centers[i, 1], c=\"k\", s=ms, marker=\"*\")\n    plt.gca().set_aspect(1)\n\n# Create data for illustration\nn = 200\n# Random ellipse centred in (0, 0) and axis (1, 0.5)\nX = np.random.randn(n, 2) * np.array([1, 0.5])\n# Random ellipse centred in (1, -2) and axis (1, 0.2)\nX2 = (np.random.randn(n, 2) * np.array([1, 0.2])) + np.array([1, -2])\n# Rotate ellipse 2 by theta\ntheta = np.pi / 4\nX2 = X2 @ np.array([[np.cos(theta), -np.sin(theta)],\n                  [np.sin(theta), np.cos(theta)]] )\n\ncenters = np.array([[0., -1.], [-1., 0.]])\ndata = np.concatenate((X, X2))\n# Plot initial step with theoretical assignment\nclasses = np.concatenate((np.ones(X2.shape[0]), np.zeros(X.shape[0])))\nplot_lloyd(data, centers, classes)\n\n# Compute and plot consecutive steps of lloyds algorithm\nfor i in [1, 2, 5]:\n    classes, centers_ = lloyd(data, centers, i)\n    plot_lloyd(data, centers, classes)\n    centers = centers_\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The original two classes and the initial guesses for the centers.\n\n\n\n\n\n\n\n\n\n\n\n(b) Point association to the clusters in the first step.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Point association to the clusters in the third step.\n\n\n\n\n\n\n\n\n\n\n\n(d) Point association to the clusters in the eighth step.\n\n\n\n\n\n\n\nFigure 1.1: Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\n\n\n\n\nShow the code for the figure\nfrom sklearn.cluster import KMeans\nnp.random.seed(6020)\n\nkmeans = KMeans(n_clusters=2, random_state=6020).fit(data)\nplot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)\nkmeans = KMeans(n_clusters=3, random_state=6020).fit(data)\nplot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Seeking two clusters.\n\n\n\n\n\n\n\n\n\n\n\n(b) Seeking three clusters.\n\n\n\n\n\n\n\nFigure 1.2: KMeans algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.1 (Apply the \\(k\\)-means algorithm to other data sets) As an exercise, to get some practice for using \\(k\\)-means, apply the algorithm to some other datasets to see how it performs.\n\nWe already looked at the Fischer Iris data set in Section 1.1 and discussed some basic features. Try with only two dimensional data, as well as four dimensional data to find the three clusters.\nFor Figure 1.7 we will look at the moons data set. Try working out the clusters in this case.\nApply the algorithm to the cats and dogs (in wavelet basis, see Section 1.2) for various principal components and find the two clusters.\n\n\n\n\n\n\n1.1.1 Applications of \\(k\\)-means\nThe \\(k\\)-means algorithm is used in a multitude of applications, we list some here:\n\nImage segmentation: To decompose an image into different regions we can use \\(k\\)-means. Applications range from robotics to surveillance, where one or more objects are separated from the rest of the image.\nCustomer segmentation/social network analysis: To segment customers along e.g. their purchase history/behaviour, preferences, demographic data, amount spent, search queries, social media interactions, etc. \\(k\\)-means is used in marketing, retail, and advertising to personalize the experience.\nText clustering: In natural language processing (NLP) \\(k\\)-means is often used to cluster similar documents or text to make analysing large volumes of text data feasible.\nFraud detection: \\(k\\)-means is a crude tool for fraud detection in finance and banking. Transactions are clustered according to similarities and anomalies are detected. There exist more sophisticated methods in finance, but \\(k\\)-means is an easy to understand start.\nAnomaly detection: In medical (image) data \\(k\\)-means is often used to detect anomalies by finding points that fall outside of clusters. The same works for cybersecurity and e.g. network traffic.\nRecommendation systems: By grouping users together it is easier to recommend them new songs, items for shopping, and more.\nQuality control: By grouping similar products we can detect quality issues and defects in production processes. If an issue is found the part can be checked and the process adjusted.\nTraffic Analysis: In transport and logistics we can analyze traffic patterns and use the information for optimization and similar trips, routes, and vehicles.\n\nWe want to highlight image segmentation as an application of \\(k\\)-means. The example found here is first shown in Geron (2022).\nThe idea of image segmentation is to decompose an image into different segments. The following variants exist:\n\nColour segmentation - pixels with similar colour get assigned the same cluster. An application of this is image satellite postprocessing to find areas of forest or sea, or finding object in robotics applications, and organs in medical images.\nSemantic segmentation - all pixels that are part of the same object get assigned to the same segment. An application of this is in autonomous driving to find pedestrians - one segment containing all pedestrians.\nInstance segmentation - similar to semantic segmentation but individual objects are assigned to the same segment. In the autonomous driving application we would find individual pedestrians.\n\nHere we show how to perform colour segmentation with \\(k\\)-means.\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from Geron (2022), see GitHub,\n\n\n\nShow the code for the figure\nimport numpy as np\nimport imageio.v3 as iio\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nim = np.asarray(iio.imread(\n        \"https://github.com/ageron/handson-ml3/blob/main/images/\"\n        \"unsupervised_learning/ladybug.png?raw=true\"))\nplt.figure()\nplt.imshow(im)\nplt.axis(\"off\")\n\nZ = im.reshape(-1, 3)\nfor colours in [8, 4, 2]:\n    kmeans = KMeans(n_clusters=colours, n_init=10, random_state=6020).fit(Z)\n    plt.figure()\n    plt.imshow(kmeans.cluster_centers_[kmeans.labels_].reshape(im.shape) / 255)\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Segmentation by 8 colours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Segmentation by 4 colours.\n\n\n\n\n\n\n\n\n\n\n\n(d) Segmentation by 2 colours.\n\n\n\n\n\n\n\nFigure 1.4: Colour segmentation for the image of a lady bug.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#unsupervised-hierarchical-clustering---dendrogram",
    "href": "clustering/unsupervised.html#unsupervised-hierarchical-clustering---dendrogram",
    "title": "1  Unsupervised learning",
    "section": "1.2 Unsupervised hierarchical clustering - Dendrogram",
    "text": "1.2 Unsupervised hierarchical clustering - Dendrogram\nSimilar to \\(k\\)-means, a simple hierarchical algorithm is used to create a dendrogram. The resulting tree allows us to easily see if data is clustered without the need of labeling or supervision.\nWe follow the example and discussion given in (Brunton and Kutz 2022, sec. 5.4).\nThere are two main approaches in creating the desired hierarchy: bottom-up often called agglomerative, top-down often called divisive.\n\nFor the agglomerative approach each observation \\(x_i\\) is initially its own cluster and in each step points are combined according to their distance. Eventually everything ends up in a single cluster and the algorithm stops.\nFor the divisive approach we go the opposite direction and start with the super cluster containing all observations §x_i$ and we gradually split up into smaller and smaller clusters. The algorithm stops, when each observation is its own leave.\n\nOf course the norm1 used has quite an influence as can be seen in (Kandolf 2025, sec. 7.1) Link where we compared LASSO and RIDGE algorithms for our optimization problem.\nTo illustrate the agglomerative approach and the difference in norms we use four points and construct the hierarchy by combining the two closest points.\n\n\n\n\n\n\n\n\n\n(a) Use two norm or euclidean norm to compute the distance - \\(\\| \\cdot \\|_2\\)\n\n\n\n\n\n\n\n\n\n(b) Use one or cityblock norm to compute the distance - \\(\\| \\cdot \\|_1\\)\n\n\n\n\n\nFigure 1.5: We always combine the two nearest points and replace them with the point in the middle. This iteration continues until only one point is left. The Dendrogram is build according to the order of points chosen. It can also include the distance.\n\n\n\nOn a larger scale, with always the first 25 points of the two clusters above we get the results shown in Figure 1.6\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster import hierarchy\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nXX = np.concatenate((X[:25, :], X2[:25, :]))\nXX.shape\nfor metric in [\"euclidean\", \"cityblock\"]:\n    plt.figure()\n    Y = pdist(XX, metric=metric)\n    Z = hierarchy.linkage(Y, method=\"average\", metric=metric)\n    thresh = 0.90 * np.max(Z[:, 2])\n    dn = hierarchy.dendrogram(Z, p=100, color_threshold=thresh)\n\n    plt.figure()\n    plt.bar(range(XX.shape[0]), dn[\"leaves\"])\n    plt.plot(np.array([0, XX.shape[0]]),\n             np.array([XX.shape[0] // 2, XX.shape[0] // 2]),\n             \"r:\", linewidth=2)\n    plt.plot(np.array([(XX.shape[0] // 2) + 1/2, (XX.shape[0] // 2) + 1/2]),\n             np.array([0, XX.shape[0]]),\n             'r:', linewidth=2)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Dendrogram for euclidean norm.\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram showing the clustering for the euclidean norm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Dendrogram for cityblock norm.\n\n\n\n\n\n\n\n\n\n\n\n(d) Histogram showing the clustering for the cityblock norm.\n\n\n\n\n\n\n\nFigure 1.6: Construction a agglomerative hierarchy for our data set.\n\n\n\nThe two dendrograms (Figure 1.6 (a) and Figure 1.6 (c)) show the hierarchical structure derived from the data set. The number of clusters can be influenced by the thresh parameter and it is also used to label the observation accordingly. It is quite similar to the number of clusters \\(k\\) in the \\(k\\)-means algorithm.\nThe two bar graphs on the right (Figure 1.6 (b) and Figure 1.6 (d)) show how the observations is clustered in the dendrogram. The bars correspond to the distance metric produced by the algorithm. The red lines indicate the region of a perfect split for the separation if the algorithm works perfectly and places every point in the original cluster. If we recall, that the first 25 points are from set 1 and the next 25 from set 2 we can see that the euclidean distance and the cityblock norm place one point in the wrong cluster.\n\n\n\n\n\n\nNote\n\n\n\nThe function scipy.cluste.hierarchy.linkage allows to specify the method of computing the distance between two clusters. The used average corresponds to unweighted pair group method with arithmetic mean UPGMA algorithm.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn In scikit-learn this is supported via the sklearn.cluster.AgglomerativeClustering class.\n\n\n\n\n\n\n\n\n\nExercise 1.2 (Apply the agglomerative clustering to the Fischer Iris data set)  \n\nWork out how to use the scikit-learn alternative.\nWe already looked at the Fischer Iris data set in Section 1.1 and discussed some basic features. Try with only two dimensional data, as well as four dimensional data, to find the three clusters.\nFor Figure 1.7 we will look at the moons data set. Try working out the clusters in this case, look at the different possibilities with the linkage option as {\"ward\", \"complete\", \"average\", \"single\"}.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#sec-clustering-ul-dbscan",
    "href": "clustering/unsupervised.html#sec-clustering-ul-dbscan",
    "title": "1  Unsupervised learning",
    "section": "1.3 Density-based spatial clustering of applications with noise (DBSCAN)",
    "text": "1.3 Density-based spatial clustering of applications with noise (DBSCAN)\nThe algorithm was introduced in Ester et al. (1996) and is an algorithm that finds areas of high density. The main idea behind the algorithm is as follows.\nOur observations (we will call them points here for consistency) form the basis and they can be in any space but as for all the algorithms presented here, there needs to exist some form to measure distance. Furthermore, DBSCAN relies on two parameters, \\(\\epsilon\\) describing the radius of neighbourhood of a point and \\(minPts\\) the minimum of points needed to form a cluster. With these parameters we perform the following steps:\n\nA point \\(p\\) is called a core point if at least \\(minPts\\) (including \\(p\\)) are within distance \\(\\epsilon\\) of \\(p\\). This region is called the \\(\\epsilon\\)-neighbourhood of \\(p\\).\nA point \\(q\\) is called directly reachable from a core point \\(p\\) iff2 \\(q\\) is in the \\(\\epsilon\\)-neighbourhood of \\(p\\).\nA point \\(p\\) is reachable from \\(p\\), if there exists a sequence of points (path) \\(p=p_1, p_2,  \\ldots, p_n=q\\) where each \\(p_{i+1}\\) is directly reachable form \\(p_i\\). Consequently, all point on the path, except \\(p\\), need to be core points.\nA point that is not reachable from any other point is considered an outlier or noise point.\n\nTherefore, we get our definition of a cluster: a core point \\(p\\) together with all points that are reachable from \\(p\\). This includes core points (at least one) and none-core points (boundary points) on the boundary as they can not be used to reach more points.\nIn contrast to \\(k\\)-means the algorithm we can find non-linear (i.e. curved) clusters. We illustrate this with the moon data set.\n\n\n\n\n\n\nImportant\n\n\n\nFor the plotting function please see GitHub, the accompanying repository to Geron (2022), as it forms the basis.\n\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef plot_dbscan(dbscan, X, size):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    # Plotting the clusters\n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask],\n                marker=\"o\", s=size, cmap=\"Paired\")\n    # Plotting the core points of the clusters\n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask],\n                marker=\"*\", s=20,)\n    # Plotting the anomalies\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    # Plotting the boundary points\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n    plt.gca().set_aspect(1)\n\nnp.random.seed(6020)\nM, M_y = make_moons(n_samples=1000, noise=0.05, random_state=6020)\n\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(M)\nplt.figure()\nplot_dbscan(dbscan, M, size=100)\n\ndbscan = DBSCAN(eps=0.2, min_samples=5)\ndbscan.fit(M)\nplt.figure()\nplot_dbscan(dbscan, M, size=600)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scikit-Learn’s DBSCAN with eps=0.05 and min_samples=5.\n\n\n\n\n\n\n\n\n\n\n\n(b) Scikit-Learn’s DBSCAN with eps=0.2 and min_samples=5.\n\n\n\n\n\n\n\nFigure 1.7: DBSCAN illustrated with the moons dataset.\n\n\n\nThe algorithm works best for clusters well separated by low density regions. As all algorithms depending on distance measures, it suffers from the curse of dimensionality3.\nAs can be seen in Figure 1.7 (a) the algorithm produces quite a lot of clusters for a small \\(\\epsilon=0.05\\). In total we get 10 clusters and quite a lot of outliers that are not all easy to retract by the naked eye.\nNevertheless, if we increase \\(\\epsilon=0.2\\) we can see that the two clusters are neatly reproduced, Figure 1.7 (b).\n\n\n\n\n\n\nNote\n\n\n\nThe DBSCAN class in Scikit-Learn does not provide a .predict() method like many other such classes. See Geron (2022) and GitHub for how to train a \\(k\\)-nearest neighbour (kNN) to perform this task.\n\n\n\n\n\n\n\n\n\nExercise 1.3 (Application of DBSCAN)  \n\nApply the DBSCAN algorithm to our toy example used through this section, to recover the two clusters as good as possible. Try different \\(\\epsilon\\) values as well as different norms (metric parameter).\nAdditionally, for a higher dimensional problem, using DBSCAN to split the Fischer Iris data set (Section 1.1) into its three clusters of flowers.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#finite-mixtures-models",
    "href": "clustering/unsupervised.html#finite-mixtures-models",
    "title": "1  Unsupervised learning",
    "section": "1.4 Finite Mixtures Models",
    "text": "1.4 Finite Mixtures Models\nFinite mixture models assume that the observations \\(x_i\\) are mixtures of \\(k\\) processes, which combine in the measurement. Each mixture is defined via a probability model with unknown parameters. The aim of the algorithm is to find the parameters such that the \\(k\\) processes best describe the \\(x_i\\) observations. The basis of the algorithm is the so called expectation-maximization (EM) algorithm of Dempster, Laird, and Rubin (1977), that is designed to find the maximum-likelihood parameters of the defined statistical models.\nThe most common choice for the probability models is the Gaussian distribution4 and for this choice, the method is better known as Gaussian mixture models (GMM). This is often quite a sensible choice, if nothing more is known about the observations, and a Gaussian distribution is therefore a save bet. As a result each process can be described by two parameters, mean and variance. Each cluster in turn, will be described by an ellipsoidal shape where size, density, orientation, and semi-axis vary.\nLike \\(k\\)-means, in the simplest version, GMM is provided with the number of clusters \\(k\\) and starts from an initial guess of the means and corresponding variances. The parameters are than iteratively updated to find a (local) maximum. There are some problems with this approach, e.g. if we set one of the processes to have zero variance and mean equal to a point. To avoid such problems cross-validation can often help to avoid problems stemming from an initialization with a poor initial guess.\nAs mentioned, the main idea of the mixture models is that the observations are a linear combination of probability density functions (PDF) \\[\nf(x_j, \\Theta) = \\sum_{p=1}^k \\alpha_p f_p(x_j, \\Theta_p),\n\\] where \\(f\\) denotes the observed/measured PDF with parameters \\(\\Theta\\), \\(f_p\\) the PDF of mixture \\(p\\) with parameters \\(\\Theta_p\\), and \\(k\\) denotes the number of mixtures. The weights \\(\\alpha_p\\) fulfil \\(\\sum_p \\alpha_p = 1\\).\nOverall we therefore can formulate the algorithm as:\n\n\n\n\n\n\n\nDefinition 1.2 (Mixture Models Algorithm) Given the observed PDF \\(f(x_j, \\Theta)\\), estimate the mixture weights \\(\\alpha_p\\) and the parameters fo the distribution \\(\\Theta_p\\).\n(see Brunton and Kutz 2022)\n\n\n\n\nLets follow the derivative for the GMM method in (Brunton and Kutz 2022, sec. 5.5) to get a better idea what is happening.\nFor GMM, \\(f\\) reads as \\[\nf(x_j, \\Theta) = \\sum_{p=1}^k \\alpha_p \\mathcal{N}_p(x_j, \\mu_p, \\sigma_p),\n\\] and for a given \\(k\\), we need to find \\(\\alpha_p, \\mu_p, \\sigma_p\\). We get \\(\\Theta\\) from the roots of \\[\nL(\\Theta) = \\sum_{j=1}^n\\log f(x_j | \\Theta).\n\\] \\(L\\) is called the log-likelihood function and we sum over all observations \\(x_j\\). We transform it into an optimization problem by setting the derivative to zero \\[\n\\frac{\\partial L(\\Theta)}{\\partial \\Theta} = 0,\n\\] and solve it via the EM algorithm. As the name suggests, there are two steps E and M.\nThe E step uses the following posterior to establish memberships. Therefore, we start of by assuming an initial guess of the vector \\(\\Theta\\) and this leads to the _posterior probability of distribution \\(p\\) of \\(x_j\\) \\[\n\\tau_p(x_j, \\Theta) = \\frac{\\alpha_p f_p(x_j, \\Theta_p)}{f(x_j, \\Theta)}.\n\\] In other words, we try to figure out if \\(x_j\\) is part of the \\(p\\)th mixture. For GMM this becomes, \\[\n\\tau_p^{(l)}(x_j) = \\frac{\\alpha_p^{(l)} \\mathcal{N}_p(x_j, \\mu_p^{(l)}, \\sigma_p^{(l)})}{f(x_j, \\Theta^{(l)})},\n\\] in the \\(l\\) iteration.\nNow the M step starts to update the parameters and mixture weights as \\[\n\\begin{align}\n\\alpha_p^{(l+1)} &= \\frac1n \\sum_{j=1}^n \\tau_p^{(l)}(x_j), \\\\\n\\mu_p^{(l+1)} &= \\frac{\\sum_{j=1}^n x_j\\tau_p^{(l)}(x_j)}{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)}, \\\\\n\\Sigma_p^{(l+1)} &= \\frac{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)[(x_j - \\mu_p^{(l+1)})(x_j - \\mu_p^{(l+1)})^\\mathrm{T}]}{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)},\n\\end{align}\n\\] with \\(\\Sigma_p^{(l+1)}\\) denotes the covariance matrix. The algorithm now alternates between E and M until convergence is reached.\nLet us try it with the cats and dogs images in wavelet basis from Section 1.2 by fitting to the second and forth principal value of the SVD. Note, we directly load the images in wavelet basis and do not recompute them.\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.8). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.mixture import GaussianMixture\nfrom scipy.stats import multivariate_normal\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\nCD = np.concatenate((dogs, cats), axis=1)\nU,S,VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\nv = VT.T\n\ndogcat = v[:,(1,3)]\nGMModel = GaussianMixture(n_components=2, n_init=10).fit(dogcat)\n\nplt.figure()\nplt.scatter(v[:80,1], v[:80,3], label=\"dogs\")\nplt.scatter(v[80:,1], v[80:,3], label=\"cats\")\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\n\nx = np.linspace(-0.15, 0.25)\ny = np.linspace(-0.25, 0.2)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = np.exp(GMModel.score_samples(XX))\nZ = Z.reshape(X.shape)\n\nCS = plt.contour(X, Y, Z,\n                levels=np.arange(3,32,4), colors='k', linestyles='solid')\nplt.legend()\nplt.gca().set_aspect(1)\n\nplt.figure()\nax = plt.axes(projection='3d')\nfor i in range(GMModel.weights_.shape[0]):\n    rv = multivariate_normal(GMModel.means_[i], GMModel.covariances_[i])\n    z = GMModel.weights_[i] * rv.pdf(np.dstack((X, Y)))\n    ax.plot_surface(X, Y, z, alpha=0.5)\n\nax.plot_wireframe(X, Y, Z, color='black', rstride=5, cstride=5)\nax.view_init(30, -80)\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\nplt.legend([r\"$\\mathcal{N}(\\mu_1, \\sigma_1)$\",\n            r\"$\\mathcal{N}(\\mu_2, \\sigma_2)$\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) We can see a nice split along the tow animals via the Gaussian distributions.\n\n\n\n\n\n\n\n\n\n\n\n(b) The PDFs of the fitted distributions, weighted via the GMM algorithm.\n\n\n\n\n\n\n\nFigure 1.8: Fitting of the second and fourth principal value of the cats and dogs images in wavelet basis via the GMM method.\n\n\n\nAs can be seen in Figure 1.8 (a) the two clusters are quite well captured by the GMM algorithm. The contour shows the linear combination of the two Gaussians. We see the two Gaussians on the right in Figure 1.8 (b) where we use the same colours as for the cats and dogs respectively. Note that the distributions are weight \\(\\alpha_p\\) from GMM but other than that there is no unit in the z direction.\nAs discussed above, the algorithm is prone to problems during the initialization, therefore we use n_init = 10 to initialize the algorithm 10 times and only keep the best result.\nWe can draw new samples from the resulting object .sample() and we can also fit data to it and we can also perform hard and soft clustering. For hard clustering the model assigns each instance to the most likely cluster (.predict()), where for soft clustering (.predict_proba()) we get the probability of membership for each cluster.\nIf the algorithm struggles to recover the clusters, we can help it by imposing the shape of the cluster via restricting the covariance matrix \\(\\Sigma_p\\). This can be done via the parameter covariance_type, see docs.\n\n\n\n\n\n\n\nExercise 1.4 (Application of GMM)  \n\nApply the GMM algorithm to our toy example used through this section to recover the two clusters as good as possible. This should be straight forward, as we constructed it via Gaussian distributions. Try constraining the algorithm by imposing the different values possible for covariance_type.\nAdditionally, for a higher dimensional problem, split the Fischer Iris data set into its three clusters of flowers.\nTry to use GMM to get clusters for the moon data set used in Section 1.3. Also try with the BayesianGaussianMixture class, where no amount of clusters needs to be specified.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGMM can also be used for anomaly detection. Simply assume every observation located in a low density region an anomaly. For this the threshold needs to be defined in accordance to our expected anomalies.\nE.g. to get about \\(4\\%\\) anomalies we can use the following snippet together with the above code:\n\n\nShow the code for the figure\nS = GMModel.score_samples(dogcat)\ndensity_threshold = np.percentile(S, 2)\nanomalies = dogcat[S &lt; density_threshold]\nplt.figure()\nplt.scatter(dogcat[:80,0], dogcat[:80,1], alpha=0.5, label=\"dogs\")\nplt.scatter(dogcat[80:,0], dogcat[80:,1], alpha=0.5, label=\"cats\")\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color=\"k\", marker=\"x\",\n            label=\"anomaly\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.9: Fitting of the second and fourth principal value of the cats and dogs images in wavelet basis via the GMM method.\n\n\n\n\n\nSee Geron (2022) and the corresponding GitHub repository.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#sec-clustering-ul-me",
    "href": "clustering/unsupervised.html#sec-clustering-ul-me",
    "title": "1  Unsupervised learning",
    "section": "1.5 Motivational example",
    "text": "1.5 Motivational example\nBefore we make the leap to supervised learning, we we introduce a new data set and see how our unsupervised approach works in this case.\nWe do this with the (in)famous MNIST data set of hand written digits. Lets start by looking at the data set and establishing some basic properties.\nWe load the data set and look at its description\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)\nprint(mnist.DESCR)\n\n**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n**Please cite**:  \n\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n\nDownloaded from openml.org.\n\n\nThe images are stored in .data and the label in .target. We can look at the first 100 digits\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nmnist = fetch_openml('mnist_784', as_frame=False)\n\nim = np.zeros((10 * 28, 10 * 28), int)\nfor i in range(10):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = mnist.data[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.10: First 100 digits from the MNIST data set.\n\n\n\n\n\nAs can be seen from Figure 1.10, most digits are easy to distinguish for us, but there are some nasty things included. Especially, there are 1s with the english style (just one straight line) of writing (1st row, 7th column), but also as a diagonal line (1st row, 4th column), and finally also the german style (connecting line at base and top) of writing (3rd row, 5th column).\nAlso the 2 digit comes in various forms, with loop (1st row, 6th column) and without (3rd row, 6th column). Finally we all know that a 3 can hide in an 8, same as a 7 and 1 can hide in a 9 and much more.\nNow, before we go any further in the investigation we split up the data set into a training and a test set, we will not use this here but we should establish some way of working right from the start.\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\nNow let us try with \\(k\\)-means to find the ten digits by finding \\(10\\) clusters.\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nk = 10\nkmeans = KMeans(n_clusters=k, n_init=1, random_state=6020).fit(X_train)\n\nim = np.zeros((1 * 28, 10 * 28), int)\nfor i in range(1):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = \\\n                kmeans.cluster_centers_[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.11: The cluster means of \\(k\\)-means for 10 clusters.\n\n\n\n\n\nAs can be seen from Figure 1.11, the cluster centers do not recover our ten digits. In general, they are, as expected, smudged as they do not represent an actual image. It looks like \\(0, 1, 2, 3, 6, 8, 9\\) are easy to discover but \\(4, 5, 7\\) not. If we look closely, we can see \\(4, 5, 7\\) represented in our clusters but not as separate digits. Obviously this is not a good way to proceed and to find our digits. We will discuss methods to perform this task in the next section Chapter 2. Furthermore, keep this part in mind, as we will come back to how this can still be helpful in a different aspect as semi-supervised learning Chapter 3.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B (Methodological) 39 (1): 1–22.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. “A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.” In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 226–31. KDD’96. Portland, Oregon: AAAI Press.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.\n\n\nLloyd, Stuart P. 1982. “Least Squares Quantization in PCM.” IEEE Trans. Inf. Theory 28 (2): 129–36. https://doi.org/10.1109/TIT.1982.1056489.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#footnotes",
    "href": "clustering/unsupervised.html#footnotes",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "see Kandolf (2025), Section 1.2 or follow the direct link↩︎\nshort for if and only if↩︎\nProblems occurring for higher dimensional data and distance measures, see WikiPedia↩︎\nsee Kandolf (2025), Example 14.6 or for a direct Link↩︎",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html",
    "href": "clustering/supervised.html",
    "title": "2  Supervised learning",
    "section": "",
    "text": "2.1 Linear Discriminant Analysis (LDA)\nIf we recall Fisher (1936) from the Iris data set, we can also find one of the first supervised learning methods in this paper. The introduced linear discriminant analysis (LDA) has survived over time and is still one of the standard techniques for classification, even though we use a more generalized and improved method nowadays.\nThe idea if LDA is to find a linear combination of features that optimally separates two or more classes. The crucial part in the algorithm is that it is guided by labelled observations. At its core, the algorithm aims to solve an optimization problem: find an optimal low-dimensional embedding of the data that shows a clear separation between their point distribution, or maximize the distance between the inter-class data and minimize the intra-class data distance.\nThe main idea of LDA is to use projection. For a two-class LDA this becomes \\[\nw = \\operatorname{arg\\, max}_w \\frac{w^\\mathrm{T}S_B w}{w^\\mathrm{T}S_W w},\n\\tag{2.1}\\] (the generalized Rayleigh quotient) where \\(w\\) is our thought after projection. The two included scatter matrices are \\[\nS_B = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^\\mathrm{T},\n\\] for between-class relation as well as \\[\nS_W = \\sum_{j=1}^2 \\sum_{x\\in\\mathcal{D}_j} (x - \\mu_j)(x - \\mu_j)^\\mathrm{T},\n\\] for within-class data. The set \\(\\mathcal{D}_j\\) denotes the subdomain of the data associated with cluster \\(j\\). The two matrices measure the variance of the data set as well as the means. To solve Equation 2.1 we need to solve the generalized eigenvalue problem1 \\[\nS_B w = \\lambda S_w w\n\\] where the maximal eigenvalue and the corresponding eigenvector are our solution.\nWe try this with the cats and dogs data set in both basis.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef analyse(CD):\n    U, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\n    v = VT.T\n    xtrain = np.concatenate((v[:60, [1, 3]], v[80:140, [1, 3]]))\n    label = np.repeat(np.array([1, -1]), 60)\n    test = np.concatenate((v[60:80, [1, 3]], v[140:160, [1, 3]]))\n    lda = LinearDiscriminantAnalysis()\n    lda.fit(xtrain, label)\n    test_class = lda.predict(test)\n    truth = np.repeat(np.array([1, -1]), 20)\n    E = 100 * (1 - np.sum(np.abs(test_class - truth) / 2) / 40)\n    plt.figure()\n    plt.bar(range(40), test_class)\n    plt.plot([-0.5, 39.5], [0, 0], \"k\", linewidth=1.0)\n    plt.plot([19.5, 19.5], [-1.1, 1.1], \"r-.\", linewidth=3)\n    plt.yticks([-0.5, 0.5], [\"cats\", \"dogs\"], rotation=90, va=\"center\")\n    plt.text(10, 1.05, \"dogs\")\n    plt.text(30, 1.05, \"cats\")\n    plt.gca().set_aspect(40 / (2 * 3))\n    return (test_class, E, v)\n\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog\"]\n\ntest_class, E, v = analyse(np.concatenate((dogs, cats), axis=1))\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\ntest_class, E_w, v_w = analyse(np.concatenate((dogs_w, cats_w), axis=1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Trained and evaluated against the raw data.\n\n\n\n\n\n\n\n\n\n\n\n(b) Trained and evaluated against the data in wavelet basis.\n\n\n\n\n\n\nFigure 2.2: Evaluation of the LDA for the second and fourth principal component on the test set of 40 animals. A bar going up corresponds to dogs and one going down to cats. The first 20 individuals should be dogs, the second 20 cats. The red dotted line shows the split. True positive can be found in the top-left as well as the bottom-right.\nIf we use our raw data set for the classification we get an overall accuracy of 67.5% with \\(\\tfrac{4}{20}\\) wrongly labelled dogs and \\(\\tfrac{9}{20}\\) wrongly labelled cats. We can increase this to an accuracy of 82.5% with \\(\\tfrac{5}{20}\\) wrongly labelled dogs and \\(\\tfrac{2}{20}\\) wrongly labelled cats.\nThis could be expected, see Figure 11 for the separation of the principal values for the two basis.\nOf course we have very limited data with only 80 images for each of the classes. In this case we should do a cross-validation and we have not shuffled the data.\nLet us see how selecting different test and training sets influence the behaviour.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nE = np.zeros(100)\nfor j in range(100):\n    r1 = np.random.permutation(80)\n    r2 = np.random.permutation(80) + 60\n    ind1 = r1[:60]\n    ind2 = r2[:60]\n    ind1t = r1[60:80]\n    ind2t = r2[60:80]\n    \n    xtrain = np.concatenate((v_w[ind1, :][:, [1, 3]], v_w[ind2, :][:, [1, 3]]))\n    test = np.concatenate((v_w[ind1t, :][:, [1, 3]], v_w[ind2t, :][:, [1, 3]]))\n    label = np.repeat(np.array([1, -1]), 60)\n\n    lda = LinearDiscriminantAnalysis()\n    test_class = lda.fit(xtrain, label).predict(test)\n\n    truth = np.repeat(np.array([1, -1]),20)\n    E[j] = 100 * (1 - np.sum(np.abs(test_class - truth) / 2) / 40)\n\nplt.figure()\nplt.bar(range(100), E)\nplt.plot([0, 100], [E.mean(), E.mean()], \"r-.\", label=\"mean\")\nplt.plot([0, 100], [50, 50], \"y-.\", label='\"coin toss\"')\nplt.xlim((-1, 100))\nplt.ylim((45, 90))\nplt.gca().set_aspect(100 / (45 * 3))\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"trial number\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: Cross validation for the data set in wavelet basis, use 100 run with different training and test sets. We always use 120 images for training and 40 for testing.\nWith a maximal accuracy of 90.0% and a minimal accuracy of 65.0% our initial result with 82.5% was quite good and above average (77.1%). We can also see that training the model is always better than just a simple coin toss or random guessing for cat or dog.\nInstead of a linear discriminants, we can also use quadratic discriminants. To show the difference let us look at the classification line of the two methods for our data in wavelet basis\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nxtrain = np.concatenate((v_w[:60, [1, 3]], v_w[80:140, [1, 3]]))\nlabel = np.repeat(np.array([1, -1]), 60)\ntest = np.concatenate((v_w[60:80, [1, 3]], v_w[140:160, [1, 3]]))\n\nplt.figure()\nplt.scatter(v_w[:80, 1], v_w[:80, 3], label=\"dogs\")\nplt.scatter(v_w[80:, 1], v_w[80:, 3], label=\"cats\")\n\nlda = LinearDiscriminantAnalysis().fit(xtrain, label)\nK = -lda.intercept_[0]\nL = -lda.coef_[0]\nx = np.arange(-0.12, 0.25, 0.005)\nplt.plot(x, -(L[0] * x + K) / L[1], \"k\", label=\"classification line\")\nplt.ylim([-0.25, 0.2])\nplt.xlim([-0.15, 0.25])\nplt.xlabel(r\"$PC_2$\")\nplt.ylabel(r\"$PC_4$\")\nplt.legend()\n\nplt.figure()\nplt.scatter(v_w[:80, 1], v_w[:80, 3], label=\"dogs\")\nplt.scatter(v_w[80:, 1], v_w[80:, 3], label=\"cats\")\n\nqda = QuadraticDiscriminantAnalysis().fit(xtrain, label)\nDecisionBoundaryDisplay.from_estimator(\n        qda,\n        xtrain,\n        grid_resolution=2000,\n        ax=plt.gca(),\n        response_method=\"predict\",\n        plot_method=\"contour\",\n        alpha=1.0,\n        levels=[0],\n    )\nplt.ylim([-0.25, 0.2])\nplt.xlim([-0.15, 0.25])\nplt.xlabel(r\"$PC_2$\")\nplt.ylabel(r\"$PC_4$\")\nplt.legend([\"dogs\", \"cats\", \"classification line\"])\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: Classification line for the LDA together with actual instances.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Classification line for the QDA together with actual instances.\nAs we can see in Figure 2.5, having a quadratic discriminant classification line can be rather beneficial, like always depending on the observations. The QDA arises from LDA when we do not assume that the covariance of each of the classes is the same.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#sec-clustering-sl-lda",
    "href": "clustering/supervised.html#sec-clustering-sl-lda",
    "title": "2  Supervised learning",
    "section": "",
    "text": "Important\n\n\n\nThe following introduction and illustrational data set, as well as the basic structure of the code is from (Brunton and Kutz 2022, Code 5.9). Also see GitHub.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor supervised learning, it is always good practice to split up our data set into a training and testing section. It is important to have a test set that the algorithm has never seen! In general a \\(80:20\\) split is common but other ratios might be advisable, depending on the data set.\nIt is also common practice to use \\(k\\)-folds cross-validation for the training set.\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (\\(k\\)-folds cross-validation) The \\(k\\)-folds cross-validation technique is a method to allow better hyperparameter tuning, especially for smaller data sets where training and validation data is small. The main idea is that we split our iterate over different validation sets by splitting up our training set. Lets say we use \\(5\\)-fold cross-validation we split the training set into 5 parts. Train with 4 parts and validate against the 5th. We than rotate and select a different validation fold. At the end we average over the 5 iterations to get our final parameters.\nThis looks something like this:\n\n\n\n\n\n\nFigure 2.1: Common split with the folds 1 to 4 for training and 5 for validation in the first iteration and folds 2 to 5 for training and 1 for validation in the last. The test set is not touched.\n\n\n\nIt is important that the test set is not included in the folds to make sure we test against observations that the algorithm has never seen!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLDA and QDA assume a normal distribution as the basis for each of the clusters. This allows us to write it also as an update procedure with Bayes2 theorem.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhere for the LDA it is possible to get the correct function for the classification line this is tricky for the QDA. Luckily the scikit-learn class/function DecisionBoundaryDisplay.from_estimator can help in such cases.\n\n\n\n\n\n\n\n\n\nExercise 2.1 (Application of LDA)  \n\nApply the LDA algorithm to the toy example (see Figure 1.1) to recover the two clusters as good as possible.\nAdditionally, for a higher dimensional problem, using LDA split the Fischer Iris data set (Section 1.1) into two clusters. Try for the harder split between versicolor and virginica types of flowers.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#sec-clustering-sl-performance",
    "href": "clustering/supervised.html#sec-clustering-sl-performance",
    "title": "2  Supervised learning",
    "section": "2.2 Measuring Performance",
    "text": "2.2 Measuring Performance\nAs in most applications, the question how good an algorithm performs is not easy to establish. In Figure 2.3, we implied we are doing better than a coin toss but we should be able to characterize this more precise.\n\n\n\n\n\n\nImportant\n\n\n\nThe following approach and the basic structure of the code is from Geron (2022), see GitHub.\n\n\nIn order to illustrate basic properties found in machine learning we use the MNIST data set together with a binary classifier based on Stochastic Gradient Descent, see Section 1.5 for more on the MNIST data set.\n\n\n\n\n\n\nNote\n\n\n\nStochastic Gradient Descent (SGD)3 is an optimization algorithm. The key idea is to replace the actual gradient by a stochastic approximation in the optimization of the loss function. This allows especially good performance for large-scale learning and sparse machine learning problems.\nWe can use this training method for classification to find the optimal parameters for our loss function and in turn, this can be used as a binary classifier.\nAs SGD methods are prone to a sensitivity in feature scaling and order we need to make sure to use normalized data and we should shuffle.\nIn scikit-learn we can use the class SGDClassifier.\n\n\nFirst we load the MNIST data set again and split it into a training and testing section, see Section 1.5.\n\n\nShow the code for loading and splitting the dataset\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import SGDClassifier\nnp.random.seed(6020)\n\nmnist = fetch_openml('mnist_784', as_frame=False)\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\n\nNext, as we only want a binary classifier, we select one number, in our case 5 and relabel our data. With the new labels we can train our classifier, compare Definition 2.9.\n\ny_train_5 = (y_train == \"5\")\ny_test_5 = (y_test == \"5\")\n\nSGD = SGDClassifier(random_state=6020)\nSGD.fit(X_train, y_train_5)\n\nIn order to get a score for our method, we use \\(k\\)-folds cross-validation Definition 2.1 and the corresponding scikit-learn function cross_val_score to perform this task for our model.\n\nscores = sklearn.model_selection.cross_val_score(\n    SGD, X_train, y_train_5, cv=5, scoring=\"accuracy\")\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\naccuracy [%]\n97.07\n96.23\n95.46\n96.62\n96.37\n\n\n\n\n\n\n\nWith scores in the high \\(90\\%\\) range the results look promession if not great, but are they really that good? In order to get a better idea just always guess that we do not see a 5 that should be the most common class in our case. To simulate this we use the DummyClassifier class.\n\ndummy = sklearn.dummy.DummyClassifier()\ndummy.fit(X_train, y_train_5)\nscores_dummy = sklearn.model_selection.cross_val_score(\n    dummy, X_train, y_train_5, cv=5, scoring=\"accuracy\")\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\naccuracy [%]\n90.96\n90.97\n90.97\n90.97\n90.97\n\n\n\n\n\n\n\nAs this is pretty much \\(91\\%\\) (as expected there are only about \\(10\\%\\) of 5s in the data set). Just using accuracy is apparently not the gold standard to measure performance, what other possibilities are there?\n\n\n\n\n\n\n\nDefinition 2.2 (Confusion Matrix) The confusion matrix, error matrix or for unsupervised learning sometimes called matching matrix allows an easy way of visualizing the performance of an algorithm.\nThe rows represent the true observations in each class, and the columns the predicted observations for each class.\nIn our \\(2\\times 2\\) case we get\n\n\n\n\n\n\nFigure 2.6: Names and abbreviations for a \\(2\\times 2\\) confusion matrix together with an example form our test case.\n\n\n\nbut it can be extended for multi-class segmentation.\n\n\n\n\nTo compute the confusion matrix we first need predictions. This can be achieved by cross_val_predict instead of cross_val_score and than we use sklearn.metrics.confusion_matrix.\nCombined, for our example, this reads as:\n\ny_train_pred = sklearn.model_selection.cross_val_predict(\n                    SGD, X_train, y_train_5, cv=3)\ncm = sklearn.metrics.confusion_matrix(y_train_5, y_train_pred)                   \n\n\n\n\n\n\n\n\n\n\nPP\nPN\n\n\n\n\nP\n53617\n962\n\n\nN\n1236\n4185\n\n\n\n\n\n\n\nFrom the values in the confusion matrix a lot of metrics can be computed4:\n\nAccuracy: \\[\nACC = \\frac{TP + TN}{P + N}\n\\]\nTrue positive rate (TPR) or recall: \\[\nTPR = \\frac{TP}{P}\n\\]\nFalse negative rate (FNR): \\[\nFNR = \\frac{FN}{P}\n\\]\nFalse positive rate (FPR): \\[\nFPR = \\frac{FP}{N}\n\\]\nTrue negative rate (TNR): \\[\nTNR = \\frac{TN}{N}\n\\]\nPositive predictive value (PPV) or precission: \\[\nPPV = \\frac{TP}{TP + FP}\n\\]\nFalse discovery rate (FDR): \\[\nFDR = \\frac{FP}{TP + FP}\n\\]\nFalse omission rate (FOR): \\[\nFOR = \\frac{FN}{TN + FN}\n\\]\nNegative predictive value (NPV): \\[\nNPV = \\frac{TN}{TN + FN}\n\\]\n\\(F_1\\) score: \\[\nF_1 = \\frac{2 TP}{2 TP + FP + FN}\n\\]\n\nIn the sklearn.metrics most of these values have a corresponding function. If we look at precission, recall, and the \\(F_1\\) score, for our example we see that our performance is viewed under a different light:\n\nprecision = sklearn.metrics.precision_score(y_train_5, y_train_pred)\nrecall = sklearn.metrics.recall_score(y_train_5, y_train_pred)\nf1_score = sklearn.metrics.f1_score(y_train_5, y_train_pred)\n\nThis tells us that our classifier correctly classifies a 5 81.31% of the time. On the other hand it only recalls or detects 77.2% of our 5s. The \\(F_1\\)-score is a combination of the two (the harmonic mean) and in our case 79.2%.\nDepending on the application we might want to have high precision (e.g. medical diagnosis to have no unnecessary treatment) or high recall (e.g. fraud detection where a missed fraudulent transaction can be costly). If we increase precision we reduce recall and the other way round so we can hardly have both. This dilemma is called precision/recall trade-off, see Section A.2 for some more explanations.\nAn alternative way to look at accuracy for binary classifiers is to look at the receiver operating characteristic (ROC). It looks at recall (TPR) vs. the false positive rate (FPR). Other than that it works similar.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#sec-clustering-sv-svm",
    "href": "clustering/supervised.html#sec-clustering-sv-svm",
    "title": "2  Supervised learning",
    "section": "2.3 Support Vector Machine (SVM)",
    "text": "2.3 Support Vector Machine (SVM)\nThe basic idea of Support Vector Machines (SVMs) is to split observations into distinct clusters via hyperplanes. The have a long history in data science and come in different forms and fashions. Over the years they became more flexible and are still one of the most used tools in industry and science.\n\n2.3.1 Linear SVM\nWe start of with the linear SVM where we construct a hyperplane \\[\n\\langle w, x\\rangle + b = 0\n\\] with a vector \\(w\\) and a constant \\(b\\). There is a natural degree of freedom in this selection of the hyperplane, see Figure 2.7 for two different choices.\n\n\n\n\n\n\n\n\n\n(a) Hyperplane with small margin.\n\n\n\n\n\n\n\n\n\n(b) Hyperplane with large margin.\n\n\n\n\n\nFigure 2.7: We see the hyperplane for the SVM classification scheme. The margin is much larger in the second choice.\n\n\n\nThe optimization inside the SVM aims to find the line that separates the classes best (fewest wrong classifications) and also keeps the largest margin between the observations (the yellow region). The vectors touching the edge of the yellow regions are called support vectors giving the name to the algorithm.\nWith the hyperplane it is easy to classify an observation by simply computing the sign of the projection, i.e. \\[\ny_j (\\langle w, x_j \\rangle + b) = \\operatorname{sign}(\\langle w, x_j \\rangle + b) = \\begin{cases} +1\\\\-1\\end{cases},\n\\] where \\(1\\) corresponds to the versicolor (orange) and \\(-1\\) setosa (blue) observations in Figure 2.7. Therefore, the classifier depends on the position of the observation and is not invariant under scaling.\n\n\n\n\n\n\n\nExercise 2.2 (Linear SVM)  \n\nCompute the vector \\(w\\) in the two cases of Figure 2.7. The vector \\(w\\) is normal to the line. For Figure 2.7 (a) two points on the line are \\(v_1 = [1.25, 4.1]^\\mathrm{T}\\), \\(v_2 = [5, 7.4]^\\mathrm{T}\\). For Figure 2.7 (b) two points on the line are \\(z_1 = [2.6, 4.25]^\\mathrm{T}\\), \\(z_2 = [2, 7]^\\mathrm{T}\\).\nClassify the two points \\[\na = [1.4, 5.1]^\\mathrm{T},\n\\] \\[\nb = [4.7, 7.0]^\\mathrm{T}.\n\\]\n\n\n\n\n\nStating the optimization function such that it is smooth for a linear SVM is a bit tricky. On the other hand, this is needed to allow for most optimization algorithm to work, as they require a gradient to some sort.\nTherefore, the following formulation is quite common. \\[\n\\underset{w, b}{\\operatorname{argmin}} \\sum_j H(y_j, \\overline{y}_j) + \\frac12\\|w\\|^2 \\quad \\text{subject to}\\quad \\min_j|\\langle x_j, w\\rangle| = 1,\n\\] with \\(H(y_j, \\overline{y}_j) = \\max(0, 1 - \\langle y_j, \\overline{y}_j\\rangle)\\), the so called Hinge loss function for counting the number of errors. Furthermore, \\(\\overline{y}_j = \\operatorname{sign}(\\langle w, x_j\\rangle + b)\\).\n\n\n2.3.2 Nonlinear SVM\nIn order to extend the SVM to more complex classification curves, the feature space can be extended. In order to do so, SVM introduces nonlinear features and computes the hyperplane on these features via a mapping \\(x \\to \\Phi(x)\\) and the hyperplane function becomes \\[\nf(x) = \\langle w, \\Phi(x)\\rangle + b,\n\\] and accordingly we classify along \\[\n\\operatorname{sign}(\\langle w, \\Phi(x_j)\\rangle + b) = \\operatorname{sign}(f(x_j)).\n\\]\nEssentially, we change the feature space such that a separation is (hopefully) easier. To illustrate this we use a simple one dimensional example as shown in Figure 2.8 (a). Clearly there is no linear separation possible. On the other hand, if we use \\[\n\\Phi(x_j) = (x_j, x_j^2)\n\\] as our transformation function we move to 2D space and the problem can easily be solved by a line at \\(y=0.25\\).\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nx = np.linspace(-1, 1, 11, endpoint=True)\nx2 = np.zeros_like(x)\ny = np.zeros_like(x)\ny[np.abs(x) &lt; 0.5] = 1\n\nplt.figure()\nplt.scatter(x[y==0], x2[y==0], label=\"class 1\")\nplt.scatter(x[y==1], x2[y==1], label=\"class 2\")\nplt.ylim([-0.1, 1])\nplt.xlim([-1, 1])\nplt.gca().set_aspect(2/3.3)\nplt.legend()\n\nx2 = np.power(x, 2)\n\nplt.figure()\ndata = np.stack([x.flatten(), x2.flatten()]).T\nsvm = LinearSVC(random_state=6020).fit(data, y.flatten())\nplt.scatter(x[y==0], x2[y==0], label=\"class 1\")\nplt.scatter(x[y==1], x2[y==1], label=\"class 2\")\nplt.ylim([-0.1, 1])\nplt.xlim([-1, 1])\nw = svm.coef_[0]\nd = svm.intercept_[0]\nline = lambda x: -w[0] / w[1] * x - d / w[1]\nplt.plot([-1, 1], [line(-1), line(1)], \"k\", label=\"classification line\")\nplt.plot([-1, 1], [0.25, 0.25], \"k:\", label=\"actual boundary line\")\nplt.gca().set_aspect(2/3.3)\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Observations that can not be separated linearly.\n\n\n\n\n\n\n\n\n\n\n\n(b) Enriched feature set with Φ(x)=(x, x^2).\n\n\n\n\n\n\nFigure 2.8: Nonlinear classification with SVM.\n\n\n\n\nAs can be seen in Figure 2.8 (b) the SVM does a great job in finding a split for the two classes, even though not selecting the optimal line, which is not surprising for the given amount of observations.\nAs mentioned before, SVMs are sensitive to scaling. Let us use this example to illustrate the difference together with the concept of pipelines often used in data science context.\n\n\n\n\n\n\nPipeline\n\n\n\nThe main idea of a pipeline is to create a composite as a ordered chain of transformations and estimators, see docs for some more insights.\n\n\nWe can use the pipeline to\n\ncreate the polynomial observations\napply a scaler to our observations\napply the Linear SVM\n\n\ncomposit_svm = sklearn.pipeline.make_pipeline(\n    sklearn.preprocessing.PolynomialFeatures(2),\n    sklearn.preprocessing.StandardScaler(),\n    LinearSVC(random_state=6020)\n)\n\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nx = np.linspace(-1, 1, 11, endpoint=True).reshape(-1, 1)\ny = np.zeros_like(x).flatten()\ny[np.abs(x.flatten()) &lt; 0.5] = 1\n\ncomposit_svm.fit(x, y)\n\nxx = composit_svm[:2].fit_transform(x)\n\nplt.figure()\nplt.scatter(xx[y==0, 1], xx[y==0, 2], label=\"class 1\")\nplt.scatter(xx[y==1, 1], xx[y==1, 2], label=\"class 2\")\n\nw = composit_svm['linearsvc'].coef_[0][1:]\nd = composit_svm['linearsvc'].intercept_[0]\nplt.plot([-1.6, 1.6], [line(-1.5), line(1.5)],\n         \"k\", label=\"scaled classification line\")\n\nplt.ylim([-1.25, 1.75])\nplt.xlim([-1.6, 1.6])\nplt.gca().set_aspect(3/9)\nplt.legend()\n\nxx = composit_svm[:1].fit_transform(x)\nplt.figure()\nplt.scatter(xx[y==0, 1], xx[y==0, 2], label=\"class 1\")\nplt.scatter(xx[y==1, 1], xx[y==1, 2], label=\"class 2\")\n\nw = composit_svm['linearsvc'].coef_[0][1:]\nd = composit_svm['linearsvc'].intercept_[0]\na = composit_svm[\"standardscaler\"].inverse_transform(\n    [np.array([0, -1.6, line(-1.6)])])[0]\nb = composit_svm[\"standardscaler\"].inverse_transform(\n    [np.array([0, 1.6, line(1.6)])])[0]\nplt.plot([a[1], b[1]], [a[2], b[2]], \n         \"k\", label=\"scaled classification line\")\n\nw = svm.coef_[0]\nd = svm.intercept_[0]\nplt.plot([-1, 1], [line(-1), line(1)], \n            \"k:\", label=\"unscaled classification line\")\n\nplt.ylim([-0.1, 1])\nplt.xlim([-1, 1])\nplt.gca().set_aspect(2/3.3)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Classification in the enriched Φ(x)=(x^0, x^1, x^2) and scaled space. Note the first dimension is ignored.\n\n\n\n\n\n\n\n\n\n\n\n(b) Difference between the classification lines when transformed back into the original space.\n\n\n\n\n\n\nFigure 2.9: Classification with autoscaler vs. no scaler.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.3 (Nonlinear SVM)  \n\nExtend the above findings to an example in 2D with a circular classification line. Create tests data of your classification by changing to np.linspace(-1, 1, 12).\n\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nimport sklearn\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnp.random.seed(6020)\nx = np.linspace(-1, 1, 11, endpoint=True)\nx2 = np.linspace(-1, 1, 11, endpoint=True)\nXX, XX2 = np.meshgrid(x, x2)\nZZ = np.pow(XX, 2) + np.pow(XX2, 2)\ny = np.zeros_like(XX)\n\ni = np.sqrt(ZZ) &lt; 0.5\ny[i] = 1\nplt.scatter(XX[y == 0], XX2[y == 0], label=\"class 1\")\nplt.scatter(XX[y == 1], XX2[y == 1], label=\"class 2\")\n\ndata = np.stack([XX.flatten(), XX2.flatten(), ZZ.flatten()]).T\nlabel = y.flatten()\n\n\n\n\n\n\n\n\nFigure 2.10: Data set in 2D\n\n\n\n\n\n\nRecall the moons example from Section 1.3 and use a degree \\(3\\) PolynomialFeatures for classification.\n\nIn both cases, plot the classification line in a projection onto the original 2D space.\n\n\n\n\n\n\n2.3.3 Kernel Methods for SVM\nWhile enriching the feature space is, without doubt, extremal helpful the curse of dimensionality is quickly starting to influence the performance. The computation of \\(w\\) is getting harder. The so called kernel trick is solving this problem. We express \\(w\\) in a different basis and solve for the parameters of the basis, i.e. \\[\nw = \\sum_{j=1}^m \\alpha_j \\Phi(x_j)\n\\] where \\(\\alpha_j\\) are called the weights of the different nonlinear observable functions \\(\\Phi(x_j)\\). Our \\(f\\) becomes \\[\nf(x) = \\sum_{j=1}^m \\alpha_j \\langle \\Phi(x_j), \\Phi(x) \\rangle + b.\n\\] The so called kernel function is defined as the inner product involved, i.e. \\[\nK(x_j, x) = \\langle \\Phi(x_j), \\Phi(x) \\rangle.\n\\] The optimization problem for \\(w\\) now reads \\[\n\\underset{\\alpha, b}{\\operatorname{argmin}} \\sum_j H(y_j, \\overline{y}_j) + \\frac12\\left\\|\\sum_{j=1} \\alpha_j \\Phi(x_j)\\right\\|^2 \\quad \\text{subject to}\\quad \\min_j|\\langle x_j, w\\rangle| = 1,\n\\] with \\(\\alpha\\) representing the vector of all the \\(\\alpha_j\\). The important part here is that we now minimize of \\(\\alpha\\), which is easier.\nThe kernel function allow almost arbitrary number of observables as it, for example, can represent a Taylor series expansion. Furthermore, it allows an implicit computation in higher dimensions by simply computing the inner product of differences between observations.\nOne of these functions are so called radial basis functions (RBF) with the simplest being a Gaussian kernel \\[\nK(x_j, x) = \\exp\\left(-\\gamma\\|x_j - x\\|^2\\right).\n\\]\nIn scikit-learn this is supported via the SVC class.\nLet us test this implementation with the help of our dogs and cats example.\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn import svm\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\nCD = np.concatenate((dogs_w, cats_w), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\nv = VT.T\n\nfeatures = np.arange(1, 21)\nxtrain = np.concatenate((v[:60, features], v[80:140, features]))\nlabel = np.repeat(np.array([1, -1]), 60)\nxtest = np.concatenate((v[60:80, features], v[140:160, features]))\ntruth = np.repeat(np.array([1, -1]), 20)\n\nsvc = svm.SVC(kernel=\"rbf\", gamma=\"auto\").fit(xtrain, label)\ntest_label = svc.predict(xtest)\ntrain_label = svc.predict(xtrain)\ncm = sklearn.metrics.confusion_matrix(test_label, truth)\nplt.figure()\nplt.scatter(xtrain[train_label == 1, 1], xtrain[train_label == 1, 3],\n            alpha=0.5, color=\"C0\", label=\"train_dogs\")\nplt.scatter(xtrain[train_label == -1, 1], xtrain[train_label == -1, 3],\n            alpha=0.5, color=\"C1\", label=\"train_cats\")\nplt.scatter(xtest[test_label == 1, 1], xtest[test_label == 1, 3],\n            color=\"C0\", label=\"test_dogs\")\nplt.scatter(xtest[test_label == -1, 1], xtest[test_label == -1, 3],\n            color=\"C1\", label=\"test_cats\")\nerror = np.vstack((xtrain[label != train_label, :][:, [1,3]],\n                   xtest[truth != test_label, :][:, [1, 3]]))\nplt.scatter(error[:, 0], error[:, 1],\n            color=\"k\", marker=\"x\", label=\"wrong classification\")\nplt.legend()\nplt.ylim([-0.25, 0.2])\nplt.xlim([-0.15, 0.25])\nplt.xlabel(r\"$PC_2$\")\nplt.ylabel(r\"$PC_4$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.11: Training a SVM with an RBF kernel for the singular vectors 2 to 22. The picture shows the classification results projected for the principal components 2 and 4.\n\n\n\n\n\nWe get a confusion matrix for our test set as\n\n\n\n\n\n\n\n\n\nPP\nPN\n\n\n\n\nP\n18\n3\n\n\nN\n2\n17\n\n\n\n\n\n\n\nIn Figure 2.11 we can see the results of the classification for the entire set of observations, shaded for the training set, and crosses marking the wrongly classified data. With 8 wrongly classified images we have quite a good result, compared to LDA or QDA Figure 2.5. Note, the classification is hard to recognise for the two classes in the simple projection. With the parameters C and gamma we can influence the classification.\n\n\n\n\n\n\n\nExercise 2.4 (Nonlinear SVM with RBF) Recall the moons example from Section 1.3 and use a SVC classification to distinguish the clusters. Look at four different results for \\(\\gamma \\in \\{0.1, 5\\}\\) and \\(C \\in \\{0.001, 1000\\}\\), compare Geron (2022).\nIn all of the four images plot the classification line in a projection onto the original 2D space.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.5 (Nonlinear SVM for regression) We can use SVM for regression. Have a look at docs and use the findings to fit the following observations with various degrees and kernel functions.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 100\nx = 6 * np.random.rand(m) - 3\ny = 1/2 * x ** 2 + x + 2 + np.random.randn(m)\nfig = plt.figure()\nplt.scatter(x, y, label=\"observations\")\nplt.show()\n\n\n\n\n\n\n\n\nCompare (Kandolf 2025, Example 5.2) Link.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#decision-trees",
    "href": "clustering/supervised.html#decision-trees",
    "title": "2  Supervised learning",
    "section": "2.4 Decision trees",
    "text": "2.4 Decision trees\nDecision trees are a common tool in data science, for classification and regression. They are a powerful class of algorithms, that can fit not only numerical data. Furthermore, they form the basis of random forests, one of the most powerful machine learning algorithms available to date.\nThey where not invented for machine learning but have been a staple in business for centuries. Their basic idea is to establish an algorithmic flow chart for making decisions. The criteria that creates the splits in each branch is related to a desired outcome and are therefore important. Often experts are called upon creating such a decision tree.\nThe decision tree learning follows the same principals to create a predictive classification model based on the provided observations and labels. Similar to DBSCAN, they form a hierarchical structure that tries to split in an optimal way. In this regard, they are the counterpart to DBSCAN, as they move from top to bottom and of course use the labels to guide the process.\nThe following key feature make them wildly use:\n\nThe usually produce interpretable results (we can draw the graph).\nThe algorithm mimics human decision making, which helps for the interpretation.\nThe can handle numerical and categorical data.\nThey perform well with large sets of data.\nThe reliability of the classification can be assessed with statistical validation.\n\nWhile there are a lot of different optimizations the base algorithm follows these steps:\n\nLook through all components (features) of an observation \\(x_j\\) that gives the best labeling prediction \\(y_j\\).\nCompare the prediction accuracy over all observations, the best result is used.\nProceed with the two new branches in the same fashion.\n\nLet us apply it to the Fischer Iris data set to better understand what is happening.\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom six import StringIO\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\nX_iris = iris.data.values\ny_iris = iris.target\n\ndecision_tree = DecisionTreeClassifier(max_depth=2, random_state=6020)\ndecision_tree.fit(X_iris, y_iris)\n\ndot_data = \"fischer_tree.dot\"\n\nsklearn.tree.export_graphviz(decision_tree, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names=iris.feature_names,\n                class_names=iris.target_names,\n                special_characters=True)\n\n\n\n\n\n\n\n\n\n\nTree\n\n\n\n0\n\npetal length (cm) ≤ 2.45\ngini = 0.667\nsamples = 150\nvalue = [50, 50, 50]\nclass = setosa\n\n\n\n1\n\ngini = 0.0\nsamples = 50\nvalue = [50, 0, 0]\nclass = setosa\n\n\n\n0-&gt;1\n\n\nTrue\n\n\n\n2\n\npetal width (cm) ≤ 1.75\ngini = 0.5\nsamples = 100\nvalue = [0, 50, 50]\nclass = versicolor\n\n\n\n0-&gt;2\n\n\nFalse\n\n\n\n3\n\ngini = 0.168\nsamples = 54\nvalue = [0, 49, 5]\nclass = versicolor\n\n\n\n2-&gt;3\n\n\n\n\n\n4\n\ngini = 0.043\nsamples = 46\nvalue = [0, 1, 45]\nclass = virginica\n\n\n\n2-&gt;4\n\n\n\n\n\n\n\n\nFigure 2.12: Decision tree for the Fischer iris data set and depth 2.\n\n\n\n\n\n\n\n\n\n\n\nDisplaying dot files\n\n\n\nIn the code for Figure 2.12 we generate a dot file, that is interpreted with quarto. This allows a better visual integration. In order to do the same offline you need to install graphviz for the installation of dot and also install the python package pydotplus.\nThan you should be able to use:\nimport pydotplus\nfrom six import StringIO\n\ndot_data = StringIO()\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.create_png()\n\n\nAs we can see, for our tree with depth \\(2\\), we only need to split along petal length (cm) and petal width (cm), leaving the two other features untouched, compare Figure 2.\nAs we only have the splits happening in these two variables we can also visualize them easily.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(df.iloc[:, 2][index], df.iloc[:, 3][index], label=name)\n\nplt.xlim([0.5, 7.5])\nplt.ylim([0, 2.6])\nth = decision_tree.tree_.threshold[[0, 2, 3, 4]]\nplt.plot([th[0], th[0]], plt.gca().get_ylim(), 'C1-.',\n          linewidth=2, label=\"split 1\")\nplt.plot([th[0], plt.gca().get_xlim()[1]], [th[1], th[1]], 'C2:',\n          linewidth=2, label=\"split 2\")\nplt.plot([4.95, 4.95], [0, th[1]], 'C3--',\n          linewidth=2, label=\"(split 3)\")\nplt.xlabel(iris.feature_names[2])\nplt.ylabel(iris.feature_names[3])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.13: Splits for the Fischer Iris data set with the first two split form the above tree and the third split would be the next step for a larger tree.\n\n\n\n\n\nIn Figure 2.13 we can see the the first two splits and the next split if we would increase the tree. With the first split, we immediately separate setosa with \\(100\\%\\) accuracy. The two other classes are a bit tricky and we can not classify everything correct right away. In total \\(6\\) out of \\(150\\) observations are wrongly classified with this simple tree.\nLet us also apply the tree classification to our cats and dogs example.\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom six import StringIO\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\nCD = np.concatenate((dogs_w, cats_w), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\nv = VT.T\n\nfeatures = np.arange(1, 21)\nxtrain = np.concatenate((v[:60, features], v[80:140, features]))\nlabel = np.repeat(np.array([1, -1]), 60)\nxtest = np.concatenate((v[60:80, features], v[140:160, features]))\ntruth = np.repeat(np.array([1, -1]), 20)\n\ndecision_tree_cvd = DecisionTreeClassifier(max_depth=2).fit(xtrain, label)\ntest_label = decision_tree_cvd.predict(xtest)\ncm = sklearn.metrics.confusion_matrix(test_label, truth)\n\ntest_label = svc.predict(xtest)\ncm = sklearn.metrics.confusion_matrix(test_label, truth)\n\ndot_data = \"cvsd_tree.dot\"\n\nsklearn.tree.export_graphviz(decision_tree_cvd,\n                out_file=dot_data,  \n                filled=True, rounded=True,\n                class_names=[\"dog\", \"cat\"],\n                special_characters=True)\n\nscore = decision_tree_cvd.score(xtest, truth)\n\n\n\n\n\n\n\n\n\n\nTree\n\n\n\n0\n\nx\n0\n ≤ 0.049\ngini = 0.5\nsamples = 120\nvalue = [60, 60]\nclass = dog\n\n\n\n1\n\nx\n2\n ≤ 0.019\ngini = 0.346\nsamples = 72\nvalue = [16, 56]\nclass = cat\n\n\n\n0-&gt;1\n\n\nTrue\n\n\n\n4\n\nx\n3\n ≤ 0.133\ngini = 0.153\nsamples = 48\nvalue = [44, 4]\nclass = dog\n\n\n\n0-&gt;4\n\n\nFalse\n\n\n\n2\n\ngini = 0.0\nsamples = 47\nvalue = [0, 47]\nclass = cat\n\n\n\n1-&gt;2\n\n\n\n\n\n3\n\ngini = 0.461\nsamples = 25\nvalue = [16, 9]\nclass = dog\n\n\n\n1-&gt;3\n\n\n\n\n\n5\n\ngini = 0.043\nsamples = 45\nvalue = [44, 1]\nclass = dog\n\n\n\n4-&gt;5\n\n\n\n\n\n6\n\ngini = 0.0\nsamples = 3\nvalue = [0, 3]\nclass = cat\n\n\n\n4-&gt;6\n\n\n\n\n\n\n\n\nFigure 2.14: Decision tree for the Fischer iris data set and depth 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPP\nPN\n\n\n\n\nP\n18\n3\n\n\nN\n2\n17\n\n\n\n\n\n\n\nIf we compare our confusion matrix for the test cases to the one of SVM we get comparable results. In general, we can see that the first split is along \\(PC_2\\) (note that we do not use the \\(PC_1\\) in the code and therefore \\(x_0=PC_2\\)) and our second split is along \\(PC_4\\). We used the same components before. The third split is along \\(PC_5\\), which we did not consider before hand. Overall the mean accuracy for out test set is 77.5%.\n\n\n\n\n\n\n\nExercise 2.6 (A tree on the moon) Recall the moons example from Section 1.3 and use a DecisionTreeClassifier classification to distinguish the clusters and plot the decision splits.\nPlay around with the parameters, e.g. min_samples_leaf = 5 and see how this influences the score for a test set.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.7 (Tree regression) We can use a decision tree for regression. Have a look at docs and use the findings to fit the observations of Exercise 2.5 with various max_depth values and no value here but limiting min_samples_leaf=10.\n\n\n\n\n\n\n\n\n\n\nSensitivity to rotation and initial state\n\n\n\nDue to the nature of decision trees and the way they split observations by lines, they become sensitive to rotation. Furthermore, the tree construction is based on a random process as the feature for the split is chosen at random.\nTo illustrate this we use a slight adaptation of the (Geron 2022, figs. 6–7).\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\ndef plot_tree_bound(tree, X, y):\n    plt.figure()\n    plt.scatter(X[y==0, 0], X[y==0, 1])\n    plt.scatter(X[y==1, 0], X[y==1, 1])\n    DecisionBoundaryDisplay.from_estimator(\n            tree,\n            X,\n            grid_resolution=2000,\n            ax=plt.gca(),\n            response_method=\"predict\",\n            plot_method=\"contour\",\n            alpha=1.0,\n            levels=[0],\n        )\n    l = np.round(np.abs(X).max() + 0.1, 2)\n    plt.xlim([-l, l])\n    plt.ylim([-l, l])\n\n\nX = np.random.rand(100, 2) - 0.5\ny = (X[:, 0] &gt; 0).astype(np.int32)\n\nangle = np.pi / 4\nrotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],\n                            [np.sin(angle), np.cos(angle)]])\nX_rot = X @ rotation_matrix\n\ntree_square = DecisionTreeClassifier(random_state=42).fit(X, y)\ntree_square_rot = DecisionTreeClassifier(random_state=42).fit(X_rot, y)\ntree_square_rot2 = DecisionTreeClassifier(random_state=6020).fit(X_rot, y)\n\npipeline = make_pipeline(StandardScaler(), PCA())\nX_pca = pipeline.fit_transform(X_rot)\ntree_square_pca = DecisionTreeClassifier(random_state=6020).fit(X_pca, y)\n\nplot_tree_bound(tree_square, X, y)\nplot_tree_bound(tree_square_rot, X_rot, y)\nplot_tree_bound(tree_square_rot2, X_rot, y)\nplot_tree_bound(tree_square_pca, X_pca, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Clear split along the middle for a vertical split.\n\n\n\n\n\n\n\n\n\n\n\n(b) More complicated structure for the rotated observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Different initial random state for the rotated observations.\n\n\n\n\n\n\n\n\n\n\n\n(d) Correction of the rotated observations via PCA and scaling, resulting in an easy split.\n\n\n\n\n\n\n\nFigure 2.15: Illustration of the sensitivity to rotation of decision trees. Note both trees split perfectly.\n\n\n\nIn Figure 2.15 (a) we see the split for the original data set - random numbers in \\([-0.5, 0.5]^2\\) and the classes separation for \\(x_1 &gt; 0\\). A simple line, i.e. one split is enough to make the separation. If we rotate the data set by by 45° we can see a way more complicated separation line in Figure 2.15 (b). Furthermore, Figure 2.15 (c) shows that the random_state has an influence as well. Finally, if we apply scaling and PCA to the data we more or less end up at our original sample again Figure 2.15 (d), showcasing the power of these feature extraction techniques once more.\nWe note, all the trees make a perfect classification, just the structure is not as easy to recognize as it could be.\n\n\nAs we could see, trees are very sensitive to the observations and the state. In order to counteract this phenomenon we can compute multiple trees and average the results. This combinations of trees is called ensemble and leads to the next topic.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#ensemble-learning-and-random-forests",
    "href": "clustering/supervised.html#ensemble-learning-and-random-forests",
    "title": "2  Supervised learning",
    "section": "2.5 Ensemble learning and random forests",
    "text": "2.5 Ensemble learning and random forests\nThe notion of wisdom of the crowd suggests, that the decision of multiple people averaged, results in a better decision/judgment than the decisions of a single person. Of course there is a lot of statistics going in, if we want to compute exactly how the result is influenced, or if we should use the same weights for each classification, or e.g. the notion of experts, and much more.\nNevertheless, we can use this concept in our context to create so called ensemble methods, the process itself is called ensemble learning. With this concept we can combine already good classification methods and get a better result than the best classification method included.\nIf we train a multitude of decision trees on various (random) subsets of our observations, we can combine the predictions of the individual trees to an ensemble prediction. The resulting method is called a random forest. This very simple process allows us to generate very powerful classification methods.\nThere are some different approaches for the combination of such methods, we only discuss them briefly, see (Geron 2022, chap. 7) for a more detailed discussion.\n\n\n\n\n\n\nImportant\n\n\n\nAll of the below discussed methods and approaches can be found in the sklearn.ensemble module.\n\n\n\n\n\n\n\n\n\nDefinition 2.3 (Voting classifiers) If we have a set of classifiers \\(C=\\{c_1, \\ldots, c_n\\}\\) of various kinds (even another ensemble classifier like a random forest is welcome), we can simple make a prediction with each resulting in \\(r_1, \\ldots, r_n\\). Now we select the class witch occurs most often, i.e. the mode of the predictions, we get a new classifier. This is called hard voting.\nIf all our classifiers can not only produce a prediction but a probability for our prediction, we can also create a soft voting classifier. All we need to do, average the probability of the predictions \\(p_1, \\ldots, p_n\\), and this will give us new probabilities for our ensemble classifier.\n\n\n\n\n\n\nFigure 2.16: Illustration of the difference between hard and soft voting for a ensemble method. For the three shown classifiers the class 0 is the most common. When moving to probabilities, the mean also predicts 0, where more convinced classifiers get a higher weight.\n\n\n\nIn scikit-learn this is can be found in the sklearn.ensemble.VotingClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.4 (Bagging and Pasting) For bagging and pasting the idea is different. Instead of influencing the output, theses approaches influence how to manipulate the training of a set of (potentially equal) classifiers, to achieve overall better results.\nBagging (bootstrap aggregating) uses sampling with replacement, i.e. the same observation can end up multiple times in the training set of the same classifier. Pasting uses sampling without replacement and therefore an observation can be in multiple classifiers but not more than once per classifier.\nTo predict, we can use hard or soft voting from above.\n\n\n\n\n\n\nFigure 2.17: Illustration of the random sampling for bagging and pasting in ensemble classifiers.\n\n\n\nWith these sampling methods it is possible to use the out-of-bag observations (everything that is not used for a particular training) for evaluation of the trained classifier. This is called out-of-bag evaluation.\nIn scikit-learn this is can be found in the sklearn.ensemble.BaggingClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.5 (Random Patches and Random Subspaces) For bagging and pasting it is also possible to sample features and not only observations, i.e. what to look at. This results in a random subset of input features for training for each classifier. This is especially useful, for high dimensional data inputs such as images, as it can speed up the learning process.\nWe call such methods random patches method if we sample both, training observations and training features.\nOn the other hand, if we keep the training observations fix and only sample the training features the resulting method is called a random subspace method.\nIn scikit-learn this is can be achieved by manipulating the arguments max_features, bootstrap_features, and max_samples in the BagginClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.6 (Random Forest) An ensemble of decision tress, (usually) trained via bagging is called a random forest.\nWith a random forest it is quite easy to find out what features are important for the overall result. In scikit-learn this is automatically computed for the sklearn.ensemble.RandomForestClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.7 (Boosting) Boosting or sometimes hypothesis boosting is the process of using ensemble methods to use slow learners to create a fast learner.\nThe general idea is to train a sequence where the output of one is the input of the next classifier. This corrects the previous result and therefore helps to achieve overall better results.\nThe most common methods are called AdaBoost (adaptive boosting) and gradient boosting.\nIn scikit-learn we can find this functionality in the classes sklearn.ensemble.AdaBoostClassifier and sklearn.ensemble.GradientBoostingClassifier.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.8 (Stacking) The general idea of stacking is, to use a blender for combining the results of our ensemble methods. The blender is not just a linear combination like for soft/hard voting but another classifier/model to perform a hopefully better combination. Of course we can stack this approach and produce multiple layers before we combine them into a single result.\n\n\n\n\n\n\nFigure 2.18: Illustration of an \\(m\\) layer stacking with various classifiers in each layer.\n\n\n\nIn scikit-learn this is can be found in the sklearn.ensemble.StackingClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.8 (Ensemble methods) We follow the example of (Geron 2022, chap. 7) to explore the various possibilities for ensemble methods.\nFor our data set we use the moons example:\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=500, noise=0.30, random_state=6020)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=6020)\n\nUse the VotingClassifier class with an LDA, a SVM, and a tree for classification. Make sure to set random_state for each, to have reproducible results. Get the overall score for the test set, as well as the individual scores for the included classifiers.\nfor name, clf in voting.named_estimators_.items():\n    print(f\"{name}, =, {clf.score(X_test, y_test)}\")\nSwitch to soft voting for your ensemble classifier and see how this influences the results.\nCreate a BaggingClassifier with 500 trees and and an appropriate max_samples value. Report the score for this new classifier, also report the out-of-bag score via .oob_score_.\nCreate directly a RandomForestClassifier with 500 trees and appropriate value for max_leaf_nodes.\nCreate a StackingClassifier with an LDA, a SVM, and a tree for classification and a random forest as the final blending step and report the score of this method.\nTrain a random forest for the Fischer Iris data set and check .feature_importances_ to get an insight on the importance of each feature.\nTrain a random forest for our dogs and cats data set in raw and wavelet form and check .feature_importances_ to get an insight on the importance of each feature of the PCA.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#multiclass-classification",
    "href": "clustering/supervised.html#multiclass-classification",
    "title": "2  Supervised learning",
    "section": "2.6 Multiclass Classification",
    "text": "2.6 Multiclass Classification\nWe mainly focused on the classification of two classes in the last sections, but obviously not all problems only consist of two classes. Some of the methods discussed, like random forests, support multiclass classification out of the box. For others, there are several approaches to create a multiclass classifier out of a binary classifier.\n\n\n\n\n\n\n\nDefinition 2.9 (One vs. the Rest (OvR) or One vs. All (OvA)) The one versus the rest (OvR) or one versus all (OvA) strategy is to train a binary classifier for each class and always interpret all other classes as the others or the rest class. To classify an observation you get the scores for each classifier and select the one with the highest score.\nFor the Fischer Iris data set this would result in three classifiers (one for each iris), for the MNIST data set in ten (one for each number).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.10 (One vs. One (OvO)) The one versus one (OvO) strategy is to train a binary classifier for always two classes and build up a set of classifiers, for \\(n\\) classes we get \\(\\tfrac{n (n-1)}{2}\\) classifiers. To classify an observation you get the result for each classifier and select the one class with the most duels won.\nFor the Fischer Iris data set this would result in three classifiers, for the MNIST data set 45.\nThe advantage of OvO over OvR is that each classifier only needs to be trained on a subset and not with the entire data set. This is especially useful for algorithms that do not scale well, like SVMs.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn scikit-learn the framework automatically realizes that we train a binary classifier for multiple classes and it will select OvR or OvO automatically, depending on the algorithm.\nNevertheless, there exist dedicated classes for the task as well sklearn.multiclass.OneVsOneClassifier and sklearn.multiclass.OneVsRestClassifier.\n\n\n\n\n\n\n\n\n\nExercise 2.9 (Multiclass classification)  \n\nUse SVM for the Fischer Iris data set and test it.\nCompare with a random forest.\nCreate a convolution matrix for more than two classes with your results and interpret the results.\n\n\n\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#footnotes",
    "href": "clustering/supervised.html#footnotes",
    "title": "2  Supervised learning",
    "section": "",
    "text": "see (Kandolf 2025, Definition 3.5) or the direct link Link↩︎\nsee (Kandolf 2025, Theorem 14.4) or the direct link Link↩︎\nsee (Kandolf 2025, sec. 6.2) or the direct link Link↩︎\nsee Wikipedia overview Link↩︎",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/semisupervised.html",
    "href": "clustering/semisupervised.html",
    "title": "3  Semi-Supervised learning",
    "section": "",
    "text": "There is a hybrid between unsupervised and supervised learning, called semi supervised learning. If we have a data set with just a couple of labelled observations, we might be able to use the discussed clustering methods to extend the labels and therefore generate more labelled data.\nWe try this with the (in)famous MNIST data set of hand written digits. As we did not do this when discussion MNIST in Section 2.2 we first looking at the dataset and establishing some basic properties.\nWe load the data set and look at its description\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml(\"mnist_784\", as_frame=False)\nprint(mnist.DESCR)\n\n**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n**Please cite**:  \n\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n\nDownloaded from openml.org.\n\n\nThe images are stored in .data and the label in .target. We can look at the first 100 digits\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nim = np.zeros((10 * 28, 10 * 28), int)\nfor i in range(10):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = mnist.data[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3.1: First 100 digits from the MNIST data set.\n\n\n\n\n\nBefore we go any further in the investigation, we split up the data set into a training and a test set.\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\nNow let us try with \\(k\\)-means to find the 10 digits.\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nk = 10\nkmeans = KMeans(n_clusters=k, n_init=1, random_state=6020).fit(X_train)\n\nim = np.zeros((1 * 28, 10 * 28), int)\nfor i in range(1):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = \\\n                kmeans.cluster_centers_[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3.2: The cluster means of \\(k\\)-means for 10 clusters.\n\n\n\n\n\nAs can be seen from Figure 3.2, the cluster centers do not recover our 10 digits. It looks like \\(0, 1, 2, 3, 6, 8, 9\\) are easy to discover but \\(4, 5, 7\\) not. If we look closely, we can see \\(4, 5, 7\\) represented in our clusters but not as separate digits. Obviously, this is not a good way to proceed to find our digits. We discussed methods to perform this task in Chapter 2 now let us consider a different scenario.\nOur aim is to only label 50 observations and not more. How can we do this smartly? For this task \\(k\\)-means is a good choice. Instead of trying to find our 10 digits we try to find 50 clusters within our data set. We use the images closest to the mean as our representative and label these images. Now instead of labeling just 50 random digits we labelled 50 cluster centers. These labels we can than spread out onto the rest of the clusters and we can test how the performance is.\n\n\n\n\n\n\nImportant\n\n\n\nDue to the nature of these notes, being compiled interactively, we restrict the data set to 2000 points.\n\nX_train = X_train[:2000]\ny_train = y_train[:2000]\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following approach is presented in a similar way in Geron (2022), see GitHub for code in more details.\n\n\nTo get a baseline for our algorithm we use a random forest for the classification and only work with 50 labels.\n\nfrom sklearn.ensemble import RandomForestClassifier\nn_labels = 50\nforest = RandomForestClassifier(random_state=6020).fit(X_train[:n_labels], y_train[:n_labels])\nscore_forest = forest.score(X_test, y_test)\nprint(f\"score for training with {n_labels} labels {score_forest}\")\n\nforest_full = RandomForestClassifier(random_state=6020).fit(X_train, y_train)\nscore_forest_full = forest_full.score(X_test, y_test)\nprint(f\"score for training with {len(y_train)} labels {score_forest_full}\")\n\nscore for training with 50 labels 0.5618\nscore for training with 2000 labels 0.9117\n\n\nWith our 50 labels we get a bit more than 56.18% correct but of course if we use all labels we can achieve results in the 90% range (if we train with all 60000 we get 97%). So how can we approach this problem?\nInstead of just randomly selecting 50 images with labels, let us create 50 clusters, label the image centers, i.e. get good representatives of the classes we are interested in.\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nk = 50\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=6020)\nX_digits_dist = kmeans.fit_transform(X_train)\n\nrepresent_digit_idx = X_digits_dist.argmin(axis=0)\nX_rep = X_train[represent_digit_idx]\n\nim = np.zeros((5 * 28, 10 * 28), int)\nfor i in range(5):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = X_rep[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3.3: The representative (closest image to the cluster mean) of the 50 clusters found with \\(k\\)-means for the first 1500 digits in the MNIST data set.\n\n\n\n\n\nWe can now label these observations\n\ny_rep = np.array([\n    \"3\", \"4\", \"1\", \"9\", \"9\", \"0\", \"3\", \"0\", \"9\", \"1\",\n    \"9\", \"8\", \"0\", \"9\", \"3\", \"6\", \"6\", \"7\", \"1\", \"6\",\n    \"6\", \"5\", \"3\", \"2\", \"2\", \"0\", \"0\", \"6\", \"7\", \"2\",\n    \"5\", \"1\", \"7\", \"3\", \"4\", \"8\", \"6\", \"0\", \"8\", \"5\",\n    \"0\", \"3\", \"2\", \"3\", \"7\", \"4\", \"5\", \"4\", \"2\", \"7\" \n])\n\nforest_rep = RandomForestClassifier(random_state=6020).fit(X_rep, y_rep)\nscore_forest_rep = forest_rep.score(X_test, y_test)\nprint(f\"score for training with {len(y_rep)} labels {score_forest_rep}\")\n\nscore for training with 50 labels 0.6551\n\n\nThis helped us increase our score significantly, but we can do better. We can extend our labels from the representatives to the entire cluster and train with that.\n\ny_train_prop = np.empty(len(X_train), dtype=str)\nfor i in range(k):\n    y_train_prop[kmeans.labels_ == i] = y_rep[i]\n\nforest_prop = RandomForestClassifier(random_state=6020).fit(X_train, y_train_prop)\nscore_forest_prop = forest_prop.score(X_test, y_test)\nprint(f\"score for training with {len(y_train_prop)} propagated labels {score_forest_prop}\")\n\nscore for training with 2000 propagated labels 0.7672\n\n\nThis again increases our score by another good 10%. If we check our propagated labels, we see that, we can not expect more as we only have an accuracy slightly higher than our classification result, i.e. we provide wrong results labels for our classification.\n\nnp.mean(y_train_prop == y_train)\n\nnp.float64(0.7745)\n\n\nLet us try to eliminate outliers by removing the 10% of instances that are far away from our cluster centers.\n\npercentile_closest = 90\n\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\nfor i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist &gt; cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1\n\npartially_propagated = (X_cluster_dist != -1)\nX_train_pprop = X_train[partially_propagated]\ny_train_pprop = y_train_prop[partially_propagated]\nnp.mean(y_train_pprop == y_train[partially_propagated])\n\nnp.float64(0.8102189781021898)\n\n\nWe have cleaned up our source but does it have a big influence?\n\nforest_pprop = RandomForestClassifier(random_state=6020).fit(X_train_pprop, y_train_pprop)\nscore_forest_pprop = forest_pprop.score(X_test, y_test)\nprint(f\"score for training with {len(y_train_pprop)} labels {score_forest_pprop}\")\n\nscore for training with 1781 labels 0.768\n\n\nWe actually do not change our result with this step much.\nNevertheless, overall we could see that by smartly labeling 50 out of 2000 instances we could increase our score from from about 56.18% to 76.8%, which is not bad.\n\n\n\n\n\n\n\nExercise 3.1 (Improve the results further) There are several ways we can improve our results.\n\nTry to optimize the parameters of the random forest, for number of trees and leaves.\nOptimize the clustering for the first step.\nWe can use other methods we have seen in Chapter 2 and combine them to an ensemble learning.\nBy starting to additionally label observations where our classifier is the least sure about.\nWe can again work with clusters to smartly label additional observations.\n\nNote: if we use all the 60000 samples we get about 84% with the presented steps.\n\n\n\n\n\n\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Semi-Supervised learning</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "T.B.D.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977.\n“Maximum Likelihood from Incomplete Data via the EM\nAlgorithm.” Journal of the Royal Statistical Society: Series\nB (Methodological) 39 (1): 1–22.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996.\n“A Density-Based Algorithm for Discovering Clusters in Large\nSpatial Databases with Noise.” In Proceedings of the Second\nInternational Conference on Knowledge Discovery and Data Mining,\n226–31. KDD’96. Portland, Oregon: AAAI Press.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic\nProblems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow 3e. 3rd ed. Sebastopol, CA:\nO’Reilly Media.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems.\nSebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen\nDatenbasierter Methoden.” Management Center Innsbruck, Course\nMaterial. https://doi.org/10.5281/zenodo.14671708.\n\n\nLandup, David. 2022. Practical Deep Learning for Computer Vision\nwith Python.\n\n\nLloyd, Stuart P. 1982. “Least Squares Quantization in\nPCM.” IEEE Trans. Inf. Theory\n28 (2): 129–36. https://doi.org/10.1109/TIT.1982.1056489.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendices/explanations.html",
    "href": "appendices/explanations.html",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "A.1 Wavelet decomposition for cats and dogs\nIn Section 1.2 we discuss using the wavelet transformation to transform the image into a different basis. Here are the details of how this is performed with cat zero as example.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(np.reshape(cats[:, 0], (64, 64)).T, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\nFigure A.1: The original cat.\nWe use the Haar-Wavelet and we only need to do one level of transformation. As per usual we get four images, each half the resolution, that represent the decomposition. The images are, a downsampled version of the original image, one highlighting the vertical features, one highlighting the horizontal features, and one highlighting the diagonal features.\nFigure A.2: Wavelet transformation of the cat.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.4\n\n\n\n\n\n\n\n\n\n\n\nFigure A.5\nFor our purposes only the vertical and horizontal feature are of interest and we combine these two images. In order to make sure the features are highlighted optimal we need to rescale the images before the combination. For this we use a similar function like the MATLAB wcodemat function.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\nFigure A.6: Combination of vertical and horizontal features unaltered.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.7: Combination of vertical and horizontal features rescaled.\nIn total this leads to the following function to transform a list of the images, given as row vectors.\nimport pywt\nimport math\n\ndef img2wave(images):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\nNote that the resulting image has only one forth of the pixels as the original image. We can also visualize the transformation steps as follows in Figure A.8.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#sec-appendix-dvc",
    "href": "appendices/explanations.html#sec-appendix-dvc",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "Show the code for the figure\nimport pywt\n\n[A_1, (cH1, cV1, cD1)] = pywt.wavedec2(np.reshape(cats[:, 0], (64, 64)).T,\n                                       wavelet=\"haar\", level=1)\nplt.figure()\nplt.imshow(A_1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cH1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cD1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\nShow the code for the figure\nimport pywt\n\nplt.figure()\nplt.imshow(cH1 + cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(rescale(cH1, 256) + rescale(cV1, 256), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\nFigure A.8: Workflow to get from the original image to the wavelet transformed version.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#sec-appendix-pvsr",
    "href": "appendices/explanations.html#sec-appendix-pvsr",
    "title": "Appendix A — More detailed explanations",
    "section": "A.2 Precision/Recall trade-off",
    "text": "A.2 Precision/Recall trade-off\nIn Section 2.2 we discuss the performance topics and we cam across the precision/recall trade-off.\nLets remind ourself of the definitions:\nRecall or true positive rate (TPR) is the rate of relevant instances that are retrieved, or true positive over all occurrences \\[\n\\operatorname{recall} = \\frac{TP}{P} = \\frac{TP}{TP + FN}.\n\\]\nPrecision on the other hand is the rate of relevant instances over all retrieved instances, or true positive over the sum of true positive and false positive. \\[\n\\operatorname{precision} = \\frac{TP}{TP + FP}.\n\\]\nIn order to understand why precision and recall influence each other we need to understand how our classifier works.\nInternally each observation given to the classifier is fed into a decision function that returns a score.\nThe score is on some scale and in the default setting, everything above zero is counted as a match, if the threshold is set differently this can change. See Figure A.9. In the presented example we can have a precision from \\(71\\%\\) to \\(100\\%\\) and at the same time a recall from \\(100\\%\\) to \\(60\\%\\).\n\n\n\n\n\n\nFigure A.9: Some representatives and their score and three different thresholds and the corresponding results for precision and recall.\n\n\n\n\n\nCode that provides the basis for the above figure.\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import SGDClassifier\nnp.random.seed(6020)\n\nmnist = fetch_openml('mnist_784', as_frame=False)\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\ny_train_5 = (y_train == \"5\")\ny_test_5 = (y_test == \"5\")\n\nSGD = SGDClassifier(random_state=6020)\nSGD.fit(X_train, y_train_5)\n\nindices = [22698, 2, 73, 132, 244, 50, 48, 11, 0, 26873]\nSGD.decision_function(X_train[indices])\n\nim = np.zeros((1 * 28, 10 * 28), int)\nfor i, j in enumerate(indices):\n    im[:28, 28*i: 28*(i+1)] = X_train[j].reshape(28,28)\n\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\n\nWe can also plot the entire prevision recall curve\n\n\nShow the code for the figure\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import cross_val_predict\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ny_scores = cross_val_predict(SGD, X_train, y_train_5, cv=5,\n                             method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\nplt.figure()\nplt.plot(thresholds, precisions[:-1], label=\"Precision\")\nplt.plot(thresholds, recalls[:-1], \"--\", label=\"Recall\")\nplt.xlim([-95000, 22000])\nplt.xlabel(\"score\")\nplt.grid()\nplt.legend()\nplt.gca().set_aspect( 117000 / 3)\n\nplt.figure()\nplt.plot(recalls, precisions)\nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.grid()\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.show()\n\n\n\n\n\n\n\n\nFigure A.10: Precision and recall vs. the score of the decision function.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.11: Precision vs. recall.\n\n\n\n\n\nWith the help of the precision vs. recall curve we can select a threshold appropriate for our classification, i.e. level between precision and recall as we see fit and our classification allows.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  }
]