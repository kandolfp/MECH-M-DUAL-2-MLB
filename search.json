[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "",
    "text": "Preface\nThese are the lecture notes for the Maschinelles Lernen in der industriellen Bildverarbeitung class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the summer term 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe want to thank the open source community for providing excellent tutorials and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout the document at hand.\nThese notes are built with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basic concepts behind modern day Data Science and machine learning techniques for industrial image processing. We will always try illustrate the theory directly with Python to show how the concepts work programmatically.\nThe following books cover large sections of the content and serve as the main references for these notes. They are an excellent read to get further into the topics presented, some also include code in Python and MATLAB:\n\nBrunton and Kutz (2022)\nGeron (2022)\nGoodfellow, Bengio, and Courville (2016)\nLandup (2022)\nHuyen (2022)\n\nThese notes are intended for engineering students and therefore the mathematical concepts will rarely include rigorous proofs.\nWe start by discussing Clustering and Classification for 1  Unsupervised learning, followed by 2  Supervised learning. In both sections we discuss some of the most prominent examples, how they work, and embed them in the context and the various aspects of the classification task. In 3  Semi-Supervised learning we address a topic of label quality and showcase how we can work with only a view labels and still provide good data for supervised methods.\nNext we take a look at Data management and data engineering where we discuss the basic strategies to store data and models, and pipelines to process and move data. With dvc we introduce a tool for connecting data, code, and trained models in a reliable and reproducible way. Furthermore, we discuss the importance of an ETL and give a simple yet powerful example with transforming the image basis to Wavelets.\nIn the third and final part of this lecture we focus on Neural Networks and Deep Learning. After introducing neural networks and connecting a very simple network back to known regression techniques we start building our own neural networks in 7  Neural Networks. For this we work with pytorch but also provide a reference in keras in the appendix. We discuss backward propagation as the key idea behind neural networks. After this general introduction and definition of the necessary terms we discuss specific classes 8  Convolutional Neural Networks, 9  Autoencoders, and 10  Transfer learning. For each we provide the general idea and showcase the capabilities. In 11  Data Preparation we address topics regarding the training data and the labelling. This section is the one with the least code as these topics are of a general form and usually already included for the datasets used here. The finish up the section on neural networks, we discuss some 12  Common Challenges in the field. The idea of this last section is to provide some guidance if something is not working as expected.\nWith these notes we can not hope to cover everything of importance with the length it needs but we can give a glimpse into\n\ndata management, labeling and preprocessing\nlife-cycle management of models\nmachine learning operations (MLOps)\ncurrent and research topics\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems. Sebastopol, CA: O’Reilly Media.\n\n\nLandup, David. 2022. Practical Deep Learning for Computer Vision with Python.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "clustering/index.html",
    "href": "clustering/index.html",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Feature selection and generation of a feature space\nOne aspect of machine learning is binning data into meaningful and distinct categories that can be used for decision-making, amongst other things. This process is referred to as clustering and classification. To achieve this task the main modus of operation is to find a low-rank feature space that is informative and interpretable.\nOne way to extract the dominate features from a dataset is the singular value decomposition (SVD) or principal component analysis, see (Kandolf 2025). The main idea is, to not work in the high dimensional measurement space, but instead in the low rank feature space. This allows us to significantly reduce the cost (of computation or learning) by performing our clustering in this low dimensional space.\nIn order to find the feature space there are two main paths for machine learning:\nAs the names suggest, in unsupervised learning, no labels are given to the algorithm and it must find patterns in the data to generate clusters and labels such that predictions can be made. This is often used to discover previously unknown patterns in the (low-rank) subspace of the data and leads to feature engineering or feature extraction. These features can than be used for building a model, e.g. a classifier.\nSupervised learning, on the other hand, uses labelled datasets. The training data is labelled for cross-validation by a teacher or expert. I.e. the input and output for a model is explicitly provided and regression methods are used to find the best model for the given labelled data. There are several different forms of supervised learning like reinforcement learning, semi-supervised learning, or active learning.\nThe main idea behind machine learning is to construct or exploit the intrinsic low-rank feature space of a given dataset, how to find these is determined by the algorithm.\nBefore we go into specific algorithms lets have a general look at what we are going to talk about. We will follow (Brunton and Kutz 2022, sec. 5.1) for this introduction.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "href": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Fischer Iris dataset\nFirst we look into one of the standard datasets well known in the community, the so called Fischer Iris dataset Fisher (1936).\n\n\n\n\n\n\nNote\n\n\n\nLuckily for us there are several data packages available for general use. One of them is the Fisher Iris dataset.\n\n\n\n\n\n\nFigure 1: Illustration of the recorded features and structure of the iris dataset inspired by the Medium article - Exploring the Iris flower dataset.\n\n\n\nWe use the possibilities of sklearn 1 to access the dataset in Python.\n\n\nLet us start right away by loading and visualizing the data in a 3D scatter plot (there are 4 features present so one is not shown).\n\n\nShow the code for the figure\nimport plotly.express as px\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nfig = px.scatter_3d(df, x=\"sepal length (cm)\", y=\"sepal width (cm)\",\n                    z=\"petal width (cm)\", color=\"target\")\ncamera = dict(\n    eye=dict(x=1.45, y=1.45, z=0.1)\n)\nfig.update_layout(scene_camera=camera)\nfig.show()\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 2: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a scatter 3D plot along the features sepal length, sepal width, and petal width, the forth feature petal width is not shown.\n\n\n\n\nAs can be seen in Figure 2 the three features are enough to easily separate Sentosa and to a very good degree Versicolor from Virginica, where the last two have a small overlap in the samples provided.\nThe petal width seems to be the best feature for the classification and no highly sophisticated machine learning algorithms are required. We can see this even better if we use a so called Pair plot from the seaborn package as seen in Figure 3.\n\n\nShow the code for the figure\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\n_ = sns.pairplot(iris.frame, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\nFigure 3: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a pair grid plot. The diagonal shows the univariate distribution of the feature.\n\n\n\n\n\nHere a grid with plots is created where each (numeric) feature is shared across a row and a column. Therefore, in a single plot we can see the combination of two, and in the diagonal we see the univariate distribution of the values. After a close inspection, we can also see here that petal width provides us with the best distinction feature and sepal width with the worst.\nEven though we already have enough to give a good classification for this dataset, it is worth to investigate a bit further. We illustrate how the principal component analysis (PCA)2 can be applied here.\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nA = df.iloc[: , :4].to_numpy()\n\nXavg = A.mean(0)\nB = A - np.tile(Xavg, (150, 1))\nU, S, VT = np.linalg.svd(B, full_matrices=False)\nC = B @ VT[:2, :].T\nq = np.linalg.norm(S[:2])**2 / np.linalg.norm(B, \"fro\")**2\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(C[index, 0], C[index, 1], label=name)\nplt.legend()\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_2$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Feature reduction with PCA for the Iris dataset. We show the first two components principal components.\n\n\n\n\n\nAs can be seen in the Figure 4 the first two components cover about 97.77% of the total variation in the samples and in accordance with these results a very good separation of the three species is possible with them\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 1 (Explained Variance) The explained variance is a measure of how much of the total variance in the data is explained via the principal component. It is equivalent to the singular value associated with the component so in the above example it is nothing else than \\[\n\\rho_i = \\frac{\\sigma_i^2}{\\|\\Sigma\\|^2}.\n\\]\nThe explained variance can be helpful to select the amount of principal values you use for a classification. But be aware, as we will see later, the largest principal value might not be the best to use.\n\n\n\n\n\n\n\n\n\nDimensionality Reduction\n\n\n\nAs we can see with the Fischer Iris dataset, we often have a problem of dimensionality, if we try to visualize something. The data has four dimensions but it is very hard for us to perceive, let alone visualize, four dimensions (if the forth dimension is not time).\nTherefore, dimensional reduction techniques are often applied. One of these is PCA others are:\n\nMultidimensional scaling (MDS) - preserve the distance between observations while doing the reduction.\nIsomap - create a graph by connecting each observation to its neighbors and reduce the dimensions by keeping the geodesic distances, i.e. the number of nodes on the graph between observations\n\\(t\\)-distributed stochastic neighbor embedding (\\(t\\)-SNE) - try to reduce dimensions while keeping similar observations close and dissimilar observations apart.\nLinear discriminant analysis (LDA) - projects data onto a hyperplane where the hyperplane is chosen in such a way that it is most discriminative for the observations see Linear Discriminant Analysis (LDA).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe sklearn package has all the tools to perform a PCA, see Link for an example using the Iris dataset, same as in the beginning of this section.\n\n\nNow let us take a look at a more complicated example from Brunton and Kutz (2022).\n\n\nDogs and Cats\nIn this dataset we explore how to distinguish between images dogs and cats, more particular the head only. The dataset contains \\(80\\) images of dogs and \\(80\\) images of cats, each with \\(64 \\times 64\\) pixels and therefore a total of \\(4096\\) feature measurements per image.\n\n\n\n\n\n\nImportant\n\n\n\nThe following dataset and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.2 - 5.4). Also see GitHub.\n\n\nAs before, a general inspection of the dataset is always a good idea. With image we usually look at some to get an overview.\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef createImage(data, x, y, width=64, length=64):\n    image = np.zeros((width * x, length * y))\n    counter = 0\n    for i in range(x):\n        for j in range(y):\n            img = np.flipud(np.reshape(data[:, counter], (width, length)))\n            image[length * i: length * (i + 1), width * j: width * (j + 1)] \\\n                    = img.T\n            counter += 1\n    return image\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(createImage(cats, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog\"]\nplt.figure()\nplt.imshow(createImage(dogs, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\nFigure 5: The first 16 cats from the dataset.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The first 16 dogs from the dataset.\n\n\n\n\n\n\nAgain, we use PCA to reduce the feature space of our data.\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nCD = np.concatenate((dogs, cats), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=False)\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U[:, j], (64, 64)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 7: First four modes of the PCA of the 160 images.\n\n\n\nIn Figure 7 we can see the first four modes of the PCA. While mode 2 (Figure 7 (b)) highlights the pointy ears common in cats, mode 3 (Figure 7 (c)) is recovers more dog like features. Consequently, these two features make a good choice for classification.\nOf course this is not the only possible representation of the initial data. We can also use the Wavelet transform3.\nThe multi resolution analysis features of the Wavelet transformation can be used to transform the image in such a way that the resulting principal components yield a better separation of the two classes.\n\n\n\n\n\n\nNote\n\n\n\nWe use the following workflow to get from the original image \\(A_0\\) to the \\(\\tilde{A}\\) image in a sort of Wavelet basis.\n\n\n\n\n\n\nFigure 8: Workflow to get from the original image to the wavelet transformed version.\n\n\n\nIn short, we combine the vertical and horizontal rescaled feature images, as we are most interested in edge detection for this example.\nNote that the image has now only a quarter of the pixels of the original image.\nFor a more detailed explanation see Wavelet decomposition for cats and dogs.\n\n\nShow code required for the transformation.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\n\nimport pywt\nimport math\n\ndef img2wave(data):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\n\n\n\n\nLet us try how this changes our first four modes.\n\nShow the code for the figures\nCD_w = img2wave(CD)\nU_w, S_w, VT_w = np.linalg.svd(CD_w - np.mean(CD_w), full_matrices=False)\nl = math.isqrt(CD_w[:, 0].shape[0])\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U_w[:, j], (l, l)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 9: First four modes of the PCA of the 160 images in our wavelet basis.\n\n\n\nRight away we can see that the ears and eyes of the cats show up more pronounced in the second mode Figure 9 (b).\nNow, how does this influence our classification problem? For this let us first see how easy it is to distinguish the two classes in a scatter plot for the first four modes.\n\n\nShow the code for the figure\nimport seaborn as sns\nimport pandas as pd\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\ntarget = [\"cat\"] * 80 + [\"dog\"] * 80\nC = U[:, :4].T @ (CD - np.mean(CD))\ndf = pd.DataFrame(C.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf[\"target\"] = target\n_ = sns.pairplot(df, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\nFigure 10: First four modes of the raw images for cats and dogs.\n\n\n\n\n\n\n\nShow the code for the figure\nimport seaborn as sns\nimport pandas as pd\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\nC_w = U_w[:, :4].T @ (CD_w - np.mean(CD))\ndf_w = pd.DataFrame(C_w.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf_w[\"target\"] = target\n_ = sns.pairplot(df_w, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\nFigure 11: First four modes of the wavelet images for cats and dogs.\n\n\n\n\n\nThe two figures (Figure 10, Figure 11) give us a good idea what the different principal values can tell us. It is quite clear, that the first mode is not of much use for differentiation of the two classes. In general the second mode seems to work best (the ears), in contrast to the raw image we can also see that the wavelet basis is slightly better for the separation.\n\n\nShow the code for the figures\nimport plotly.express as px\n\n\nfig = px.scatter_3d(df, x='PV2', y='PV3', z='PV4', color=\"target\")\nfig.show()\nfig = px.scatter_3d(df_w, x=\"PV2\", y=\"PV3\", z=\"PV4\", color=\"target\")\nfig.show()\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 12: Scatter plots of modes 2 to 4 for the raw images.\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 13: Scatter plots of modes 2 to 4 for the wavelet images.\n\n\n\n\nNow that we have an idea what we are dealing with, we can start looking into the two previously described paths in machine learning. The difference between supervised and unsupervised learning can be summarized by the following two images.\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(df.iloc[:, 0][index], df.iloc[:, 2][index], label=name)\n\nplt.figure()\nplt.scatter(df.iloc[:, 0], df.iloc[:, 2], color=\"gray\")\n\n\n\n\n\n\n\n\n\n\nFigure 14: Labelled data for supervised learning.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Unlabelled data for unsupervised learning.\n\n\n\n\n\n\nYou either provide/have labels or not.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#footnotes",
    "href": "clustering/index.html#footnotes",
    "title": "Clustering and Classification",
    "section": "",
    "text": "To add it to pdm us pdm add scikit-learn.↩︎\nsee Kandolf (2025), Section 4.2 or follow the direct link↩︎\nsee Kandolf (2025), Section 10 or follow the direct link↩︎",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html",
    "href": "clustering/unsupervised.html",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "1.1 \\(k\\)-Means Clustering\nWe start with unsupervised learning. The goal of unsupervised learning is to discover clusters in the data of observations that have no labels, i.e. we have nothing to look at as reference. There are several algorithms to perform this task, the most prominent is the \\(k\\)-means clustering algorithm.\nThe \\(k\\)-means algorithm tries to partition a set of \\(m\\) (vector-valued) data observations into \\(k\\) clusters. Where in general, the dimension of the data and the amount of observations is known, the number of clusters is often not known a priori.\nThe general idea is, to label each observation as belonging to a cluster with the nearest mean (the representative of the cluster). The resulting clusters are called Voronoi cells, see Wikipedia - Voronoi diagram.\nWe illustrate the proceedings with the help of some artificial observations in two dimensional space, and show how the clustering takes place.\nIn Figure 1.1 (a) we see the two distinct clusters and the initial guesses for the centers. In the successive plots, we see how the centers move and converge to the final position, as seen in Figure 1.1 (d). In this case the algorithm converges after the sixth step.\nOf course the algorithm is sensitive to the initial guess and therefore modern versions provide strategies to determine the initial guess, as well as the number of clusters.\nWe can see this in action in the sklearn.cluster.KMeans version.\nAs can be seen in Figure 1.2 (a) the algorithm comes up with (almost) the same split between the two sets as our crude implementation. If we try for three clusters Figure 1.2 (b) the result is sensible as well.\nThe major success, of this algorithm in general use, is based on the fast convergence, that requires no supervision. We can also see that it is not very accurate, compare Figure 1.1 (a) and Figure 1.1 (d). This is no surprise, as the algorithm has not all information available.\nHow can we determine how accurate the algorithm is? If we have no labels this is of course not easy to do but cross-validation is a good tool.\nIn our case we can produce the labels and we can also split the data beforehand into a training set and a test set. Usually a so called \\(80:20\\) split is used, i.e. \\(80\\%\\) training data and \\(20\\%\\) test data.\nShow the code for the figure\nfrom sklearn.cluster import KMeans\nnp.random.seed(6020)\n\n# Shuffle data\nX_shuffle = X[np.random.permutation(X.shape[0]), :]\nX2_shuffle = X2[np.random.permutation(X2.shape[0]), :]\n\n# Split data into two parts\nsplit = n // 5 * 4\ndata = np.concatenate((X_shuffle[:split, :], X2_shuffle[:split, :]))\ntest = np.concatenate((X_shuffle[split:, :], X2_shuffle[split:, :]))\n\n# Create clustesr\nkmeans = KMeans(n_clusters=2, random_state=6020).fit(data)\nclasses = kmeans.predict(data)\ntest_classes = kmeans.predict(test)\n\n# Find wrong classifications\nerror = test_classes != np.concatenate((np.zeros(n-split), np.ones(n-split)))\n\n# Plotting\nplt.figure()\ncolour = [\"tab:orange\", \"tab:blue\"]\nfor i in range(2):\n    plt.scatter(data[classes==i, 0], data[classes==i, 1],\n                c=colour[i], alpha=0.5, label=\"train\")\n    plt.scatter(test[test_classes==i, 0], test[test_classes==i, 1],\n                c=colour[i], label=\"test\")\n\nplt.scatter(test[error, 0], test[error, 1], marker=\"x\", c=\"k\", label=\"error\")\nplt.gca().set_aspect(1)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.3: Validation against a test set.\nThe result of this can be seen in Figure 1.3 where we have two points wrongly classified to the opposite cluster.\nThere exist several extensions of the basic \\(k\\)-means algorithm to improve the results and overall performance. Two such versions are the accelerated \\(k\\)-means, as well as mini-batch \\(k\\)-means. Both can be found in Geron (2022).",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#sec-clustering-usl-kmeans",
    "href": "clustering/unsupervised.html#sec-clustering-usl-kmeans",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "Definition 1.1 (\\(k\\)-Means Algorithm) For a given set of \\(m\\) observations \\((x_1, x_2, \\ldots, x_m)\\), with \\(x_i\\in \\mathrm{R}^n\\) the algorithm strives to find \\(k\\) sets \\((S_1, S_2, \\ldots, S_k)\\) such that the variance inside the cluster is minimized, i.e. \\[\n\\underset{S}{\\operatorname{argmin}} \\sum_{i=1}^k \\sum_{x\\in S_i}\\| x - \\mu_i \\|^2,\n\\] where \\(\\mu_i\\) denotes the mean of \\(S_i\\).\nThe algorithm itself is recursive, for a given \\(k\\)\n\nRandomly initialize \\(k\\) points \\(\\mu_1, \\mu_2, \\ldots, \\mu_k\\), as the cluster centers,\nLabel each observation \\(x_i\\) by the nearest cluster center \\(\\mu_j\\), all points with the same label form the set \\(S_j\\).\nCompute the mean of each cluster \\(S_j\\) and replace the current \\(\\mu_j\\) by it.\nRepeat, starting from step 2, until the cluster centers stay stable up to some tolerance.\n\nThis algorithm was first introduced in Lloyd (1982) and is therefore often called Lloyd algorithm.\n\n\nShow the code for Lloyd algorithm\ndef lloyd(data, centers, steps):\n    classes = np.zeros(data.shape[0])\n    centers_ = np.copy(centers)\n    for i in range(steps):\n        for j in range(data.shape[0]):\n            classes[j] = np.argmin(np.linalg.norm(centers_ - np.tile(data[j, :],\n                                   (centers.shape[0], 1)), axis=1))\n        for j in range(centers.shape[0]):\n            centers_[j, :] = np.mean(data[classes == j, :], axis=0)\n        \n    return (classes, centers_)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn general Definition 1.1 is NP-hard and therefore computationally not viable. Nevertheless, a number of heuristic algorithms exist to provide a good performance, despite having no guarantee for global convergence.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following dataset and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.5 - 5.6). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\n# Helper for plotting\ndef plot_lloyd(data, centers, classes, ms=500):\n    plt.figure()\n    for i in range(centers.shape[0]):\n        plt.scatter(data[classes==i, 0], data[classes==i, 1])\n        plt.scatter(centers[i, 0], centers[i, 1], c=\"k\", s=ms, marker=\"*\")\n    plt.gca().set_aspect(1)\n\n# Create data for illustration\nn = 200\n# Random ellipse centred in (0, 0) and axis (1, 0.5)\nX = np.random.randn(n, 2) * np.array([1, 0.5])\n# Random ellipse centred in (1, -2) and axis (1, 0.2)\nX2 = (np.random.randn(n, 2) * np.array([1, 0.2])) + np.array([1, -2])\n# Rotate ellipse 2 by theta\ntheta = np.pi / 4\nX2 = X2 @ np.array([[np.cos(theta), -np.sin(theta)],\n                  [np.sin(theta), np.cos(theta)]] )\n\ncenters = np.array([[0., -1.], [-1., 0.]])\ndata = np.concatenate((X, X2))\n# Plot initial step with theoretical assignment\nclasses = np.concatenate((np.ones(X2.shape[0]), np.zeros(X.shape[0])))\nplot_lloyd(data, centers, classes)\n\n# Compute and plot consecutive steps of lloyds algorithm\nfor i in [1, 2, 5]:\n    classes, centers_ = lloyd(data, centers, i)\n    plot_lloyd(data, centers, classes)\n    centers = centers_\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The original two classes and the initial guesses for the centers.\n\n\n\n\n\n\n\n\n\n\n\n(b) Point association to the clusters in the first step.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Point association to the clusters in the third step.\n\n\n\n\n\n\n\n\n\n\n\n(d) Point association to the clusters in the eighth step.\n\n\n\n\n\n\n\nFigure 1.1: Lloyds algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\n\n\n\n\nShow the code for the figure\nfrom sklearn.cluster import KMeans\nnp.random.seed(6020)\n\nkmeans = KMeans(n_clusters=2, random_state=6020).fit(data)\nplot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)\nkmeans = KMeans(n_clusters=3, random_state=6020).fit(data)\nplot_lloyd(data, kmeans.cluster_centers_, kmeans.labels_)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Seeking two clusters.\n\n\n\n\n\n\n\n\n\n\n\n(b) Seeking three clusters.\n\n\n\n\n\n\n\nFigure 1.2: KMeans algorithm in action for a generic set of two datasets generated with a Gaussian distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.1 (Apply the \\(k\\)-means algorithm to other datasets) As an exercise, to get some practice for using \\(k\\)-means, apply the algorithm to some other datasets to see how it performs.\n\nWe already looked at the Fischer Iris dataset in Section 1.1 and discussed some basic features. Try with only two dimensional data, as well as four dimensional data to find the three clusters.\nFor Figure 1.7 we will look at the moons dataset. Try working out the clusters in this case.\nApply the algorithm to the cats and dogs (in wavelet basis, see Section 1.2) for various principal components and find the two clusters.\n\n\n\n\n\n\n1.1.1 Applications of \\(k\\)-means\nThe \\(k\\)-means algorithm is used in a multitude of applications, we list some here:\n\nImage segmentation: To decompose an image into different regions we can use \\(k\\)-means. Applications range from robotics to surveillance, where one or more objects are separated from the rest of the image.\nCustomer segmentation/social network analysis: To segment customers along e.g. their purchase history/behaviour, preferences, demographic data, amount spent, search queries, social media interactions, etc. \\(k\\)-means is used in marketing, retail, and advertising to personalize the experience.\nText clustering: In natural language processing (NLP) \\(k\\)-means is often used to cluster similar documents or text to make analysing large volumes of text data feasible.\nFraud detection: \\(k\\)-means is a crude tool for fraud detection in finance and banking. Transactions are clustered according to similarities and anomalies are detected. There exist more sophisticated methods in finance, but \\(k\\)-means is an easy to understand start.\nAnomaly detection: In medical (image) data \\(k\\)-means is often used to detect anomalies by finding points that fall outside of clusters. The same works for cybersecurity and e.g. network traffic.\nRecommendation systems: By grouping users together it is easier to recommend them new songs, items for shopping, and more.\nQuality control: By grouping similar products we can detect quality issues and defects in production processes. If an issue is found the part can be checked and the process adjusted.\nTraffic Analysis: In transport and logistics we can analyze traffic patterns and use the information for optimization and similar trips, routes, and vehicles.\n\nWe want to highlight image segmentation as an application of \\(k\\)-means. The example found here is first shown in Geron (2022).\nThe idea of image segmentation is to decompose an image into different segments. The following variants exist:\n\nColour segmentation - pixels with similar colour get assigned the same cluster. An application of this is image satellite postprocessing to find areas of forest or sea, or finding object in robotics applications, and organs in medical images.\nSemantic segmentation - all pixels that are part of the same object get assigned to the same segment. An application of this is in autonomous driving to find pedestrians - one segment containing all pedestrians.\nInstance segmentation - similar to semantic segmentation but individual objects are assigned to the same segment. In the autonomous driving application we would find individual pedestrians.\n\nHere we show how to perform colour segmentation with \\(k\\)-means.\n\n\n\n\n\n\nImportant\n\n\n\nThe following dataset and the basic structure of the code is from Geron (2022), see GitHub,\n\n\n\nShow the code for the figure\nimport numpy as np\nimport imageio.v3 as iio\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nim = np.asarray(iio.imread(\n        \"https://github.com/ageron/handson-ml3/blob/main/images/\"\n        \"unsupervised_learning/ladybug.png?raw=true\"))\nplt.figure()\nplt.imshow(im)\nplt.axis(\"off\")\n\nZ = im.reshape(-1, 3)\nfor colours in [8, 4, 2]:\n    kmeans = KMeans(n_clusters=colours, n_init=10, random_state=6020).fit(Z)\n    plt.figure()\n    plt.imshow(kmeans.cluster_centers_[kmeans.labels_].reshape(im.shape) / 255)\n    plt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Segmentation by 8 colours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Segmentation by 4 colours.\n\n\n\n\n\n\n\n\n\n\n\n(d) Segmentation by 2 colours.\n\n\n\n\n\n\n\nFigure 1.4: Colour segmentation for the image of a lady bug.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#unsupervised-hierarchical-clustering---dendrogram",
    "href": "clustering/unsupervised.html#unsupervised-hierarchical-clustering---dendrogram",
    "title": "1  Unsupervised learning",
    "section": "1.2 Unsupervised hierarchical clustering - Dendrogram",
    "text": "1.2 Unsupervised hierarchical clustering - Dendrogram\nSimilar to \\(k\\)-means, a simple hierarchical algorithm is used to create a dendrogram. The resulting tree allows us to easily see if data is clustered without the need of labeling or supervision.\nWe follow the example and discussion given in (Brunton and Kutz 2022, sec. 5.4).\nThere are two main approaches in creating the desired hierarchy: bottom-up often called agglomerative, top-down often called divisive.\n\nFor the agglomerative approach each observation \\(x_i\\) is initially its own cluster and in each step points are combined according to their distance. Eventually everything ends up in a single cluster and the algorithm stops.\nFor the divisive approach we go the opposite direction and start with the super cluster containing all observations \\(x_i\\) and we gradually split up into smaller and smaller clusters. The algorithm stops, when each observation is its own leave.\n\nOf course the norm1 used has quite an influence as can be seen in (Kandolf 2025, sec. 7.1) Link where we compared LASSO and RIDGE algorithms for our optimization problem.\nTo illustrate the agglomerative approach and the difference in norms we use four points and construct the hierarchy by combining the two closest points.\n\n\n\n\n\n\n\n\n\n(a) Use two norm or euclidean norm to compute the distance - \\(\\| \\cdot \\|_2\\)\n\n\n\n\n\n\n\n\n\n(b) Use one or cityblock norm to compute the distance - \\(\\| \\cdot \\|_1\\)\n\n\n\n\n\nFigure 1.5: We always combine the two nearest points and replace them with the point in the middle. This iteration continues until only one point is left. The Dendrogram is build according to the order of points chosen. It can also include the distance.\n\n\n\nOn a larger scale, with always the first 25 points of the two clusters above we get the results shown in Figure 1.6\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster import hierarchy\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nXX = np.concatenate((X[:25, :], X2[:25, :]))\nXX.shape\nfor metric in [\"euclidean\", \"cityblock\"]:\n    plt.figure()\n    Y = pdist(XX, metric=metric)\n    Z = hierarchy.linkage(Y, method=\"average\", metric=metric)\n    thresh = 0.90 * np.max(Z[:, 2])\n    dn = hierarchy.dendrogram(Z, p=100, color_threshold=thresh)\n\n    plt.figure()\n    plt.bar(range(XX.shape[0]), dn[\"leaves\"])\n    plt.plot(np.array([0, XX.shape[0]]),\n             np.array([XX.shape[0] // 2, XX.shape[0] // 2]),\n             \"r:\", linewidth=2)\n    plt.plot(np.array([(XX.shape[0] // 2) + 1/2, (XX.shape[0] // 2) + 1/2]),\n             np.array([0, XX.shape[0]]),\n             'r:', linewidth=2)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Dendrogram for euclidean norm.\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram showing the clustering for the euclidean norm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Dendrogram for cityblock norm.\n\n\n\n\n\n\n\n\n\n\n\n(d) Histogram showing the clustering for the cityblock norm.\n\n\n\n\n\n\n\nFigure 1.6: Construction a agglomerative hierarchy for our dataset.\n\n\n\nThe two dendrograms (Figure 1.6 (a) and Figure 1.6 (c)) show the hierarchical structure derived from the dataset. The number of clusters can be influenced by the thresh parameter and it is also used to label the observation accordingly. It is quite similar to the number of clusters \\(k\\) in the \\(k\\)-means algorithm.\nThe two bar graphs on the right (Figure 1.6 (b) and Figure 1.6 (d)) show how the observations is clustered in the dendrogram. The bars correspond to the distance metric produced by the algorithm. The red lines indicate the region of a perfect split for the separation if the algorithm works perfectly and places every point in the original cluster. If we recall, that the first 25 points are from set 1 and the next 25 from set 2 we can see that the euclidean distance and the cityblock norm place one point in the wrong cluster.\n\n\n\n\n\n\nNote\n\n\n\nThe function scipy.cluste.hierarchy.linkage allows to specify the method of computing the distance between two clusters. The used average corresponds to unweighted pair group method with arithmetic mean UPGMA algorithm.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn scikit-learn this is supported via the sklearn.cluster.AgglomerativeClustering class.\n\n\n\n\n\n\n\n\n\nExercise 1.2 (Apply the agglomerative clustering to the Fischer Iris dataset)  \n\nWork out how to use the scikit-learn alternative.\nWe already looked at the Fischer Iris dataset in Section 1.1 and discussed some basic features. Try with only two dimensional data, as well as four dimensional data, to find the three clusters.\nFor Figure 1.7 we will look at the moons dataset. Try working out the clusters in this case, look at the different possibilities with the linkage option as {\"ward\", \"complete\", \"average\", \"single\"}.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#sec-clustering-ul-dbscan",
    "href": "clustering/unsupervised.html#sec-clustering-ul-dbscan",
    "title": "1  Unsupervised learning",
    "section": "1.3 Density-based spatial clustering of applications with noise (DBSCAN)",
    "text": "1.3 Density-based spatial clustering of applications with noise (DBSCAN)\nThe algorithm was introduced in Ester et al. (1996) and is an algorithm that finds areas of high density. The main idea behind the algorithm is as follows.\nOur observations (we will call them points here for consistency) form the basis and they can be in any space but as for all the algorithms presented here, there needs to exist some form to measure distance. Furthermore, DBSCAN relies on two parameters, \\(\\epsilon\\) describing the radius of neighbourhood of a point and \\(minPts\\) the minimum of points needed to form a cluster. With these parameters we perform the following steps:\n\nA point \\(p\\) is called a core point if at least \\(minPts\\) (including \\(p\\)) are within distance \\(\\epsilon\\) of \\(p\\). This region is called the \\(\\epsilon\\)-neighbourhood of \\(p\\).\nA point \\(q\\) is called directly reachable from a core point \\(p\\) iff2 \\(q\\) is in the \\(\\epsilon\\)-neighbourhood of \\(p\\).\nA point \\(p\\) is reachable from \\(p\\), if there exists a sequence of points (path) \\(p=p_1, p_2,  \\ldots, p_n=q\\) where each \\(p_{i+1}\\) is directly reachable form \\(p_i\\). Consequently, all point on the path, except \\(p\\), need to be core points.\nA point that is not reachable from any other point is considered an outlier or noise point.\n\nTherefore, we get our definition of a cluster: a core point \\(p\\) together with all points that are reachable from \\(p\\). This includes core points (at least one) and none-core points (boundary points) on the boundary as they can not be used to reach more points.\nIn contrast to \\(k\\)-means the algorithm we can find non-linear (i.e. curved) clusters. We illustrate this with the moon dataset.\n\n\n\n\n\n\nImportant\n\n\n\nFor the plotting function please see GitHub, the accompanying repository to Geron (2022), as it forms the basis.\n\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef plot_dbscan(dbscan, X, size):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    # Plotting the clusters\n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask],\n                marker=\"o\", s=size, cmap=\"Paired\")\n    # Plotting the core points of the clusters\n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask],\n                marker=\"*\", s=20,)\n    # Plotting the anomalies\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    # Plotting the boundary points\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n    plt.gca().set_aspect(1)\n\nnp.random.seed(6020)\nM, M_y = make_moons(n_samples=1000, noise=0.05, random_state=6020)\n\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(M)\nplt.figure()\nplot_dbscan(dbscan, M, size=100)\n\ndbscan = DBSCAN(eps=0.2, min_samples=5)\ndbscan.fit(M)\nplt.figure()\nplot_dbscan(dbscan, M, size=600)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scikit-Learn’s DBSCAN with eps=0.05 and min_samples=5.\n\n\n\n\n\n\n\n\n\n\n\n(b) Scikit-Learn’s DBSCAN with eps=0.2 and min_samples=5.\n\n\n\n\n\n\n\nFigure 1.7: DBSCAN illustrated with the moons dataset.\n\n\n\nThe algorithm works best for clusters well separated by low density regions. As all algorithms depending on distance measures, it suffers from the curse of dimensionality3.\nAs can be seen in Figure 1.7 (a) the algorithm produces quite a lot of clusters for a small \\(\\epsilon=0.05\\). In total we get 10 clusters and quite a lot of outliers that are not all easy to retract by the naked eye.\nNevertheless, if we increase \\(\\epsilon=0.2\\) we can see that the two clusters are neatly reproduced, Figure 1.7 (b).\n\n\n\n\n\n\nNote\n\n\n\nThe DBSCAN class in Scikit-Learn does not provide a .predict() method like many other such classes. See Geron (2022) and GitHub for how to train a \\(k\\)-nearest neighbour (kNN) to perform this task.\n\n\n\n\n\n\n\n\n\nExercise 1.3 (Application of DBSCAN)  \n\nApply the DBSCAN algorithm to our toy example (the two ellipsoids from Section 1.1) used through this section, to recover the two clusters as good as possible. Try different \\(\\epsilon\\) values as well as different norms (metric parameter).\nAdditionally, for a higher dimensional problem, using DBSCAN to split the Fischer Iris dataset (Section 1.1) into its three clusters of flowers.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#finite-mixtures-models",
    "href": "clustering/unsupervised.html#finite-mixtures-models",
    "title": "1  Unsupervised learning",
    "section": "1.4 Finite Mixtures Models",
    "text": "1.4 Finite Mixtures Models\nFinite mixture models assume that the observations \\(x_i\\) are mixtures of \\(k\\) processes, which combine in the measurement. Each mixture is defined via a probability model with unknown parameters. The aim of the algorithm is to find the parameters such that the \\(k\\) processes best describe the \\(x_i\\) observations. The basis of the algorithm is the so called expectation-maximization (EM) algorithm of Dempster, Laird, and Rubin (1977), that is designed to find the maximum-likelihood parameters of the defined statistical models.\nThe most common choice for the probability models is the Gaussian distribution4 and for this choice, the method is better known as Gaussian mixture models (GMM). This is often quite a sensible choice, if nothing more is known about the observations, and a Gaussian distribution is therefore a save bet. As a result each process can be described by two parameters, mean and variance. Each cluster in turn, will be described by an ellipsoidal shape where size, density, orientation, and semi-axis vary.\nLike \\(k\\)-means, in the simplest version, GMM is provided with the number of clusters \\(k\\) and starts from an initial guess of the means and corresponding variances. The parameters are than iteratively updated to find a (local) maximum. There are some problems with this approach, e.g. if we set one of the processes to have zero variance and mean equal to a point. To avoid such problems cross-validation can often help to avoid problems stemming from an initialization with a poor initial guess.\nAs mentioned, the main idea of the mixture models is that the observations are a linear combination of probability density functions (PDF) \\[\nf(x_j, \\Theta) = \\sum_{p=1}^k \\alpha_p f_p(x_j, \\Theta_p),\n\\] where \\(f\\) denotes the observed/measured PDF with parameters \\(\\Theta\\), \\(f_p\\) the PDF of mixture \\(p\\) with parameters \\(\\Theta_p\\), and \\(k\\) denotes the number of mixtures. The weights \\(\\alpha_p\\) fulfil \\(\\sum_p \\alpha_p = 1\\).\nOverall we therefore can formulate the algorithm as:\n\n\n\n\n\n\n\nDefinition 1.2 (Mixture Models Algorithm) Given the observed PDF \\(f(x_j, \\Theta)\\), estimate the mixture weights \\(\\alpha_p\\) and the parameters fo the distribution \\(\\Theta_p\\).\n(see Brunton and Kutz 2022)\n\n\n\n\nLets follow the derivative for the GMM method in (Brunton and Kutz 2022, sec. 5.5) to get a better idea what is happening.\nFor GMM, \\(f\\) reads as \\[\nf(x_j, \\Theta) = \\sum_{p=1}^k \\alpha_p \\mathcal{N}_p(x_j, \\mu_p, \\sigma_p),\n\\] and for a given \\(k\\), we need to find \\(\\alpha_p, \\mu_p, \\sigma_p\\). We get \\(\\Theta\\) from the roots of \\[\nL(\\Theta) = \\sum_{j=1}^n\\log f(x_j | \\Theta).\n\\] \\(L\\) is called the log-likelihood function and we sum over all observations \\(x_j\\). We transform it into an optimization problem by setting the derivative to zero \\[\n\\frac{\\partial L(\\Theta)}{\\partial \\Theta} = 0,\n\\] and solve it via the EM algorithm. As the name suggests, there are two steps E and M.\nThe E step uses the following posterior to establish memberships. Therefore, we start of by assuming an initial guess of the vector \\(\\Theta\\) and this leads to the _posterior probability of distribution \\(p\\) of \\(x_j\\) \\[\n\\tau_p(x_j, \\Theta) = \\frac{\\alpha_p f_p(x_j, \\Theta_p)}{f(x_j, \\Theta)}.\n\\] In other words, we try to figure out if \\(x_j\\) is part of the \\(p\\)th mixture. For GMM this becomes, \\[\n\\tau_p^{(l)}(x_j) = \\frac{\\alpha_p^{(l)} \\mathcal{N}_p(x_j, \\mu_p^{(l)}, \\sigma_p^{(l)})}{f(x_j, \\Theta^{(l)})},\n\\] in the \\(l\\) iteration.\nNow the M step starts to update the parameters and mixture weights as \\[\n\\begin{aligned}\n\\alpha_p^{(l+1)} &= \\frac1n \\sum_{j=1}^n \\tau_p^{(l)}(x_j), \\\\\n\\mu_p^{(l+1)} &= \\frac{\\sum_{j=1}^n x_j\\tau_p^{(l)}(x_j)}{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)}, \\\\\n\\Sigma_p^{(l+1)} &= \\frac{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)[(x_j - \\mu_p^{(l+1)})(x_j - \\mu_p^{(l+1)})^\\mathrm{T}]}{\\sum_{j=1}^n \\tau_p^{(l)}(x_j)},\n\\end{aligned}\n\\] with \\(\\Sigma_p^{(l+1)}\\) denotes the covariance matrix. The algorithm now alternates between E and M until convergence is reached.\nLet us try it with the cats and dogs images in wavelet basis from Section 1.2 by fitting to the second and forth principal value of the SVD. Note, we directly load the images in wavelet basis and do not recompute them.\n\n\n\n\n\n\nImportant\n\n\n\nThe following dataset and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.8). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.mixture import GaussianMixture\nfrom scipy.stats import multivariate_normal\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\nCD = np.concatenate((dogs, cats), axis=1)\nU,S,VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\nv = VT.T\n\ndogcat = v[:,(1,3)]\nGMModel = GaussianMixture(n_components=2, n_init=10).fit(dogcat)\n\nplt.figure()\nplt.scatter(v[:80,1], v[:80,3], label=\"dogs\")\nplt.scatter(v[80:,1], v[80:,3], label=\"cats\")\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\n\nx = np.linspace(-0.15, 0.25)\ny = np.linspace(-0.25, 0.2)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = np.exp(GMModel.score_samples(XX))\nZ = Z.reshape(X.shape)\n\nCS = plt.contour(X, Y, Z,\n                levels=np.arange(3,32,4), colors='k', linestyles='solid')\nplt.legend()\nplt.gca().set_aspect(1)\n\nplt.figure()\nax = plt.axes(projection='3d')\nfor i in range(GMModel.weights_.shape[0]):\n    rv = multivariate_normal(GMModel.means_[i], GMModel.covariances_[i])\n    z = GMModel.weights_[i] * rv.pdf(np.dstack((X, Y)))\n    ax.plot_surface(X, Y, z, alpha=0.5)\n\nax.plot_wireframe(X, Y, Z, color='black', rstride=5, cstride=5)\nax.view_init(30, -80)\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\nplt.legend([r\"$\\mathcal{N}(\\mu_1, \\sigma_1)$\",\n            r\"$\\mathcal{N}(\\mu_2, \\sigma_2)$\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) We can see a nice split along the tow animals via the Gaussian distributions.\n\n\n\n\n\n\n\n\n\n\n\n(b) The PDFs of the fitted distributions, weighted via the GMM algorithm.\n\n\n\n\n\n\n\nFigure 1.8: Fitting of the second and fourth principal value of the cats and dogs images in wavelet basis via the GMM method.\n\n\n\nAs can be seen in Figure 1.8 (a) the two clusters are quite well captured by the GMM algorithm. The contour shows the linear combination of the two Gaussians. We see the two Gaussians on the right in Figure 1.8 (b) where we use the same colours as for the cats and dogs respectively. Note that the distributions are weight \\(\\alpha_p\\) from GMM but other than that there is no unit in the z direction.\nAs discussed above, the algorithm is prone to problems during the initialization, therefore we use n_init = 10 to initialize the algorithm 10 times and only keep the best result.\nWe can draw new samples from the resulting object .sample() and we can also fit data to it and we can also perform hard and soft clustering. For hard clustering the model assigns each instance to the most likely cluster (.predict()), where for soft clustering (.predict_proba()) we get the probability of membership for each cluster.\nIf the algorithm struggles to recover the clusters, we can help it by imposing the shape of the cluster via restricting the covariance matrix \\(\\Sigma_p\\). This can be done via the parameter covariance_type, see docs.\n\n\n\n\n\n\n\nExercise 1.4 (Application of GMM)  \n\nApply the GMM algorithm to our toy example (the two ellipsoids from Section 1.1) used through this section to recover the two clusters as good as possible. This should be straight forward, as we constructed it via Gaussian distributions. Try constraining the algorithm by imposing the different values possible for covariance_type.\nAdditionally, for a higher dimensional problem, split the Fischer Iris dataset into its three clusters of flowers.\nTry to use GMM to get clusters for the moon dataset used in Section 1.3. Also try with the BayesianGaussianMixture class, where no amount of clusters needs to be specified.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGMM can also be used for anomaly detection. Simply assume every observation located in a low density region an anomaly. For this the threshold needs to be defined in accordance to our expected anomalies.\nE.g. to get about \\(4\\%\\) anomalies we can use the following snippet together with the above code:\n\n\nShow the code for the figure\nS = GMModel.score_samples(dogcat)\ndensity_threshold = np.percentile(S, 2)\nanomalies = dogcat[S &lt; density_threshold]\nplt.figure()\nplt.scatter(dogcat[:80,0], dogcat[:80,1], alpha=0.5, label=\"dogs\")\nplt.scatter(dogcat[80:,0], dogcat[80:,1], alpha=0.5, label=\"cats\")\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_4$\")\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color=\"k\", marker=\"x\",\n            label=\"anomaly\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.9: Fitting of the second and fourth principal value of the cats and dogs images in wavelet basis via the GMM method.\n\n\n\n\n\nSee Geron (2022) and the corresponding GitHub repository.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#sec-clustering-ul-me",
    "href": "clustering/unsupervised.html#sec-clustering-ul-me",
    "title": "1  Unsupervised learning",
    "section": "1.5 Motivational example",
    "text": "1.5 Motivational example\nBefore we make the leap to supervised learning, we we introduce a new dataset and see how our unsupervised approach works in this case.\nWe do this with the (in)famous MNIST dataset of hand written digits. Lets start by looking at the dataset and establishing some basic properties.\nWe load the dataset and look at its description\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)\nprint(mnist.DESCR)\n\n**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n**Please cite**:  \n\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n\nDownloaded from openml.org.\n\n\nThe images are stored in .data and the label in .target. We can look at the first 100 digits\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nmnist = fetch_openml('mnist_784', as_frame=False)\n\nim = np.zeros((10 * 28, 10 * 28), int)\nfor i in range(10):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = mnist.data[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.10: First 100 digits from the MNIST dataset.\n\n\n\n\n\nAs can be seen from Figure 1.10, most digits are easy to distinguish for us, but there are some nasty things included. Especially, there are 1s with the english style (just one straight line) of writing (1st row, 7th column), but also as a diagonal line (1st row, 4th column), and finally also the german style (connecting line at base and top) of writing (3rd row, 5th column).\nAlso the 2 digit comes in various forms, with loop (1st row, 6th column) and without (3rd row, 6th column). Finally we all know that a 3 can hide in an 8, same as a 7 and 1 can hide in a 9 and much more.\nNow, before we go any further in the investigation we split up the dataset into a training and a test set, we will not use this here but we should establish some way of working right from the start.\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\nNow let us try with \\(k\\)-means to find the ten digits by finding \\(10\\) clusters.\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nk = 10\nkmeans = KMeans(n_clusters=k, n_init=1, random_state=6020).fit(X_train)\n\nim = np.zeros((1 * 28, 10 * 28), int)\nfor i in range(1):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = \\\n                kmeans.cluster_centers_[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.11: The cluster means of \\(k\\)-means for 10 clusters.\n\n\n\n\n\nAs can be seen from Figure 1.11, the cluster centers do not recover our ten digits. In general, they are, as expected, smudged as they do not represent an actual image. It looks like \\(0, 1, 2, 3, 6, 8, 9\\) are easy to discover but \\(4, 5, 7\\) not. If we look closely, we can see \\(4, 5, 7\\) represented in our clusters but not as separate digits. Obviously this is not a good way to proceed and to find our digits. We will discuss methods to perform this task in the next section Chapter 2. Furthermore, keep this part in mind, as we will come back to how this can still be helpful in a different aspect as semi-supervised learning Chapter 3.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B (Methodological) 39 (1): 1–22.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. “A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.” In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 226–31. KDD’96. Portland, Oregon: AAAI Press.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.\n\n\nLloyd, Stuart P. 1982. “Least Squares Quantization in PCM.” IEEE Trans. Inf. Theory 28 (2): 129–36. https://doi.org/10.1109/TIT.1982.1056489.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/unsupervised.html#footnotes",
    "href": "clustering/unsupervised.html#footnotes",
    "title": "1  Unsupervised learning",
    "section": "",
    "text": "see Kandolf (2025), Section 1.2 or follow the direct link↩︎\nshort for if and only if↩︎\nProblems occurring for higher dimensional data and distance measures, see WikiPedia↩︎\nsee Kandolf (2025), Example 14.6 or for a direct Link↩︎",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unsupervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html",
    "href": "clustering/supervised.html",
    "title": "2  Supervised learning",
    "section": "",
    "text": "2.1 Linear Discriminant Analysis (LDA)\nIf we recall Fisher (1936) from the Iris dataset, we can also find one of the first supervised learning methods in this paper. The introduced linear discriminant analysis (LDA) has survived over time and is still one of the standard techniques for classification, even though we use a more generalized and improved method nowadays.\nThe idea of LDA is to find a linear combination of features that optimally separates two or more classes. The crucial part in the algorithm is that it is guided by labelled observations. At its core, the algorithm aims to solve an optimization problem: find an optimal low-dimensional embedding of the data that shows a clear separation between their point distribution, or maximize the distance between the inter-class data and minimize the intra-class data distance.\nThe main idea of LDA is to use projection. For a two-class LDA this becomes \\[\nw = \\operatorname{arg\\, max}_w \\frac{w^\\mathrm{T}S_B w}{w^\\mathrm{T}S_W w},\n\\tag{2.1}\\] (the generalized Rayleigh quotient) where \\(w\\) is our thought after projection. The two included scatter matrices are \\[\nS_B = (\\mu_2 - \\mu_1)(\\mu_2 - \\mu_1)^\\mathrm{T},\n\\] for between-class relation as well as \\[\nS_W = \\sum_{j=1}^2 \\sum_{x\\in\\mathcal{D}_j} (x - \\mu_j)(x - \\mu_j)^\\mathrm{T},\n\\] for within-class data. The set \\(\\mathcal{D}_j\\) denotes the subdomain of the data associated with cluster \\(j\\). The two matrices measure the variance of the dataset as well as the means. To solve Equation 2.1 we need to solve the generalized eigenvalue problem1 \\[\nS_B w = \\lambda S_w w\n\\] where the maximal eigenvalue and the corresponding eigenvector are our solution.\nWe try this with the cats and dogs dataset in both basis.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef analyse(CD):\n    U, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\n    v = VT.T\n    xtrain = np.concatenate((v[:60, [1, 3]], v[80:140, [1, 3]]))\n    label = np.repeat(np.array([1, -1]), 60)\n    test = np.concatenate((v[60:80, [1, 3]], v[140:160, [1, 3]]))\n    lda = LinearDiscriminantAnalysis()\n    lda.fit(xtrain, label)\n    test_class = lda.predict(test)\n    truth = np.repeat(np.array([1, -1]), 20)\n    E = 100 * (1 - np.sum(np.abs(test_class - truth) / 2) / 40)\n    plt.figure()\n    plt.bar(range(40), test_class)\n    plt.plot([-0.5, 39.5], [0, 0], \"k\", linewidth=1.0)\n    plt.plot([19.5, 19.5], [-1.1, 1.1], \"r-.\", linewidth=3)\n    plt.yticks([-0.5, 0.5], [\"cats\", \"dogs\"], rotation=90, va=\"center\")\n    plt.text(10, 1.05, \"dogs\")\n    plt.text(30, 1.05, \"cats\")\n    plt.gca().set_aspect(40 / (2 * 3))\n    return (test_class, E, v)\n\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog\"]\n\ntest_class, E, v = analyse(np.concatenate((dogs, cats), axis=1))\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\ntest_class, E_w, v_w = analyse(np.concatenate((dogs_w, cats_w), axis=1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Trained and evaluated against the raw data.\n\n\n\n\n\n\n\n\n\n\n\n(b) Trained and evaluated against the data in wavelet basis.\n\n\n\n\n\n\nFigure 2.2: Evaluation of the LDA for the second and fourth principal component on the test set of 40 animals. A bar going up corresponds to dogs and one going down to cats. The first 20 individuals should be dogs, the second 20 cats. The red dotted line shows the split. True positive can be found in the top-left as well as the bottom-right.\nIf we use our raw dataset for the classification we get an overall accuracy of 67.5% with \\(\\tfrac{4}{20}\\) wrongly labelled dogs and \\(\\tfrac{9}{20}\\) wrongly labelled cats. We can increase this to an accuracy of 82.5% with \\(\\tfrac{5}{20}\\) wrongly labelled dogs and \\(\\tfrac{2}{20}\\) wrongly labelled cats.\nThis could be expected, see Figure 10 for the separation of the principal values for the two basis.\nOf course we have very limited data with only 80 images for each of the classes. In this case we should do a cross-validation and we have not shuffled the data.\nLet us see how selecting different test and training sets influence the behaviour.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nE = np.zeros(100)\nfor j in range(100):\n    r1 = np.random.permutation(80)\n    r2 = np.random.permutation(80) + 60\n    ind1 = r1[:60]\n    ind2 = r2[:60]\n    ind1t = r1[60:80]\n    ind2t = r2[60:80]\n    \n    xtrain = np.concatenate((v_w[ind1, :][:, [1, 3]], v_w[ind2, :][:, [1, 3]]))\n    test = np.concatenate((v_w[ind1t, :][:, [1, 3]], v_w[ind2t, :][:, [1, 3]]))\n    label = np.repeat(np.array([1, -1]), 60)\n\n    lda = LinearDiscriminantAnalysis()\n    test_class = lda.fit(xtrain, label).predict(test)\n\n    truth = np.repeat(np.array([1, -1]),20)\n    E[j] = 100 * (1 - np.sum(np.abs(test_class - truth) / 2) / 40)\n\nplt.figure()\nplt.bar(range(100), E)\nplt.plot([0, 100], [E.mean(), E.mean()], \"r-.\", label=\"mean\")\nplt.plot([0, 100], [50, 50], \"y-.\", label='\"coin toss\"')\nplt.xlim((-1, 100))\nplt.ylim((45, 90))\nplt.gca().set_aspect(100 / (45 * 3))\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"trial number\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: Cross validation for the dataset in wavelet basis, use 100 run with different training and test sets. We always use 120 images for training and 40 for testing.\nWith a maximal accuracy of 90.0% and a minimal accuracy of 65.0% our initial result with 82.5% was quite good and above average (77.1%). We can also see that training the model is always better than just a simple coin toss or random guessing for cat or dog.\nInstead of a linear discriminants, we can also use quadratic discriminants. To show the difference let us look at the classification line of the two methods for our data in wavelet basis\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nxtrain = np.concatenate((v_w[:60, [1, 3]], v_w[80:140, [1, 3]]))\nlabel = np.repeat(np.array([1, -1]), 60)\ntest = np.concatenate((v_w[60:80, [1, 3]], v_w[140:160, [1, 3]]))\n\nplt.figure()\nplt.scatter(v_w[:80, 1], v_w[:80, 3], label=\"dogs\")\nplt.scatter(v_w[80:, 1], v_w[80:, 3], label=\"cats\")\n\nlda = LinearDiscriminantAnalysis().fit(xtrain, label)\nK = -lda.intercept_[0]\nL = -lda.coef_[0]\nx = np.arange(-0.12, 0.25, 0.005)\nplt.plot(x, -(L[0] * x + K) / L[1], \"k\", label=\"classification line\")\nplt.ylim([-0.25, 0.2])\nplt.xlim([-0.15, 0.25])\nplt.xlabel(r\"$PC_2$\")\nplt.ylabel(r\"$PC_4$\")\nplt.legend()\n\nplt.figure()\nplt.scatter(v_w[:80, 1], v_w[:80, 3], label=\"dogs\")\nplt.scatter(v_w[80:, 1], v_w[80:, 3], label=\"cats\")\n\nqda = QuadraticDiscriminantAnalysis().fit(xtrain, label)\nDecisionBoundaryDisplay.from_estimator(\n        qda,\n        xtrain,\n        grid_resolution=2000,\n        ax=plt.gca(),\n        response_method=\"predict\",\n        plot_method=\"contour\",\n        alpha=1.0,\n        levels=[0],\n    )\nplt.ylim([-0.25, 0.2])\nplt.xlim([-0.15, 0.25])\nplt.xlabel(r\"$PC_2$\")\nplt.ylabel(r\"$PC_4$\")\nplt.legend([\"dogs\", \"cats\", \"classification line\"])\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: Classification line for the LDA together with actual instances.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Classification line for the QDA together with actual instances.\nAs we can see in Figure 2.5, having a quadratic discriminant classification line can be rather beneficial, like always depending on the observations. The QDA arises from LDA when we do not assume that the covariance of each of the classes is the same.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#sec-clustering-sl-lda",
    "href": "clustering/supervised.html#sec-clustering-sl-lda",
    "title": "2  Supervised learning",
    "section": "",
    "text": "Important\n\n\n\nThe following introduction and illustrational dataset, as well as the basic structure of the code is from (Brunton and Kutz 2022, Code 5.9). Also see GitHub.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor supervised learning, it is always good practice to split up our dataset into a training and testing section. It is important to have a test set that the algorithm has never seen! In general a \\(80:20\\) split is common but other ratios might be advisable, depending on the dataset.\nIt is also common practice to use \\(k\\)-folds cross-validation for the training set.\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (\\(k\\)-folds cross-validation) The \\(k\\)-folds cross-validation technique is a method to allow better hyperparameter tuning, especially for smaller datasets where training and validation data is small. The main idea is that we split our iterate over different validation sets by splitting up our training set. Lets say we use \\(5\\)-fold cross-validation we split the training set into 5 parts. Therefore, we train the model with four parts and validate against the fifth. We then rotate the folds and select a different one for validation. At the end, we average over the five iterations to get our final parameters.\nThis looks something like this:\n\n\n\n\n\n\nFigure 2.1: Common split with the folds 1 to 4 for training and 5 for validation in the first iteration and folds 2 to 5 for training and 1 for validation in the last. The test set is not touched.\n\n\n\nIt is important that the test set is not included in the folds to make sure we test against observations that the algorithm has never seen!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLDA and QDA assume a normal distribution as the basis for each of the clusters. This allows us to write it also as an update procedure with Bayes2 theorem.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhere for the LDA it is possible to get the correct function for the classification line this is tricky for the QDA. Luckily the scikit-learn class/function DecisionBoundaryDisplay.from_estimator can help in such cases.\n\n\n\n\n\n\n\n\n\nExercise 2.1 (Application of LDA)  \n\nApply the LDA algorithm to the toy example (see Figure 1.1) to recover the two clusters as good as possible.\nAdditionally, for a higher dimensional problem, using LDA split the Fischer Iris dataset (Section 1.1) into two clusters. Try for the harder split between versicolor and virginica types of flowers.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#sec-clustering-sl-performance",
    "href": "clustering/supervised.html#sec-clustering-sl-performance",
    "title": "2  Supervised learning",
    "section": "2.2 Measuring Performance",
    "text": "2.2 Measuring Performance\nAs in most applications, the question how good an algorithm performs is not easy to establish. In Figure 2.3, we implied we are doing better than a coin toss but we should be able to characterize this more precise.\n\n\n\n\n\n\nImportant\n\n\n\nThe following approach and the basic structure of the code is from Geron (2022), see GitHub.\n\n\nIn order to illustrate basic properties found in machine learning we use the MNIST dataset together with a binary classifier based on Stochastic Gradient Descent, see Section 1.5 for more on the MNIST dataset.\n\n\n\n\n\n\nNote\n\n\n\nStochastic Gradient Descent (SGD)3 is an optimization algorithm. The key idea is to replace the actual gradient by a stochastic approximation in the optimization of the loss function. This allows especially good performance for large-scale learning and sparse machine learning problems.\nWe can use this training method for classification to find the optimal parameters for our loss function and in turn, this can be used as a binary classifier.\nAs SGD methods are prone to a sensitivity in feature scaling and order we need to make sure to use normalized data and we should shuffle.\nIn scikit-learn we can use the class SGDClassifier.\n\n\nFirst we load the MNIST dataset again and split it into a training and testing section, see Section 1.5.\n\n\nShow the code for loading and splitting the dataset\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import SGDClassifier\nnp.random.seed(6020)\n\nmnist = fetch_openml('mnist_784', as_frame=False)\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\n\nNext, as we only want a binary classifier, we select one number, in our case 5 and relabel our data. With the new labels we can train our classifier, compare Definition 2.9.\n\ny_train_5 = (y_train == \"5\")\ny_test_5 = (y_test == \"5\")\n\nSGD = SGDClassifier(random_state=6020)\nSGD.fit(X_train, y_train_5)\n\nIn order to get a score for our method, we use \\(k\\)-folds cross-validation Definition 2.1 and the corresponding scikit-learn function cross_val_score to perform this task for our model.\n\nscores = sklearn.model_selection.cross_val_score(\n    SGD, X_train, y_train_5, cv=5, scoring=\"accuracy\")\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\naccuracy [%]\n97.07\n96.23\n95.46\n96.62\n96.37\n\n\n\n\n\n\n\nWith scores in the high \\(90\\%\\) range the results look promising, if not great, but are they really that good? In order to get a better idea just always guess that we do not see a 5 that should be the most common class in our case. To simulate this we use the DummyClassifier class.\n\ndummy = sklearn.dummy.DummyClassifier()\ndummy.fit(X_train, y_train_5)\nscores_dummy = sklearn.model_selection.cross_val_score(\n    dummy, X_train, y_train_5, cv=5, scoring=\"accuracy\")\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\naccuracy [%]\n90.96\n90.97\n90.97\n90.97\n90.97\n\n\n\n\n\n\n\nAs this is pretty much \\(91\\%\\) (as expected there are only about \\(10\\%\\) of 5s in the dataset). Just using accuracy is apparently not the gold standard to measure performance, what other possibilities are there?\n\n\n\n\n\n\n\nDefinition 2.2 (Confusion Matrix) The confusion matrix, error matrix or for unsupervised learning sometimes called matching matrix allows an easy way of visualizing the performance of an algorithm.\nThe rows represent the true observations in each class, and the columns the predicted observations for each class.\nIn our \\(2\\times 2\\) case we get\n\n\n\n\n\n\nFigure 2.6: Names and abbreviations for a \\(2\\times 2\\) confusion matrix together with an example form our test case.\n\n\n\nbut it can be extended for multi-class segmentation.\n\n\n\n\nTo compute the confusion matrix we first need predictions. This can be achieved by cross_val_predict instead of cross_val_score and than we use sklearn.metrics.confusion_matrix.\nCombined, for our example, this reads as:\n\ny_train_pred = sklearn.model_selection.cross_val_predict(\n                    SGD, X_train, y_train_5, cv=3)\ncm = sklearn.metrics.confusion_matrix(y_train_5, y_train_pred)                   \n\n\n\n\n\n\n\n\n\n\nPP\nPN\n\n\n\n\nP\n53617\n962\n\n\nN\n1236\n4185\n\n\n\n\n\n\n\nFrom the values in the confusion matrix a lot of metrics can be computed4:\n\nAccuracy: \\[\nACC = \\frac{TP + TN}{P + N}\n\\]\nTrue positive rate (TPR) or recall: \\[\nTPR = \\frac{TP}{P}\n\\]\nFalse negative rate (FNR): \\[\nFNR = \\frac{FN}{P}\n\\]\nFalse positive rate (FPR): \\[\nFPR = \\frac{FP}{N}\n\\]\nTrue negative rate (TNR): \\[\nTNR = \\frac{TN}{N}\n\\]\nPositive predictive value (PPV) or precission: \\[\nPPV = \\frac{TP}{TP + FP}\n\\]\nFalse discovery rate (FDR): \\[\nFDR = \\frac{FP}{TP + FP}\n\\]\nFalse omission rate (FOR): \\[\nFOR = \\frac{FN}{TN + FN}\n\\]\nNegative predictive value (NPV): \\[\nNPV = \\frac{TN}{TN + FN}\n\\]\n\\(F_1\\) score: \\[\nF_1 = \\frac{2 TP}{2 TP + FP + FN}\n\\]\n\nIn the sklearn.metrics most of these values have a corresponding function. If we look at precission, recall, and the \\(F_1\\) score, for our example we see that our performance is viewed under a different light:\n\nprecision = sklearn.metrics.precision_score(y_train_5, y_train_pred)\nrecall = sklearn.metrics.recall_score(y_train_5, y_train_pred)\nf1_score = sklearn.metrics.f1_score(y_train_5, y_train_pred)\n\nThis tells us that our classifier correctly classifies a 5 81.31% of the time. On the other hand it only recalls or detects 77.2% of our 5s. The \\(F_1\\)-score is a combination of the two (the harmonic mean) and in our case 79.2%.\nDepending on the application we might want to have high precision (e.g. medical diagnosis to have no unnecessary treatment) or high recall (e.g. fraud detection where a missed fraudulent transaction can be costly). If we increase precision we reduce recall and the other way round so we can hardly have both. This dilemma is called precision/recall trade-off, see Section A.2 for some more explanations.\nAn alternative way to look at accuracy for binary classifiers is to look at the receiver operating characteristic (ROC). It looks at recall (TPR) vs. the false positive rate (FPR). Other than that it works similar.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#sec-clustering-sv-svm",
    "href": "clustering/supervised.html#sec-clustering-sv-svm",
    "title": "2  Supervised learning",
    "section": "2.3 Support Vector Machine (SVM)",
    "text": "2.3 Support Vector Machine (SVM)\nThe basic idea of Support Vector Machines (SVMs) is to split observations into distinct clusters via hyperplanes. The have a long history in data science and come in different forms and fashions. Over the years they became more flexible and are still one of the most used tools in industry and science.\n\n2.3.1 Linear SVM\nWe start of with the linear SVM where we construct a hyperplane \\[\n\\langle w, x\\rangle + b = 0\n\\] with a vector \\(w\\) and a constant \\(b\\). There is a natural degree of freedom in this selection of the hyperplane, see Figure 2.7 for two different choices.\n\n\n\n\n\n\n\n\n\n(a) Hyperplane with small margin.\n\n\n\n\n\n\n\n\n\n(b) Hyperplane with large margin.\n\n\n\n\n\nFigure 2.7: We see the hyperplane for the SVM classification scheme. The margin is much larger in the second choice.\n\n\n\nThe optimization inside the SVM aims to find the line that separates the classes best (fewest wrong classifications) and also keeps the largest margin between the observations (the yellow region). The vectors touching the edge of the yellow regions are called support vectors giving the name to the algorithm.\nWith the hyperplane it is easy to classify an observation by simply computing the sign of the projection, i.e. \\[\ny_j (\\langle w, x_j \\rangle + b) = \\operatorname{sign}(\\langle w, x_j \\rangle + b) = \\begin{cases} +1\\\\-1\\end{cases},\n\\] where \\(1\\) corresponds to the versicolor (orange) and \\(-1\\) setosa (blue) observations in Figure 2.7. Therefore, the classifier depends on the position of the observation and is not invariant under scaling.\n\n\n\n\n\n\n\nExercise 2.2 (Linear SVM)  \n\nCompute the vector \\(w\\) in the two cases of Figure 2.7. The vector \\(w\\) is normal to the line. For Figure 2.7 (a) two points on the line are \\(v_1 = [1.25, 4.1]^\\mathrm{T}\\), \\(v_2 = [5, 7.4]^\\mathrm{T}\\). For Figure 2.7 (b) two points on the line are \\(z_1 = [2.6, 4.25]^\\mathrm{T}\\), \\(z_2 = [2, 7]^\\mathrm{T}\\).\nClassify the two points \\[\na = [1.4, 5.1]^\\mathrm{T},\n\\] \\[\nb = [4.7, 7.0]^\\mathrm{T}.\n\\]\n\n\n\n\n\nStating the optimization function such that it is smooth for a linear SVM is a bit tricky. On the other hand, this is needed to allow for most optimization algorithm to work, as they require a gradient to some sort.\nTherefore, the following formulation is quite common. \\[\n\\underset{w, b}{\\operatorname{argmin}} \\sum_j H(y_j, \\overline{y}_j) + \\frac12\\|w\\|^2 \\quad \\text{subject to}\\quad \\min_j|\\langle x_j, w\\rangle| = 1,\n\\] with \\(H(y_j, \\overline{y}_j) = \\max(0, 1 - \\langle y_j, \\overline{y}_j\\rangle)\\), the so called Hinge loss function for counting the number of errors. Furthermore, \\(\\overline{y}_j = \\operatorname{sign}(\\langle w, x_j\\rangle + b)\\).\n\n\n2.3.2 Nonlinear SVM\nIn order to extend the SVM to more complex classification curves, the feature space can be extended. In order to do so, SVM introduces nonlinear features and computes the hyperplane on these features via a mapping \\(x \\to \\Phi(x)\\) and the hyperplane function becomes \\[\nf(x) = \\langle w, \\Phi(x)\\rangle + b,\n\\] and accordingly we classify along \\[\n\\operatorname{sign}(\\langle w, \\Phi(x_j)\\rangle + b) = \\operatorname{sign}(f(x_j)).\n\\]\nEssentially, we change the feature space such that a separation is (hopefully) easier. To illustrate this we use a simple one dimensional example as shown in Figure 2.8 (a). Clearly there is no linear separation possible. On the other hand, if we use \\[\n\\Phi(x_j) = (x_j, x_j^2)\n\\] as our transformation function we move to 2D space and the problem can easily be solved by a line at \\(y=0.25\\).\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nx = np.linspace(-1, 1, 11, endpoint=True)\nx2 = np.zeros_like(x)\ny = np.zeros_like(x)\ny[np.abs(x) &lt; 0.5] = 1\n\nplt.figure()\nplt.scatter(x[y==0], x2[y==0], label=\"class 1\")\nplt.scatter(x[y==1], x2[y==1], label=\"class 2\")\nplt.ylim([-0.1, 1])\nplt.xlim([-1, 1])\nplt.gca().set_aspect(2/3.3)\nplt.legend()\n\nx2 = np.power(x, 2)\n\nplt.figure()\ndata = np.stack([x.flatten(), x2.flatten()]).T\nsvm = LinearSVC(random_state=6020).fit(data, y.flatten())\nplt.scatter(x[y==0], x2[y==0], label=\"class 1\")\nplt.scatter(x[y==1], x2[y==1], label=\"class 2\")\nplt.ylim([-0.1, 1])\nplt.xlim([-1, 1])\nw = svm.coef_[0]\nd = svm.intercept_[0]\nline = lambda x: -w[0] / w[1] * x - d / w[1]\nplt.plot([-1, 1], [line(-1), line(1)], \"k\", label=\"classification line\")\nplt.plot([-1, 1], [0.25, 0.25], \"k:\", label=\"actual boundary line\")\nplt.gca().set_aspect(2/3.3)\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Observations that can not be separated linearly.\n\n\n\n\n\n\n\n\n\n\n\n(b) Enriched feature set with Φ(x)=(x, x^2).\n\n\n\n\n\n\nFigure 2.8: Nonlinear classification with SVM.\n\n\n\n\nAs can be seen in Figure 2.8 (b) the SVM does a great job in finding a split for the two classes, even though not selecting the optimal line, which is not surprising for the given amount of observations.\nAs mentioned before, SVMs are sensitive to scaling. Let us use this example to illustrate the difference together with the concept of pipelines often used in data science context.\n\n\n\n\n\n\nPipeline\n\n\n\nThe main idea of a pipeline is to create a composite as a ordered chain of transformations and estimators, see docs for some more insights.\n\n\nWe can use the pipeline to\n\ncreate the polynomial observations\napply a scaler to our observations\napply the Linear SVM\n\n\ncomposit_svm = sklearn.pipeline.make_pipeline(\n    sklearn.preprocessing.PolynomialFeatures(2),\n    sklearn.preprocessing.StandardScaler(),\n    LinearSVC(random_state=6020)\n)\n\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nx = np.linspace(-1, 1, 11, endpoint=True).reshape(-1, 1)\ny = np.zeros_like(x).flatten()\ny[np.abs(x.flatten()) &lt; 0.5] = 1\n\ncomposit_svm.fit(x, y)\n\nxx = composit_svm[:2].fit_transform(x)\n\nplt.figure()\nplt.scatter(xx[y==0, 1], xx[y==0, 2], label=\"class 1\")\nplt.scatter(xx[y==1, 1], xx[y==1, 2], label=\"class 2\")\n\nw = composit_svm['linearsvc'].coef_[0][1:]\nd = composit_svm['linearsvc'].intercept_[0]\nplt.plot([-1.6, 1.6], [line(-1.5), line(1.5)],\n         \"k\", label=\"scaled classification line\")\n\nplt.ylim([-1.25, 1.75])\nplt.xlim([-1.6, 1.6])\nplt.gca().set_aspect(3/9)\nplt.legend()\n\nxx = composit_svm[:1].fit_transform(x)\nplt.figure()\nplt.scatter(xx[y==0, 1], xx[y==0, 2], label=\"class 1\")\nplt.scatter(xx[y==1, 1], xx[y==1, 2], label=\"class 2\")\n\nw = composit_svm['linearsvc'].coef_[0][1:]\nd = composit_svm['linearsvc'].intercept_[0]\na = composit_svm[\"standardscaler\"].inverse_transform(\n    [np.array([0, -1.6, line(-1.6)])])[0]\nb = composit_svm[\"standardscaler\"].inverse_transform(\n    [np.array([0, 1.6, line(1.6)])])[0]\nplt.plot([a[1], b[1]], [a[2], b[2]], \n         \"k\", label=\"scaled classification line\")\n\nw = svm.coef_[0]\nd = svm.intercept_[0]\nplt.plot([-1, 1], [line(-1), line(1)], \n            \"k:\", label=\"unscaled classification line\")\n\nplt.ylim([-0.1, 1])\nplt.xlim([-1, 1])\nplt.gca().set_aspect(2/3.3)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Classification in the enriched Φ(x)=(x^0, x^1, x^2) and scaled space. Note the first dimension is ignored.\n\n\n\n\n\n\n\n\n\n\n\n(b) Difference between the classification lines when transformed back into the original space.\n\n\n\n\n\n\nFigure 2.9: Classification with autoscaler vs. no scaler.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.3 (Nonlinear SVM)  \n\nExtend the above findings to an example in 2D with a circular classification line. Create tests data of your classification by changing to np.linspace(-1, 1, 12).\n\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nimport sklearn\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnp.random.seed(6020)\nx = np.linspace(-1, 1, 11, endpoint=True)\nx2 = np.linspace(-1, 1, 11, endpoint=True)\nXX, XX2 = np.meshgrid(x, x2)\nZZ = np.pow(XX, 2) + np.pow(XX2, 2)\ny = np.zeros_like(XX)\n\ni = np.sqrt(ZZ) &lt; 0.5\ny[i] = 1\nplt.scatter(XX[y == 0], XX2[y == 0], label=\"class 1\")\nplt.scatter(XX[y == 1], XX2[y == 1], label=\"class 2\")\n\ndata = np.stack([XX.flatten(), XX2.flatten(), ZZ.flatten()]).T\nlabel = y.flatten()\n\n\n\n\n\n\n\n\nFigure 2.10: Dataset in 2D\n\n\n\n\n\n\nRecall the moons example from Section 1.3 and use a degree \\(3\\) PolynomialFeatures for classification.\n\nIn both cases, plot the classification line in a projection onto the original 2D space.\n\n\n\n\n\n\n2.3.3 Kernel Methods for SVM\nWhile enriching the feature space is, without doubt, extremal helpful the curse of dimensionality is quickly starting to influence the performance. The computation of \\(w\\) is getting harder. The so called kernel trick is solving this problem. We express \\(w\\) in a different basis and solve for the parameters of the basis, i.e. \\[\nw = \\sum_{j=1}^m \\alpha_j \\Phi(x_j)\n\\] where \\(\\alpha_j\\) are called the weights of the different nonlinear observable functions \\(\\Phi(x_j)\\). Our \\(f\\) becomes \\[\nf(x) = \\sum_{j=1}^m \\alpha_j \\langle \\Phi(x_j), \\Phi(x) \\rangle + b.\n\\] The so called kernel function is defined as the inner product involved, i.e. \\[\nK(x_j, x) = \\langle \\Phi(x_j), \\Phi(x) \\rangle.\n\\] The optimization problem for \\(w\\) now reads \\[\n\\underset{\\alpha, b}{\\operatorname{argmin}} \\sum_j H(y_j, \\overline{y}_j) + \\frac12\\left\\|\\sum_{j=1} \\alpha_j \\Phi(x_j)\\right\\|^2 \\quad \\text{subject to}\\quad \\min_j|\\langle x_j, w\\rangle| = 1,\n\\] with \\(\\alpha\\) representing the vector of all the \\(\\alpha_j\\). The important part here is that we now minimize of \\(\\alpha\\), which is easier.\nThe kernel function allow almost arbitrary number of observables as it, for example, can represent a Taylor series expansion. Furthermore, it allows an implicit computation in higher dimensions by simply computing the inner product of differences between observations.\nOne of these functions are so called radial basis functions (RBF) with the simplest being a Gaussian kernel \\[\nK(x_j, x) = \\exp\\left(-\\gamma\\|x_j - x\\|^2\\right).\n\\]\nIn scikit-learn this is supported via the SVC class.\nLet us test this implementation with the help of our dogs and cats example.\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn import svm\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\nCD = np.concatenate((dogs_w, cats_w), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\nv = VT.T\n\nfeatures = np.arange(1, 21)\nxtrain = np.concatenate((v[:60, features], v[80:140, features]))\nlabel = np.repeat(np.array([1, -1]), 60)\nxtest = np.concatenate((v[60:80, features], v[140:160, features]))\ntruth = np.repeat(np.array([1, -1]), 20)\n\nsvc = svm.SVC(kernel=\"rbf\", gamma=\"auto\").fit(xtrain, label)\ntest_label = svc.predict(xtest)\ntrain_label = svc.predict(xtrain)\ncm = sklearn.metrics.confusion_matrix(test_label, truth)\nplt.figure()\nplt.scatter(xtrain[train_label == 1, 1], xtrain[train_label == 1, 3],\n            alpha=0.5, color=\"C0\", label=\"train_dogs\")\nplt.scatter(xtrain[train_label == -1, 1], xtrain[train_label == -1, 3],\n            alpha=0.5, color=\"C1\", label=\"train_cats\")\nplt.scatter(xtest[test_label == 1, 1], xtest[test_label == 1, 3],\n            color=\"C0\", label=\"test_dogs\")\nplt.scatter(xtest[test_label == -1, 1], xtest[test_label == -1, 3],\n            color=\"C1\", label=\"test_cats\")\nerror = np.vstack((xtrain[label != train_label, :][:, [1,3]],\n                   xtest[truth != test_label, :][:, [1, 3]]))\nplt.scatter(error[:, 0], error[:, 1],\n            color=\"k\", marker=\"x\", label=\"wrong classification\")\nplt.legend()\nplt.ylim([-0.25, 0.2])\nplt.xlim([-0.15, 0.25])\nplt.xlabel(r\"$PC_2$\")\nplt.ylabel(r\"$PC_4$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.11: Training a SVM with an RBF kernel for the singular vectors 2 to 22. The picture shows the classification results projected for the principal components 2 and 4.\n\n\n\n\n\nWe get a confusion matrix for our test set as\n\n\n\n\n\n\n\n\n\nPP\nPN\n\n\n\n\nP\n18\n3\n\n\nN\n2\n17\n\n\n\n\n\n\n\nIn Figure 2.11 we can see the results of the classification for the entire set of observations, shaded for the training set, and crosses marking the wrongly classified data. With 8 wrongly classified images we have quite a good result, compared to LDA or QDA Figure 2.5. Note, the classification is hard to recognise for the two classes in the simple projection. With the parameters C and gamma we can influence the classification.\n\n\n\n\n\n\n\nExercise 2.4 (Nonlinear SVM with RBF) Recall the moons example from Section 1.3 and use a SVC classification to distinguish the clusters. Look at four different results for \\(\\gamma \\in \\{0.1, 5\\}\\) and \\(C \\in \\{0.001, 1000\\}\\), compare Geron (2022).\nIn all of the four images plot the classification line in a projection onto the original 2D space.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.5 (Nonlinear SVM for regression) We can use SVM for regression. Have a look at docs and use the findings to fit the following observations with various degrees and kernel functions.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 100\nx = 6 * np.random.rand(m) - 3\ny = 1/2 * x ** 2 + x + 2 + np.random.randn(m)\nfig = plt.figure()\nplt.scatter(x, y, label=\"observations\")\nplt.show()\n\n\n\n\n\n\n\n\nCompare (Kandolf 2025, Example 5.2) Link.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#decision-trees",
    "href": "clustering/supervised.html#decision-trees",
    "title": "2  Supervised learning",
    "section": "2.4 Decision trees",
    "text": "2.4 Decision trees\nDecision trees are a common tool in data science, for classification and regression. They are a powerful class of algorithms, that can fit not only numerical data. Furthermore, they form the basis of random forests, one of the most powerful machine learning algorithms available to date.\nThey where not invented for machine learning but have been a staple in business for centuries. Their basic idea is to establish an algorithmic flow chart for making decisions. The criteria that creates the splits in each branch is related to a desired outcome and are therefore important. Often experts are called upon creating such a decision tree.\nThe decision tree learning follows the same principals to create a predictive classification model based on the provided observations and labels. Similar to DBSCAN, they form a hierarchical structure that tries to split in an optimal way. In this regard, they are the counterpart to DBSCAN, as they move from top to bottom and of course use the labels to guide the process.\nThe following key feature make them wildly use:\n\nThe usually produce interpretable results (we can draw the graph).\nThe algorithm mimics human decision making, which helps for the interpretation.\nThe can handle numerical and categorical data.\nThey perform well with large sets of data.\nThe reliability of the classification can be assessed with statistical validation.\n\nWhile there are a lot of different optimizations the base algorithm follows these steps:\n\nLook through all components (features) of an observation \\(x_j\\) that gives the best labeling prediction \\(y_j\\).\nCompare the prediction accuracy over all observations, the best result is used.\nProceed with the two new branches in the same fashion.\n\nLet us apply it to the Fischer Iris dataset to better understand what is happening.\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom six import StringIO\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\nX_iris = iris.data.values\ny_iris = iris.target\n\ndecision_tree = DecisionTreeClassifier(max_depth=2, random_state=6020)\ndecision_tree.fit(X_iris, y_iris)\n\ndot_data = \"fischer_tree.dot\"\n\nsklearn.tree.export_graphviz(decision_tree, out_file=dot_data,  \n                filled=True, rounded=True,\n                feature_names=iris.feature_names,\n                class_names=iris.target_names,\n                special_characters=True)\n\n\n\n\n\n\n\n\n\n\nTree\n\n\n\n0\n\npetal length (cm) ≤ 2.45\ngini = 0.667\nsamples = 150\nvalue = [50, 50, 50]\nclass = setosa\n\n\n\n1\n\ngini = 0.0\nsamples = 50\nvalue = [50, 0, 0]\nclass = setosa\n\n\n\n0-&gt;1\n\n\nTrue\n\n\n\n2\n\npetal width (cm) ≤ 1.75\ngini = 0.5\nsamples = 100\nvalue = [0, 50, 50]\nclass = versicolor\n\n\n\n0-&gt;2\n\n\nFalse\n\n\n\n3\n\ngini = 0.168\nsamples = 54\nvalue = [0, 49, 5]\nclass = versicolor\n\n\n\n2-&gt;3\n\n\n\n\n\n4\n\ngini = 0.043\nsamples = 46\nvalue = [0, 1, 45]\nclass = virginica\n\n\n\n2-&gt;4\n\n\n\n\n\n\n\n\nFigure 2.12: Decision tree for the Fischer iris dataset and depth 2.\n\n\n\n\n\n\n\n\n\n\n\nDisplaying dot files\n\n\n\nIn the code for Figure 2.12 we generate a dot file, that is interpreted with quarto. This allows a better visual integration. In order to do the same offline you need to install graphviz for the installation of dot and also install the python package pydotplus.\nThan you should be able to use:\nimport pydotplus\nfrom six import StringIO\n\ndot_data = StringIO()\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.create_png()\n\n\nAs we can see, for our tree with depth \\(2\\), we only need to split along petal length (cm) and petal width (cm), leaving the two other features untouched, compare Figure 2.\nAs we only have the splits happening in these two variables we can also visualize them easily.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(df.iloc[:, 2][index], df.iloc[:, 3][index], label=name)\n\nplt.xlim([0.5, 7.5])\nplt.ylim([0, 2.6])\nth = decision_tree.tree_.threshold[[0, 2, 3, 4]]\nplt.plot([th[0], th[0]], plt.gca().get_ylim(), 'C1-.',\n          linewidth=2, label=\"split 1\")\nplt.plot([th[0], plt.gca().get_xlim()[1]], [th[1], th[1]], 'C2:',\n          linewidth=2, label=\"split 2\")\nplt.plot([4.95, 4.95], [0, th[1]], 'C3--',\n          linewidth=2, label=\"(split 3)\")\nplt.xlabel(iris.feature_names[2])\nplt.ylabel(iris.feature_names[3])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.13: Splits for the Fischer Iris dataset with the first two split form the above tree and the third split would be the next step for a larger tree.\n\n\n\n\n\nIn Figure 2.13 we can see the the first two splits and the next split if we would increase the tree. With the first split, we immediately separate setosa with \\(100\\%\\) accuracy. The two other classes are a bit tricky and we can not classify everything correct right away. In total \\(6\\) out of \\(150\\) observations are wrongly classified with this simple tree.\nLet us also apply the tree classification to our cats and dogs example.\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom six import StringIO\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\nCD = np.concatenate((dogs_w, cats_w), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=0)\nv = VT.T\n\nfeatures = np.arange(1, 21)\nxtrain = np.concatenate((v[:60, features], v[80:140, features]))\nlabel = np.repeat(np.array([1, -1]), 60)\nxtest = np.concatenate((v[60:80, features], v[140:160, features]))\ntruth = np.repeat(np.array([1, -1]), 20)\n\ndecision_tree_cvd = DecisionTreeClassifier(max_depth=2).fit(xtrain, label)\ntest_label = decision_tree_cvd.predict(xtest)\ncm = sklearn.metrics.confusion_matrix(test_label, truth)\n\ntest_label = svc.predict(xtest)\ncm = sklearn.metrics.confusion_matrix(test_label, truth)\n\ndot_data = \"cvsd_tree.dot\"\n\nsklearn.tree.export_graphviz(decision_tree_cvd,\n                out_file=dot_data,  \n                filled=True, rounded=True,\n                class_names=[\"dog\", \"cat\"],\n                special_characters=True)\n\nscore = decision_tree_cvd.score(xtest, truth)\n\n\n\n\n\n\n\n\n\n\nTree\n\n\n\n0\n\nx\n0\n ≤ 0.049\ngini = 0.5\nsamples = 120\nvalue = [60, 60]\nclass = dog\n\n\n\n1\n\nx\n2\n ≤ 0.019\ngini = 0.346\nsamples = 72\nvalue = [16, 56]\nclass = cat\n\n\n\n0-&gt;1\n\n\nTrue\n\n\n\n4\n\nx\n3\n ≤ 0.133\ngini = 0.153\nsamples = 48\nvalue = [44, 4]\nclass = dog\n\n\n\n0-&gt;4\n\n\nFalse\n\n\n\n2\n\ngini = 0.0\nsamples = 47\nvalue = [0, 47]\nclass = cat\n\n\n\n1-&gt;2\n\n\n\n\n\n3\n\ngini = 0.461\nsamples = 25\nvalue = [16, 9]\nclass = dog\n\n\n\n1-&gt;3\n\n\n\n\n\n5\n\ngini = 0.043\nsamples = 45\nvalue = [44, 1]\nclass = dog\n\n\n\n4-&gt;5\n\n\n\n\n\n6\n\ngini = 0.0\nsamples = 3\nvalue = [0, 3]\nclass = cat\n\n\n\n4-&gt;6\n\n\n\n\n\n\n\n\nFigure 2.14: Decision tree for the Fischer iris dataset and depth 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPP\nPN\n\n\n\n\nP\n18\n3\n\n\nN\n2\n17\n\n\n\n\n\n\n\nIf we compare our confusion matrix for the test cases to the one of SVM we get comparable results. In general, we can see that the first split is along \\(PC_2\\) (note that we do not use the \\(PC_1\\) in the code and therefore \\(x_0=PC_2\\)) and our second split is along \\(PC_4\\). We used the same components before. The third split is along \\(PC_5\\), which we did not consider before hand. Overall the mean accuracy for out test set is 77.5%.\n\n\n\n\n\n\n\nExercise 2.6 (A tree on the moon) Recall the moons example from Section 1.3 and use a DecisionTreeClassifier classification to distinguish the clusters and plot the decision splits.\nPlay around with the parameters, e.g. min_samples_leaf = 5 and see how this influences the score for a test set.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.7 (Tree regression) We can use a decision tree for regression. Have a look at docs and use the findings to fit the observations of Exercise 2.5 with various max_depth values and no value here but limiting min_samples_leaf=10.\n\n\n\n\n\n\n\n\n\n\nSensitivity to rotation and initial state\n\n\n\nDue to the nature of decision trees and the way they split observations by lines, they become sensitive to rotation. Furthermore, the tree construction is based on a random process as the feature for the split is chosen at random.\nTo illustrate this we use a slight adaptation of the (Geron 2022, figs. 6–7).\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\ndef plot_tree_bound(tree, X, y):\n    plt.figure()\n    plt.scatter(X[y==0, 0], X[y==0, 1])\n    plt.scatter(X[y==1, 0], X[y==1, 1])\n    DecisionBoundaryDisplay.from_estimator(\n            tree,\n            X,\n            grid_resolution=2000,\n            ax=plt.gca(),\n            response_method=\"predict\",\n            plot_method=\"contour\",\n            alpha=1.0,\n            levels=[0],\n        )\n    l = np.round(np.abs(X).max() + 0.1, 2)\n    plt.xlim([-l, l])\n    plt.ylim([-l, l])\n\n\nX = np.random.rand(100, 2) - 0.5\ny = (X[:, 0] &gt; 0).astype(np.int32)\n\nangle = np.pi / 4\nrotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],\n                            [np.sin(angle), np.cos(angle)]])\nX_rot = X @ rotation_matrix\n\ntree_square = DecisionTreeClassifier(random_state=42).fit(X, y)\ntree_square_rot = DecisionTreeClassifier(random_state=42).fit(X_rot, y)\ntree_square_rot2 = DecisionTreeClassifier(random_state=6020).fit(X_rot, y)\n\npipeline = make_pipeline(StandardScaler(), PCA())\nX_pca = pipeline.fit_transform(X_rot)\ntree_square_pca = DecisionTreeClassifier(random_state=6020).fit(X_pca, y)\n\nplot_tree_bound(tree_square, X, y)\nplot_tree_bound(tree_square_rot, X_rot, y)\nplot_tree_bound(tree_square_rot2, X_rot, y)\nplot_tree_bound(tree_square_pca, X_pca, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Clear split along the middle for a vertical split.\n\n\n\n\n\n\n\n\n\n\n\n(b) More complicated structure for the rotated observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Different initial random state for the rotated observations.\n\n\n\n\n\n\n\n\n\n\n\n(d) Correction of the rotated observations via PCA and scaling, resulting in an easy split.\n\n\n\n\n\n\n\nFigure 2.15: Illustration of the sensitivity to rotation of decision trees. Note both trees split perfectly.\n\n\n\nIn Figure 2.15 (a) we see the split for the original dataset - random numbers in \\([-0.5, 0.5]^2\\) and the classes separation for \\(x_1 &gt; 0\\). A simple line, i.e. one split is enough to make the separation. If we rotate the dataset by by 45° we can see a way more complicated separation line in Figure 2.15 (b). Furthermore, Figure 2.15 (c) shows that the random_state has an influence as well. Finally, if we apply scaling and PCA to the data we more or less end up at our original sample again Figure 2.15 (d), showcasing the power of these feature extraction techniques once more.\nWe note, all the trees make a perfect classification, just the structure is not as easy to recognize as it could be.\n\n\nAs we could see, trees are very sensitive to the observations and the state. In order to counteract this phenomenon we can compute multiple trees and average the results. This combinations of trees is called ensemble and leads to the next topic.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#ensemble-learning-and-random-forests",
    "href": "clustering/supervised.html#ensemble-learning-and-random-forests",
    "title": "2  Supervised learning",
    "section": "2.5 Ensemble learning and random forests",
    "text": "2.5 Ensemble learning and random forests\nThe notion of wisdom of the crowd suggests, that the decision of multiple people averaged, results in a better decision/judgment than the decisions of a single person. Of course there is a lot of statistics going in, if we want to compute exactly how the result is influenced, or if we should use the same weights for each classification, or e.g. the notion of experts, and much more.\nNevertheless, we can use this concept in our context to create so called ensemble methods, the process itself is called ensemble learning. With this concept we can combine already good classification methods and get a better result than the best classification method included.\nIf we train a multitude of decision trees on various (random) subsets of our observations, we can combine the predictions of the individual trees to an ensemble prediction. The resulting method is called a random forest. This very simple process allows us to generate very powerful classification methods.\nThere are some different approaches for the combination of such methods, we only discuss them briefly, see (Geron 2022, chap. 7) for a more detailed discussion.\n\n\n\n\n\n\nImportant\n\n\n\nAll of the below discussed methods and approaches can be found in the sklearn.ensemble module.\n\n\n\n\n\n\n\n\n\nDefinition 2.3 (Voting classifiers) If we have a set of classifiers \\(C=\\{c_1, \\ldots, c_n\\}\\) of various kinds (even another ensemble classifier like a random forest is welcome), we can simple make a prediction with each resulting in \\(r_1, \\ldots, r_n\\). Now we select the class which occurs most often, i.e. the mode of the predictions, we get a new classifier. This is called hard voting.\nIf all our classifiers can not only produce a prediction but a probability for our prediction, we can also create a soft voting classifier. All we need to do, average the probability of the predictions \\(p_1, \\ldots, p_n\\), and this will give us new probabilities for our ensemble classifier.\n\n\n\n\n\n\nFigure 2.16: Illustration of the difference between hard and soft voting for a ensemble method. For the three shown classifiers the class 0 is the most common. When moving to probabilities, the mean also predicts 0, where more convinced classifiers get a higher weight.\n\n\n\nIn scikit-learn this is can be found in the sklearn.ensemble.VotingClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.4 (Bagging and Pasting) For bagging and pasting the idea is different. Instead of influencing the output, theses approaches influence how to manipulate the training of a set of (potentially equal) classifiers, to achieve overall better results.\nBagging (bootstrap aggregating) uses sampling with replacement, i.e. the same observation can end up multiple times in the training set of the same classifier. Pasting uses sampling without replacement and therefore an observation can be in multiple classifiers but not more than once per classifier.\nTo predict, we can use hard or soft voting from above.\n\n\n\n\n\n\nFigure 2.17: Illustration of the random sampling for bagging and pasting in ensemble classifiers.\n\n\n\nWith these sampling methods it is possible to use the out-of-bag observations (everything that is not used for a particular training) for evaluation of the trained classifier. This is called out-of-bag evaluation.\nIn scikit-learn this is can be found in the sklearn.ensemble.BaggingClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.5 (Random Patches and Random Subspaces) For bagging and pasting it is also possible to sample features and not only observations, i.e. what to look at. This results in a random subset of input features for training for each classifier. This is especially useful, for high dimensional data inputs such as images, as it can speed up the learning process.\nWe call such methods random patches method if we sample both, training observations and training features.\nOn the other hand, if we keep the training observations fix and only sample the training features the resulting method is called a random subspace method.\nIn scikit-learn this is can be achieved by manipulating the arguments max_features, bootstrap_features, and max_samples in the BagginClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.6 (Random Forest) An ensemble of decision tress, (usually) trained via bagging is called a random forest.\nWith a random forest it is quite easy to find out what features are important for the overall result. In scikit-learn this is automatically computed for the sklearn.ensemble.RandomForestClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.7 (Boosting) Boosting or sometimes hypothesis boosting is the process of using ensemble methods to use slow learners to create a fast learner.\nThe general idea is to train a sequence where the output of one is the input of the next classifier. This corrects the previous result and therefore helps to achieve overall better results.\nThe most common methods are called AdaBoost (adaptive boosting) and gradient boosting.\nIn scikit-learn we can find this functionality in the classes sklearn.ensemble.AdaBoostClassifier and sklearn.ensemble.GradientBoostingClassifier.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.8 (Stacking) The general idea of stacking is, to use a blender for combining the results of our ensemble methods. The blender is not just a linear combination like for soft/hard voting but another classifier/model to perform a hopefully better combination. Of course we can stack this approach and produce multiple layers before we combine them into a single result.\n\n\n\n\n\n\nFigure 2.18: Illustration of an \\(m\\) layer stacking with various classifiers in each layer.\n\n\n\nIn scikit-learn this is can be found in the sklearn.ensemble.StackingClassifier class.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.8 (Ensemble methods) We follow the example of (Geron 2022, chap. 7) to explore the various possibilities for ensemble methods.\nFor our dataset we use the moons example:\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=500, noise=0.30, random_state=6020)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=6020)\n\nUse the VotingClassifier class with an LDA, a SVM, and a tree for classification. Make sure to set random_state for each, to have reproducible results. Get the overall score for the test set, as well as the individual scores for the included classifiers.\nfor name, clf in voting.named_estimators_.items():\n    print(f\"{name}, =, {clf.score(X_test, y_test)}\")\nSwitch to soft voting for your ensemble classifier and see how this influences the results.\nCreate a BaggingClassifier with 500 trees and and an appropriate max_samples value. Report the score for this new classifier, also report the out-of-bag score via .oob_score_.\nCreate directly a RandomForestClassifier with 500 trees and appropriate value for max_leaf_nodes.\nCreate a StackingClassifier with an LDA, a SVM, and a tree for classification and a random forest as the final blending step and report the score of this method.\nTrain a random forest for the Fischer Iris dataset and check .feature_importances_ to get an insight on the importance of each feature.\nTrain a random forest for our dogs and cats dataset in raw and wavelet form and check .feature_importances_ to get an insight on the importance of each feature of the PCA.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#multiclass-classification",
    "href": "clustering/supervised.html#multiclass-classification",
    "title": "2  Supervised learning",
    "section": "2.6 Multiclass Classification",
    "text": "2.6 Multiclass Classification\nWe mainly focused on the classification of two classes in the last sections, but obviously not all problems only consist of two classes. Some of the methods discussed, like random forests, support multiclass classification out of the box. For others, there are several approaches to create a multiclass classifier out of a binary classifier.\n\n\n\n\n\n\n\nDefinition 2.9 (One vs. the Rest (OvR) or One vs. All (OvA)) The one versus the rest (OvR) or one versus all (OvA) strategy is to train a binary classifier for each class and always interpret all other classes as the others or the rest class. To classify an observation you get the scores for each classifier and select the one with the highest score.\nFor the Fischer Iris dataset this would result in three classifiers (one for each iris), for the MNIST dataset in ten (one for each number).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.10 (One vs. One (OvO)) The one versus one (OvO) strategy is to train a binary classifier for always two classes and build up a set of classifiers, for \\(n\\) classes we get \\(\\tfrac{n (n-1)}{2}\\) classifiers. To classify an observation you get the result for each classifier and select the one class with the most duels won.\nFor the Fischer Iris dataset this would result in three classifiers, for the MNIST dataset 45.\nThe advantage of OvO over OvR is that each classifier only needs to be trained on a subset and not with the entire dataset. This is especially useful for algorithms that do not scale well, like SVMs.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn scikit-learn the framework automatically realizes that we train a binary classifier for multiple classes and it will select OvR or OvO automatically, depending on the algorithm.\nNevertheless, there exist dedicated classes for the task as well sklearn.multiclass.OneVsOneClassifier and sklearn.multiclass.OneVsRestClassifier.\n\n\n\n\n\n\n\n\n\nExercise 2.9 (Multiclass classification)  \n\nUse SVM for the Fischer Iris dataset and test it.\nCompare with a random forest.\nCreate a confusion matrix for more than two classes with your results and interpret the results.\n\n\n\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/supervised.html#footnotes",
    "href": "clustering/supervised.html#footnotes",
    "title": "2  Supervised learning",
    "section": "",
    "text": "see (Kandolf 2025, Definition 3.5) or the direct link Link↩︎\nsee (Kandolf 2025, Theorem 14.4) or the direct link Link↩︎\nsee (Kandolf 2025, sec. 6.2) or the direct link Link↩︎\nsee Wikipedia overview Link↩︎",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "clustering/semisupervised.html",
    "href": "clustering/semisupervised.html",
    "title": "3  Semi-Supervised learning",
    "section": "",
    "text": "There is a hybrid between unsupervised and supervised learning, called semi supervised learning. If we have a dataset with just a couple of labelled observations, we might be able to use the discussed clustering methods to extend the labels and therefore generate more labelled data.\nWe try this with the (in)famous MNIST dataset of hand written digits. As we did not do this when discussion MNIST in Section 2.2 we first looking at the dataset and establishing some basic properties.\nWe load the dataset and look at its description\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml(\"mnist_784\", as_frame=False)\nprint(mnist.DESCR)\n\n**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n**Please cite**:  \n\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n\nDownloaded from openml.org.\n\n\nThe images are stored in .data and the label in .target. We can look at the first 100 digits\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nim = np.zeros((10 * 28, 10 * 28), int)\nfor i in range(10):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = mnist.data[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3.1: First 100 digits from the MNIST dataset.\n\n\n\n\n\nBefore we go any further in the investigation, we split up the dataset into a training and a test set.\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\nNow let us try with \\(k\\)-means to find the 10 digits.\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nk = 10\nkmeans = KMeans(n_clusters=k, n_init=1, random_state=6020).fit(X_train)\n\nim = np.zeros((1 * 28, 10 * 28), int)\nfor i in range(1):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = \\\n                kmeans.cluster_centers_[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3.2: The cluster means of \\(k\\)-means for 10 clusters.\n\n\n\n\n\nAs can be seen from Figure 3.2, the cluster centers do not recover our 10 digits. It looks like \\(0, 1, 2, 3, 6, 8, 9\\) are easy to discover but \\(4, 5, 7\\) not. If we look closely, we can see \\(4, 5, 7\\) represented in our clusters but not as separate digits. Obviously, this is not a good way to proceed to find our digits. We discussed methods to perform this task in Chapter 2 now let us consider a different scenario.\nOur aim is to only label 50 observations and not more. How can we do this smartly? For this task \\(k\\)-means is a good choice. Instead of trying to find our 10 digits we try to find 50 clusters within our dataset. We use the images closest to the mean as our representative and label these images. Now instead of labeling just 50 random digits we labelled 50 cluster centers. These labels we can than spread out onto the rest of the clusters and we can test how the performance is.\n\n\n\n\n\n\nImportant\n\n\n\nDue to the nature of these notes, being compiled interactively, we restrict the dataset to 2000 points.\n\nX_train = X_train[:2000]\ny_train = y_train[:2000]\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following approach is presented in a similar way in Geron (2022), see GitHub for code in more details.\n\n\nTo get a baseline for our algorithm we use a random forest for the classification and only work with 50 labels.\n\nfrom sklearn.ensemble import RandomForestClassifier\nn_labels = 50\nforest = RandomForestClassifier(random_state=6020).fit(X_train[:n_labels], y_train[:n_labels])\nscore_forest = forest.score(X_test, y_test)\nprint(f\"score for training with {n_labels} labels {score_forest}\")\n\nforest_full = RandomForestClassifier(random_state=6020).fit(X_train, y_train)\nscore_forest_full = forest_full.score(X_test, y_test)\nprint(f\"score for training with {len(y_train)} labels {score_forest_full}\")\n\nscore for training with 50 labels 0.5618\nscore for training with 2000 labels 0.9117\n\n\nWith our 50 labels we get a bit more than 56.18% correct but of course if we use all labels we can achieve results in the 90% range (if we train with all 60000 we get 97%). So how can we approach this problem?\nInstead of just randomly selecting 50 images with labels, let us create 50 clusters, label the image centers, i.e. get good representatives of the classes we are interested in.\n\n\nShow the code for the figure\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nk = 50\nkmeans = KMeans(n_clusters=k, n_init=10, random_state=6020)\nX_digits_dist = kmeans.fit_transform(X_train)\n\nrepresent_digit_idx = X_digits_dist.argmin(axis=0)\nX_rep = X_train[represent_digit_idx]\n\nim = np.zeros((5 * 28, 10 * 28), int)\nfor i in range(5):\n    for j in range(10):\n        im[28*i:28*(i+1), 28*j:28*(j+1)] = X_rep[i*10 + j].reshape(28, 28)\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3.3: The representative (closest image to the cluster mean) of the 50 clusters found with \\(k\\)-means for the first 1500 digits in the MNIST dataset.\n\n\n\n\n\nWe can now label these observations\n\ny_rep = np.array([\n    \"3\", \"4\", \"1\", \"9\", \"9\", \"0\", \"3\", \"0\", \"9\", \"1\",\n    \"9\", \"8\", \"0\", \"9\", \"3\", \"6\", \"6\", \"7\", \"1\", \"6\",\n    \"6\", \"5\", \"3\", \"2\", \"2\", \"0\", \"0\", \"6\", \"7\", \"2\",\n    \"5\", \"1\", \"7\", \"3\", \"4\", \"8\", \"6\", \"0\", \"8\", \"5\",\n    \"0\", \"3\", \"2\", \"3\", \"7\", \"4\", \"5\", \"4\", \"2\", \"7\" \n])\n\nforest_rep = RandomForestClassifier(random_state=6020).fit(X_rep, y_rep)\nscore_forest_rep = forest_rep.score(X_test, y_test)\nprint(f\"score for training with {len(y_rep)} labels {score_forest_rep}\")\n\nscore for training with 50 labels 0.6551\n\n\nThis helped us increase our score significantly, but we can do better. We can extend our labels from the representatives to the entire cluster and train with that.\n\ny_train_prop = np.empty(len(X_train), dtype=str)\nfor i in range(k):\n    y_train_prop[kmeans.labels_ == i] = y_rep[i]\n\nforest_prop = RandomForestClassifier(random_state=6020).fit(X_train, y_train_prop)\nscore_forest_prop = forest_prop.score(X_test, y_test)\nprint(f\"score for training with {len(y_train_prop)} propagated labels {score_forest_prop}\")\n\nscore for training with 2000 propagated labels 0.7672\n\n\nThis again increases our score by another good 10%. If we check our propagated labels, we see that, we can not expect more as we only have an accuracy slightly higher than our classification result, i.e. we provide wrong results labels for our classification.\n\nnp.mean(y_train_prop == y_train)\n\nnp.float64(0.7745)\n\n\nLet us try to eliminate outliers by removing the 10% of instances that are far away from our cluster centers.\n\npercentile_closest = 90\n\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\nfor i in range(k):\n    in_cluster = (kmeans.labels_ == i)\n    cluster_dist = X_cluster_dist[in_cluster]\n    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n    above_cutoff = (X_cluster_dist &gt; cutoff_distance)\n    X_cluster_dist[in_cluster & above_cutoff] = -1\n\npartially_propagated = (X_cluster_dist != -1)\nX_train_pprop = X_train[partially_propagated]\ny_train_pprop = y_train_prop[partially_propagated]\nnp.mean(y_train_pprop == y_train[partially_propagated])\n\nnp.float64(0.8102189781021898)\n\n\nWe have cleaned up our source but does it have a big influence?\n\nforest_pprop = RandomForestClassifier(random_state=6020).fit(X_train_pprop, y_train_pprop)\nscore_forest_pprop = forest_pprop.score(X_test, y_test)\nprint(f\"score for training with {len(y_train_pprop)} labels {score_forest_pprop}\")\n\nscore for training with 1781 labels 0.768\n\n\nWe actually do not change our result with this step much.\nNevertheless, overall we could see that by smartly labeling 50 out of 2000 instances we could increase our score from from about 56.18% to 76.8%, which is not bad.\n\n\n\n\n\n\n\nExercise 3.1 (Improve the results further) There are several ways we can improve our results.\n\nTry to optimize the parameters of the random forest, for number of trees and leaves.\nOptimize the clustering for the first step.\nWe can use other methods we have seen in Chapter 2 and combine them to an ensemble learning.\nBy starting to additionally label observations where our classifier is the least sure about.\nWe can again work with clusters to smartly label additional observations.\n\nNote: if we use all the 60000 samples we get about 84% with the presented steps.\n\n\n\n\n\n\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "Clustering and Classification",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Semi-Supervised learning</span>"
    ]
  },
  {
    "objectID": "data/index.html",
    "href": "data/index.html",
    "title": "Data Management and Data Engineering",
    "section": "",
    "text": "In the previous sections we discussed classification with the help of various examples. We did not talk about how we provide the observations/data for our training, validation, and testing. They where just there or more precisely, we just loaded them. Furthermore, we also did not talk about how to persist the model we generated, after the program finished the model was gone again and needs to be retrained to further use it. Nevertheless, data management and data engineering are essential aspects of our topics at hand. As with other topics discussed in these notes, we can not hope to cover the entire field comprehensively, but we can provide an introduction and highlight the main and basic concepts that help us to get started. Therefore, we will focus on a couple of aspects and use only one specific software solution for the implementation. On the one hand, this limits us and the notes might will be outdated sooner but in the spirit of the practical introduction it will allow us to work on the topics at hand. The intend is to see it in action and to help understand the practical aspects of the challenges within these topics.\nThere are several trivial but important key aspects to managing data:\nand so much more.\nThe last step in 3  Semi-Supervised learning is actually quite a useful illustration to highlight the importance of having correct data. We could see that with wrong labels we can not hope to generate correct result. Consequently, if we can not say for sure if we had correct labels for the training three months ago we can not validate results.\nIn addition, if we come to images, we could also see how changing the basis (raw to wavelet) changed our results, see Dogs and Cats. As each pixel of an image can be considered a feature for our training, we can imagine that changing only a small amount might change our model and our performance. Now consider that most formats for storing images include some kind of image compression (see some discussed in Kandolf (2025)) we can imagine that this can have major influence on the training and resulting model. Therefore, we also need to keep track how these features (our images) are generated and stored if we also want to make sure we do not get unexpected behaviour in our results.\nThe entire field of data management and data engineering is not new but received a lot of focus in the machine learning age, it also spawned several (research) fields which are often captured under the umbrella of data science.\nA nice deep dive into the topics that focuses on concepts and not on technologies is Reis (2022). It build on the so called data engineering lifecycle illustrated in Figure 1.\nIn this chapter we will see some aspects of the lifecycle in action. We will also integrate our code for model and data generation into this framework. Several aspects to the related topics are very important but can not be fully covered here, we highlight sections of importance.\nSo let us start and dive into the shallows of data management.",
    "crumbs": [
      "Data Management and Data Engineering"
    ]
  },
  {
    "objectID": "data/index.html#footnotes",
    "href": "data/index.html#footnotes",
    "title": "Data Management and Data Engineering",
    "section": "",
    "text": "We talk about a drift as an evolution of data that invalidates the data model, see Wikipedia, accessed on the 21.03.2025↩︎",
    "crumbs": [
      "Data Management and Data Engineering"
    ]
  },
  {
    "objectID": "data/model.html",
    "href": "data/model.html",
    "title": "4  Model persistence",
    "section": "",
    "text": "4.1 Open Neural Network Exchange - ONNX\nSo far, we either loaded a dataset or generated it on the fly. Therefore, we start by looking into ways to persist the models we generated.\nThe general idea is to simply store the object we generate and load it at some later time. Nevertheless, this can be quite tricky as we will see in the following.\nFor example it might be that we do our training in a different environment than the inference or prediction. It might even be the case, that we switch programming language for these tasks to extract the best performance.\nAs we mainly work with scikit-learn we introduce the concepts with it. Our first step is to check the documentation docs - model persistence and we can use it as reference for this introduction. We introduce different possibilities, all of them have strengths and weaknesses, unfortunately there is no gold standard.\nLet us use the following toy example with our cats and dogs as reference, see Section 1.2.\nThe use-case for ONNX is to persist a model without necessarily using the Python object itself. This is especially useful, when the runtime for distributing the model is not Python or a very restricted Python environment.\nNow let us see how we can persist the model of Listing 4.1 via ONNX.\nfrom skl2onnx import to_onnx\n1onx = to_onnx(voting_clf, X_train[:1].astype(np.int64))\nwith open(\"model.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())\n\n\n1\n\nNot all data types are supported, so we need to convert our reference training sample to int64 (potentially increasing storage demands).\nAs mentioned, the file format is binary so it does not make a lot of sense to actually read the resulting file in plain text but we can have a look at the size.\n80K model.onnx\nUnfortunately, there is no method to convert back to our scikit-learn. What we can do is to use the onnxruntime and see if we still get the same score as before.\nimport onnxruntime as ort\n\nmodel = ort.InferenceSession(\"model.onnx\")\ninput_name = model.get_inputs()[0].name\npredictions = model.run(None, {input_name: X_test.astype(np.int64)})\n\nscore = sklearn.metrics.accuracy_score(y_test, predictions[0])\nprint(f\"We have a test_score of for {score} for the recovered model.\")\npredictions = model.run(None, {input_name: X_train.astype(np.int64)})\nscore = sklearn.metrics.accuracy_score(y_train, predictions[0])\nprint(f\"We have a train_score of for {score} for the recovered model.\")\n\nWe have a test_score of for 0.825 for the recovered model.\nWe have a train_score of for 1.0 for the recovered model.\nAs we can see, the score is actually better than before, which is odd and definitely not intended.\nFurthermore, if we inspect our predictions output from above a bit more it looks like we have switched to soft voting.\nOverall, we can see that ONNX is a way to persist a model such that we can make predictions with it but we do no longer have the Python object. Of course it is possible that we can write our own provided and persist our required models to a better state, see sklearn-onnx docs Regarding file size, we can already say it is efficient and it provides some independence from our training environment.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model persistence</span>"
    ]
  },
  {
    "objectID": "data/model.html#sec-data-model-onnx",
    "href": "data/model.html#sec-data-model-onnx",
    "title": "4  Model persistence",
    "section": "",
    "text": "ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers. LEARN MORE\nSource: https://onnx.ai/, accessed 07.03.2025.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is due to the fact, that skl2onnx is not able to convert all scikit-learn models exactly. This is especially true for the SVC class included in our composite model. Therefore, the class is stored with the same weights but slightly different parameters.\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.1 (Test how the recovery works for SVC) Try to rewrite the model and check the resulting score after recovery vs. the original score for the following modifications.\n\nRemove the probability=True for SVC.\nReplace SVC by LinearSVC.\nRemove the SVC all together and replace it with a LogisticRegression classifier.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model persistence</span>"
    ]
  },
  {
    "objectID": "data/model.html#sec-data-model-pickle",
    "href": "data/model.html#sec-data-model-pickle",
    "title": "4  Model persistence",
    "section": "4.2 pickle - Python object serialization",
    "text": "4.2 pickle - Python object serialization\nWe can also swing the pendulum in the other direction and use the Python standard library pickle to persist our model.\nBefore we go into more details, we should emphasise the potential security problem we introduce with pickle as stated in its own docs:\n\n\n\n\n\n\nThe pickle module is not secure. Only unpickle data you trust.\n\n\n\nIt is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Never unpickle data that could have come from an untrusted source, or that could have been tampered with.\nConsider signing data with hmac if you need to ensure that it has not been tampered with.\nSafer serialization formats such as json may be more appropriate if you are processing untrusted data, see Comparison with json.\n\n\nAs pickle is the native implementation in Python. It is easy to use and works for (almost) all models and configurations. The downside is, that we need to absolutely trust the source of our model. We need to trust all steps from the storage provider, through the network to our own infrastructure.\nFurthermore, the environment we load the model into needs to be the same as the one we stored it from. As we have already seen how the dependency hell1 influences our development, we bring import these issues together with the pickle file.\nThis notably implies, it is not guaranteed that a model can be loaded with a different scikit-learn version or let alone a different numpy version, that is only a sub-dependency of scikit-learn. Furthermore, if a different hardware is involved there might be problems as well, e.g. a different architecture of an integer or float. As a consequence, if we use pickle a thorough version control with package management is key!\nIf we have a model that moves around different processes via the disc or is restored frequently from storage but can not be permanently in storage and therefore performance for loading and storing is of interest we can also use joblib as a more performant alternative.\nNow let us see how we can persist the model of Listing 4.1 as a pickle file.\n\nfrom pickle import dump\nwith open(\"model.pkl\", \"wb\") as f:\n    dump(voting_clf, f, protocol=5)\n\nAgain, the file format is binary so it does not make a lot of sense to actually read the file in plain text but we can have a look at the size\n\n\n304K    model.pkl\n\n\nand we can see that the storage demands are higher than for ONNX.\nWe restore the model via\n\nfrom pickle import load\nwith open(\"model.pkl\", \"rb\") as f:\n    clf = load(f)\nscore = clf.score(X_test, y_test)\nprint(f\"We have test_score of {score} after loading the object again.\")\nscore = clf.score(X_train, y_train)\nprint(f\"We have train_score of {score} after loading the object again.\")\n\nWe have test_score of 0.8 after loading the object again.\nWe have train_score of 1.0 after loading the object again.\n\n\nAs we can see, the score stays the same and we can deal with the loaded object in the same way as with the original.\n\n\n\n\n\n\n\nExercise 4.2 (Further investigations for pickle)  \n\nFor the loaded model, switch to soft voting by calling\nclf[1].voting = \"soft\"\nclf[1].named_estimators[\"svc\"].probability = True\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\nUse joblib to persist and load the module, also check the file size.\nSwitch for the SVC to a \"rbf\" kernel and see if you can fully recover the object.\nSome user defined functions can cause problems for pickle try persisting the model with cloudpickle and test with the user defined kernel function rbf = lambda x, y: np.exp(1e-2 * np.abs(x@y.T)).",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model persistence</span>"
    ]
  },
  {
    "objectID": "data/model.html#skops.io---the-more-secure-python-alternative",
    "href": "data/model.html#skops.io---the-more-secure-python-alternative",
    "title": "4  Model persistence",
    "section": "4.3 skops.io - the more secure Python alternative",
    "text": "4.3 skops.io - the more secure Python alternative\nAs an alternative to pickle we can use skops.io. It is developed as a secure alternative for pickle and therefore supports a wide range of objects. The main idea is, that only trusted functions are loaded and not everything included in the file. It is also possible to verify our data before loading it into our program, increasing the security further. Still, it returns the Python object, if it can be loaded, and we can manipulate it in the same fashion as with pickle.\nAs a downside, the process is slower and some user defined functions/object might not work as desired. This also implies, that we need to have the same environment for loading as we had for storing the Python object, similar to pickle.\nThe interface itself is simple and follows pickle.\n\nimport skops.io as sio\nobj = sio.dump(voting_clf, \"model.skops\")\n\nFor comparison, we show the size of the file\n\n\n26M model.skops\n\n\nand we can see that this format has a significant higher overhead as the other formats.\nRetrieving the model is a two step process, first loading the untrusted types and than loading the verified objects.\n\n1unknown_types = sio.get_untrusted_types(file=\"model.skops\")\nfor i, a in enumerate(unknown_types):\n    print(f\"Unknown type at {i} is {a}.\")\n\nclf = sio.load(\"model.skops\", trusted=unknown_types)\nscore = clf.score(X_test, y_test)\nprint(f\"We have test_score of {score} after loading the object again.\")\nscore = clf.score(X_train, y_train)\nprint(f\"We have train_score of {score} after loading the object again.\")\n\n\n1\n\nWe should investigate the contents of unknown_types, and only load if we trust everything we see.\n\n\n\n\nUnknown type at 0 is sklearn.utils._bunch.Bunch.\nWe have test_score of 0.8 after loading the object again.\nWe have train_score of 1.0 after loading the object again.\n\n\n\n\n\n\n\n\n\nExercise 4.3 (Further investigations for skops.io)  \n\nThe included PCA works with float64, this is not necessary, can we reduce the file size by switching to float16? Hint: look at voting_clf[0].components_.\nApply the self defined kernel from Exercise 4.2 and test the load/recover cycle.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model persistence</span>"
    ]
  },
  {
    "objectID": "data/model.html#comparison-of-the-different-approaches",
    "href": "data/model.html#comparison-of-the-different-approaches",
    "title": "4  Model persistence",
    "section": "4.4 Comparison of the different approaches",
    "text": "4.4 Comparison of the different approaches\nThe docs are doing an excellent job in summarizing the key differences.\n\nBased on the different approaches for model persistence, the key points for each approach can be summarized as follows:\n\nONNX: It provides a uniform format for persisting any machine learning or deep learning model (other than scikit-learn) and is useful for model inference (predictions). It can however, result in compatibility issues with different frameworks.\nskops.io: Trained scikit-learn models can be easily shared and put into production using skops.io. It is more secure compared to alternate approaches based on pickle because it does not load arbitrary code unless explicitly asked for by the user. Such code needs to be packaged and importable in the target Python environment.\njoblib: Efficient memory mapping techniques make it faster when using the same persisted model in multiple Python processes when using mmap_mode=\"r\". It also gives easy shortcuts to compress and decompress the persisted object without the need for extra code. However, it may trigger the execution of malicious code when loading a model from an untrusted source as any other pickle-based persistence mechanism.\npickle: It is native to Python and most Python objects can be serialized and deserialized using pickle, including custom Python classes and functions as long as they are defined in a package that can be imported in the target environment. While pickle can be used to easily save and load scikit-learn models, it may trigger the execution of malicious code while loading a model from an untrusted source. pickle can also be very efficient memorywise if the model was persisted with protocol=5 but it does not support memory mapping.\ncloudpickle: It has comparable loading efficiency as pickle and joblib (without memory mapping), but offers additional flexibility to serialize custom Python code such as lambda expressions and interactively defined functions and classes. It might be a last resort to persist pipelines with custom Python components such as a sklearn.preprocessing.FunctionTransformer that wraps a function defined in the training script itself or more generally outside of any importable Python package. Note that cloudpickle offers no forward compatibility guarantees and you might need the same version of cloudpickle to load the persisted model along with the same version of all the libraries used to define the model. As the other pickle-based persistence mechanisms, it may trigger the execution of malicious code while loading a model from an untrusted source.\n\nSource: scikit-learn.org, accessed 07.03.2025.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model persistence</span>"
    ]
  },
  {
    "objectID": "data/model.html#further-considerations",
    "href": "data/model.html#further-considerations",
    "title": "4  Model persistence",
    "section": "4.5 Further considerations",
    "text": "4.5 Further considerations\nNow that we know how to persist our models, or at least hope to do so, we need to talk about how we keep track of our different model versions (parameters, training data, random seeds, etc.).\nIn the previous exercises we created multiple versions of our model and stored them to disc. If we now look at the different files, do we still know which version corresponds to which code block?\nAs we experiment with different parameters for our composite method - in pursuit of better results - we’ll likely generate even more model variations. To ensure reproducibility, we need a way to track our models alongside the code and parameters that produces them. This is what we are going to look into in the next section.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model persistence</span>"
    ]
  },
  {
    "objectID": "data/model.html#footnotes",
    "href": "data/model.html#footnotes",
    "title": "4  Model persistence",
    "section": "",
    "text": "see MECH-M-DUAL-1-SWD, Section 4.1↩︎",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model persistence</span>"
    ]
  },
  {
    "objectID": "data/code.html",
    "href": "data/code.html",
    "title": "5  Code persistence",
    "section": "",
    "text": "5.1 Externalize the parameters/configuration\nWhen talking about storing the model we quickly realise that there is more to this task than simply calling pickle.dump. The same is true for persisting the model, data, and code together.\nLuckily, we already have the perfect tool for controlling the code in use, git1. We can make sure to commit our source files and when using a proper package manager like pdm2 we can hope to reproduce our environment for the formats that require these features.\nTo illustrate this, we move the code from Listing 4.1 into a project. In this process we move from the dense script to a proper file structure by splitting up the source script into several parts. Furthermore, we rework some of the details, like only downloading the data once.\nAfter the redesign we get a structure looking something like the following, see 22788a6 for reference and file content:\nWe can train the model by calling train.py (in this case with a logger3 on DEBUG enabled).\nOf course we can also load the model again and do inference with it by calling inference.py\nNow we can start connecting the model and the code. Our model was created at commit 22788a6 and stored into the directory model.\nNow there are several things we can do to make sure this is reflected within our little project.\nThis is rather cumbersome and requires a lot of discipline, it will also become tricky if several people work on the same project and run experiments with different parameters.\nFirst thing we do is we externalize the configuration to make sure this is no longer part of our main code source. This means we can filter between actual changes to the structure and source and a commit for a simple experiment.\nyaml is the format to go for these aspects, see Wikipedia. It is a human readable data serialization language. One possible interpretation of the config (among many others is) can be seen in the next code block.\nWe can use the Python package omegaconf to load and use it.\nOne feature of the OmegaConfclass is that we can use unpacking4 and therefore write a line like:\ndramatically simplifying our code. Have a look at commit 9f7dead to see this implemented and in action. As a our default parameter template we include the above parameters as params.yaml in the main directory of our project.\nNow that the config is externalized we can continue on our quest to persist all aspects of this project together in a useful way.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code persistence</span>"
    ]
  },
  {
    "objectID": "data/code.html#externalize-the-parametersconfiguration",
    "href": "data/code.html#externalize-the-parametersconfiguration",
    "title": "5  Code persistence",
    "section": "",
    "text": "PCA:\n  type: sklearn.decomposition.PCA\n  init_args:\n    n_components: 41\n\nVotingClassifier:\n  type: sklearn.ensemble.VotingClassifier\n  init_args:\n    flatten_transform: False\n  estimators:\n    - LinearDiscriminantAnalysis\n    - RandomForestClassifier\n    - SVC\n\nLinearDiscriminantAnalysis:\n  type: sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n  init_args:\n    solver: svd\n\nRandomForestClassifier:\n  type: sklearn.ensemble.RandomForestClassifier\n  init_args:\n    n_estimators: 500\n    max_leaf_nodes: 2\n    random_state: 6020\n\nSVC:\n  type: sklearn.svm.SVC\n  init_args:\n    kernel: linear\n    probability: True\n    random_state: 6020\n\n\nPCA(**params[\"PCA\"].init_args)\n\n\n\n\n\n\n\n\nExercise 5.2 (Externalize params) We can generate the entire model from the params, even the different classes. By using the function from importlib import import_module we can dynamically load a class with the following snippet:\nmodule = import_module(params[\"PCA\"].type.rsplit(\".\", 1)[0])\nPCA = getattr(module, params[\"PCA\"].type.rsplit(\".\", 1)[-1])(\n    **params[\"PCA\"].init_args\n)\nUse this to make the model creation more and more dynamic.\n\nReplace the array estimators=[] by dynamically loading the different estimators.\nUse the same for the pipeline\n\nNote: If you see an advantage in rewriting the config structure to make your code easier feel free to do so.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code persistence</span>"
    ]
  },
  {
    "objectID": "data/code.html#data-persistence",
    "href": "data/code.html#data-persistence",
    "title": "5  Code persistence",
    "section": "5.2 Data persistence",
    "text": "5.2 Data persistence\nOur model depends on the code, the configuration, but crucially also on the input data itself. In order to make sure that we can reliably reproduce a model we also need to make sure our data is reproducible.\nIn our reference project we use some files from GitHub (see @Brunton and Kutz (2022) for as reference) but let us still make sure they are tracked within our system.\n\n\n\n\n\n\nImportant\n\n\n\nWe use a simple tool for the next paragraphs to illustrate the concepts. There are plenty of alternatives that can and are used.\nThe selection of dvc is purely motivated by the following features: it can easily be used in such our lecture, has some basic features that illustrate the requirements on such systems, integrates nicely with Python, has only limited dependencies outside of the Python eco-system.\nAlternatives include:\n\nGit Large File Storage\nPachyderm\nlakefs\n\nLike always, the best platform depends on the project and the available infrastructure.\n\n\nOne tool for data version control is dvc. As it is written in Python we can even add and track the version via our package manager pdm add dvc. Once installed we can initialize it in our project,\nMLB-DATA$ pdm run dvc init\n\nInitialized DVC repository.\n\nYou can now commit the changes to git.\n\n+---------------------------------------------------------------------+\n|                                                                     |\n|        DVC has enabled anonymous aggregate usage analytics.         |\n|     Read the analytics documentation (and how to opt-out) here:     |\n|             &lt;https://dvc.org/doc/user-guide/analytics&gt;              |\n|                                                                     |\n+---------------------------------------------------------------------+\n\nWhat's next?\n------------\n- Check out the documentation: &lt;https://dvc.org/doc&gt;\n- Get help and share ideas: &lt;https://dvc.org/chat&gt;\n- Star us on GitHub: &lt;https://github.com/iterative/dvc&gt;\nTo make the changes permanent we commit the directory .dvc (with all the files included) to git and therefore our project now runs with dvc, see commit df38086 for reference.\nTo add the data directory simply run\nMLB-DATA$ pdm run dvc add data\n\n100% Adding...|███████████████████████████████████████|1/1 [00:00,  5.67file/s]\n\nTo track the changes with git, run:\n\n        git add data.dvc\n\nTo enable auto staging, run:\n\n        dvc config core.autostage true\nAgain, to make it permanent in the project we need to add data.dvc to git (as suggested).\n\n\n\n\n\n\nNote\n\n\n\nAt this point we have the two files catData_w.mat and dogData_w.mat in this directory and they are under version control from dvc.\n\n\nIf we take a look into data.dvc we can see that it tracks the files via md5 SHAs and includes some additional useful information .\nMLB-DATA$ cat data.dvc\n\nouts:\n- md5: 5987e80830fc2caf6d475da3deca1dfe.dir\n  size: 111165\n  nfiles: 2\n  hash: md5\n  path: data\nAs mentioned, dvc works similar to git so eventually we will need to include a remote that we push data to. For now we just work locally, similar as we could do for a git repository.\nOther than that, we can now change the files use dvc add data and as soon as we commit the corresponding change in the data.dvc to git we know exactly what data is used and we can also restore it.\nTo do these operations dvc uses a cache (default location is in .dvc/cache) and the files are links to the cache.\nThe most important dvc commands are (we link the docs for an extended reference):\n\ndvc add to add a file or directory.\ndvc checkout brings your work space up to date, according to the .dvc files current states.\ndvc commit updates the .dvc files and stores the content in the cache, most of the time called implicitly.\ndvc config view and change the config for the repo or globally for dvc on the system.\ndvc data status chow changes to the files in the work space with respect to thegit HEAD.\ndvc destroy remove all files and dvc structures for the current project, including the cache. The symlinks will be replaced by the actual data so the current state is preserved.\ndvc exp has multiple subcommands and is used to handle experiments, we will use this command later.\ndvc fetch download files from the remote repository to the cache.\ndvc pull download files from the remote and make them visible in the working space.\ndvc push upload the tracked files to the remote.\n\nFor the other commands run dvc --help or look at the docs.\n\n\n\n\n\n\nNote\n\n\n\nAs can be seen from the command, dvc was build with git in mind and feels quite similar. This means it uses the same commands for the same (or almost same) operations. Unfortionalty or luckily (depending on our preferences), it also brings in the sometimes confusing command structure and the concepts like a working space.\nRecall the introduction to git for some of these 5. We will use this to also recall some details about git to, hopefully, further foster the understanding\n\n\nNow our files are tracked, but as you probably realised we did not add the module folder to dvc. This is due to the fact that we can use the dvc exp feature to allow for more fine grained control and even parameter overviews. Furthermore, we can use logging features to integrate with this system even better.\ndvc also allows advanced nice pipelines (we look at a small example later) and automatic computation as well as monitoring. In all its facets this is quite advanced and can be introduced when our project grows.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code persistence</span>"
    ]
  },
  {
    "objectID": "data/code.html#dvclive-for-experiment-management",
    "href": "data/code.html#dvclive-for-experiment-management",
    "title": "5  Code persistence",
    "section": "5.3 dvclive for experiment management",
    "text": "5.3 dvclive for experiment management\ndvclive works best with the big ML Frameworks like keras or pytorch but we can also utilize it for our example project. The introduction to the experiment management from the dvc perspective can be found in the docs.\nTo show some of the dvclive features we reworked to code, see commit 519a85 for the changes (including pdm add dvclive). Now, when we run our job once more it will create the dvclive directory with a couple of subdirectories containing our metrics, looking like the following output.\nMLB-DATA$ pdm run src/MECH-M-DUAL-2-MLB-DATA/train.py\n\nINFO:root: We have a hard voting train-score of 1.0\nINFO:root: We have a hard voting test-score of 0.8\n100% Adding...|███████████████████████████████████████|1/1 [00:00,  7.64file/s]\n\n\n\n\n\n\nOutput with logger on DEBUG.\n\n\n\n\n\nMLB-DATA$ pdm run python src/MECH-M-DUAL-2-MLB-DATA/train.py\n\nDEBUG:root: Loaded the data with Split of 60 to 20 per category.\nDEBUG:root: Load config\nDEBUG:root: Create classifier\nDEBUG:root: Train classifier\nDEBUG:root: Score classifier\nINFO:root: We have a hard voting train-score of 1.0\nINFO:root: We have a hard voting test-score of 0.8\nDEBUG:root: Save clf to skops file dvclive/artifacts/model.skops\n100% Adding...|███████████████████████████████████████|1/1 [00:00,  9.35file/s]\nDEBUG:scmrepo.git: Stashing workspace\nDEBUG:scmrepo.git.stash: Stashing changes in 'refs/stash'\nDEBUG:scmrepo.git: Detaching HEAD at 'HEAD'\nDEBUG:scmrepo.git.stash: Applying stash commit '7d0806908ca50c01b5626e944591e138a8cebc28'\nCollecting files and computing hashes in data        |0.00 [00:00,     ?file/s]\nDEBUG:fsspec.memoryfs: open file /.UFCO7_lfUrOd1sAFVpO-vw.tmp\nDEBUG:fsspec.memoryfs: info: memory://.UFCO7_lfUrOd1sAFVpO-vw.tmp\nCollecting files and computing hashes in data        |0.00 [00:00,     ?file/s]\nDEBUG:fsspec.memoryfs: open file /.4cDC9-sZfddrOSDnK8EeJA.tmp\nDEBUG:fsspec.memoryfs: info: memory://.4cDC9-sZfddrOSDnK8EeJA.tmp\nDEBUG:scmrepo.git.stash: Stashing changes in 'refs/exps/stash'\nDEBUG:scmrepo.git: Restore HEAD to 'main'\nDEBUG:scmrepo.git: Restoring stashed workspace\nDEBUG:scmrepo.git.stash: Popping from stash 'refs/stash'\nDEBUG:scmrepo.git.stash: Applying stash commit '7d0806908ca50c01b5626e944591e138a8cebc28'\nDEBUG:scmrepo.git.stash: Dropping 'refs/stash@{0}'\nDEBUG:scmrepo.git.stash: Dropping 'refs/exps/stash@{0}'\nDEBUG:scmrepo.git: Detaching HEAD at 'f264314e5442315c74c395a89c2c73a3b7269f90'\nDEBUG:scmrepo.git.stash: Applying stash commit 'adac86009edec08745dffc95c3e6eb68c9fb4dcb'\nDEBUG:fsspec.memoryfs: open file /.cSw5cWuYdyH1AkprvFGF8g.tmp\nDEBUG:fsspec.memoryfs: info: memory://.cSw5cWuYdyH1AkprvFGF8g.tmp\nCollecting files and computing hashes in data         0.00 [00:00,     ?file/s]\nDEBUG:fsspec.memoryfs: open file /.-QV8fKiPGOS3tR1JS0SE2w.tmp\nDEBUG:fsspec.memoryfs: info: memory://.-QV8fKiPGOS3tR1JS0SE2w.tmp\nCollecting files and computing hashes in data        |0.00 [00:00,     ?file/s]\nDEBUG:fsspec.memoryfs: open file /.nJTqHs5Fi91gZyK82VYbvA.tmp\nDEBUG:fsspec.memoryfs: info: memory://.nJTqHs5Fi91gZyK82VYbvA.tmp\nDEBUG:scmrepo.git: Restore HEAD to 'main'\nHere we can also see how dvc interacts with git to store the files.\n\n\n\nAnd the resulting files are stored in dvclive.\nMLB-DATA$ tree dvclive/\n\ndvclive/\n├── artifacts\n│   └── model.skops\n├── metrics.json\n├── params.yaml\n└── plots\n    ├── metrics\n    │   ├── testscore.tsv\n    │   └── trainscore.tsv\n    └── sklearn\n        └── confusion_matrix.json\n\n5 directories, 6 files\nThe experiment is automatically added. We can check this with dvc exp show\nMLB-DATA$ pdm run dvc exp show\n\n ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  Experiment                 Created        trainscore   testscore   PCA/n_components   LinearDiscriminantAnalysis/solver   RandomForestClassifier/n_estimators   RandomForestClassifier/max_leaf_nodes   RandomForestClassifier/random_state   SVC/kernel   SVC/probability   SVC/random_state   PCA.type                    PCA.init_args.n_components   VotingClassifier.type               VotingClassifier.init_args.flatten_transform   VotingClassifier.estimators                                       LinearDiscriminantAnalysis.type                            LinearDiscriminantAnalysis.init_args.solver   RandomForestClassifier.type               RandomForestClassifier.init_args.n_estimators   RandomForestClassifier.init_args.max_leaf_nodes   RandomForestClassifier.init_args.random_state   SVC.type          SVC.init_args.kernel   SVC.init_args.probability   SVC.init_args.random_state  \n ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  workspace                  -                       1         0.8   41                 svd                                 500                                   2                                       6020                                  linear       True              6020               sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                        \n  main                       Mar 19, 2025            -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                        \n  └── 9e83c73 [sural-cyma]   10:03 AM                1         0.8   41                 svd                                 500                                   2                                       6020                                  linear       True              6020               sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                        \n ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n\n\n\n\n\nNote\n\n\n\ndvc exp show provides an overview of the experiments base on the git commit SHAs. The output can be counter intuitive. Check out the option ( dvc exp show --help), especially --num and -A to control where to search for experiments.\nEach experiment has a unique name, as we did not specify anything a random name is created. In our case sural-cyma (they are often fun but can be hard to infer meaning and speaking names are most the time more useful, see -n option).\nIt is also possible provide a commit message to give more details to the experiment.\n\n\nThe overview also allows us to see the score for our model and the parameters, this allows us to quickly compare models.\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the parameter from params.yaml are automatically added and our code somewhat duplicated them.\n\n\ndvclive relies on git to do its magic for files and the dvc cache for large files. How this works is that a reference inside git is created for each experiment and stores the changes there. We can see this in the above logger output or via git.\n1MLB-DATA$ git lg\n\n| * 1f2bee5 - (29 hours ago) dvc: commit experiment 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a - Peter Kandolf\n|/  \n* 519a854 - (29 hours ago) feat: include dvc-live - Peter Kandolf\n* f264314 - (2 days ago) feat: add data to dvc - Peter Kandolf\n* df38086 - (2 days ago) feat: init dvc - Peter Kandolf\n* 073fb93 - (2 days ago) fixup! feat: externalize params via omegaconfig - Peter Kandolf\n* 9f7dead - (2 days ago) feat: externalize params via omegaconfig - Peter Kandolf\n* 22788a6 - (2 days ago) init: project - Peter Kandolf\n\n1\n\nThis is a shorthand see6.\n\n\nFurthermore, we did not commit our changes to git (bad practice!!!) but they are stored alongside with the experiment, so no information is lost (we can find them in .git/refs , see below.)\nTo clean up, we commit our changes as we know they work and then rerun the code for a new experiment, see 519a854. We can remove the previous version with dvc exp rm &lt;name&gt;.\nBy default, this is not moved to the git remote, to do so we need to run dvc exp push &lt;git remote&gt;.\nMLB-DATA$ pdm run dvc exp push origin\n\nCollecting                                           |0.00 [00:00,    ?entry/s]\nPushing\nExperiment sural-cyma is up to date on Git remote 'origin'.\nWe can also see this in the git reflog (compare above output).\nMLB-DATA$ git reflog\n\n519a854 (HEAD -&gt; main) HEAD@{0}: dvc: Restore HEAD to 'main'\n1f2bee5 HEAD@{1}: commit: dvc: commit experiment 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n519a854 (HEAD -&gt; main) HEAD@{2}: checkout: moving from main to 519a8544c82667cec5356f92ddde77993f0a0e76\n519a854 (HEAD -&gt; main) HEAD@{3}: dvc: Restore HEAD to 'main'\n519a854 (HEAD -&gt; main) HEAD@{4}: checkout: moving from main to 519a8544c82667cec5356f92ddde77993f0a0e76\n519a854 (HEAD -&gt; main) HEAD@{5}: commit: feat: include dvc-live\nf264314 HEAD@{6}: dvc: Restore HEAD to 'main'\n9e83c73 HEAD@{7}: commit: dvc: commit experiment 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\nf264314 HEAD@{8}: checkout: moving from main to f264314e5442315c74c395a89c2c73a3b7269f90\nf264314 HEAD@{9}: dvc: Restore HEAD to 'main'\nf264314 HEAD@{10}: checkout: moving from main add dat to f264314e5442315c74c395a89c2c73a3b7269f90\nf264314 HEAD@{11}: dvc: Restore HEAD to 'main'\n4afc6af HEAD@{12}: commit: dvc: commit experiment 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\nf264314 HEAD@{13}: checkout: moving from main to f264314e5442315c74c395a89c2c73a3b7269f90\nf264314 HEAD@{14}: dvc: Restore HEAD to 'main'\nf264314 HEAD@{15}: checkout: moving from main to f264314e5442315c74c395a89c2c73a3b7269f90\nIn the file system we can look inside (via ls on Linux).\nMLB-DATA$ ls .git/refs/exps/51/9a8544c82667cec5356f92ddde77993f0a0e76/\n\nsural-cyma\nWe can also see, that a new file has appeared in our root directory, dvc.yaml with the following content.\nparams:\n- dvclive/params.yaml\nmetrics:\n- dvclive/metrics.json\nplots:\n- dvclive/plots/metrics:\n    x: step\n- dvclive/plots/sklearn/confusion_matrix.json:\n    template: confusion\n    x: actual\n    y: predicted\n    title: Confusion Matrix\n    x_label: True Label\n    y_label: Predicted Label\nartifacts:\n  model:\n    path: dvclive/artifacts/model.skops\n    type: model\nThe content reflects our call and integration with dvclive from the dvc perspective and is called a stage. Furthermore, the file dvclive/artifacts/model.skops.dvc keeps track of the model itself.\n\n\n\n\n\n\nImportant\n\n\n\nIn order to show the next feature we need to remove the dvc.yaml and the dvclive/artifacts/model.skops.dvc files again. As the current configuration would produce a conflict with the stage we want to introduce next, please delete these files if you type along.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code persistence</span>"
    ]
  },
  {
    "objectID": "data/code.html#dvc-pipeline",
    "href": "data/code.html#dvc-pipeline",
    "title": "5  Code persistence",
    "section": "5.4 dvc pipeline",
    "text": "5.4 dvc pipeline\nThe pipeline features we are after are part of the dvc stage command and we add our training call in the following fashion.\n1MLB-DATA$ pdm run dvc stage add --name train \\\n2          --deps data --deps src/MECH-M-DUAL-2-MLB-DATA/ --deps params.yaml \\\n3          --outs dvclive \\\n4          pdm run python src/MECH-M-DUAL-2-MLB-DATA/train.py\n\nAdded stage 'train' in 'dvc.yaml'                                     \n\nTo track the changes with git, run:\n\n        git add dvc.yaml dvclive/artifacts/.gitignore\n\n1\n\nDefine the name of the stage.\n\n2\n\nInclude the dependencies, dvc will keep track of these files and only reruns the code if there are any changes in these files/directories.\n\n3\n\nDefine the output directory to keep track of all the files.\n\n4\n\nCommand to run to execute the stage.\n\n\nBefore we follow the instructions for the git commit, we make a dvc commit.\nMLB-DATA$ pdm run dvc commit\nThis creates the dvc.lock file associated with the stage. It keeps track of the dependencies (see above explanations):\nschema: '2.0'\nstages:\n  train:\n    cmd: pdm run python src/MECH-M-DUAL-2-MLB-DATA/train.py\n    deps:\n    - path: data\n      hash: md5\n      md5: 5987e80830fc2caf6d475da3deca1dfe.dir\n      size: 111165\n      nfiles: 2\n    - path: params.yaml\n      hash: md5\n      md5: cb73b44317c559fce7c5e035ba5be854\n      size: 644\n    - path: src/MECH-M-DUAL-2-MLB-DATA/\n      hash: md5\n      md5: c93360b2cf461f6b2f8e9656882331a7.dir\n      size: 14538\n      nfiles: 8\n    outs:\n    - path: dvclive\n      hash: md5\n      md5: c2bbfd7cb23c3aa8700bc24287b56fee.dir\n      size: 5569537\n      nfiles: 6\nHave a look at 1282a7e9 to see how this is reflected in our reference project.\nNow we can put it to action and execute (all) stages and therefore create a new experiment.\nMLB-DATA$ pdm run dvc exp run\n\nReproducing experiment 'mesic-beep'  \nBuilding workspace index                            |16.0 [00:00, 1.23kentry/s]\nComparing indexes                                   |15.0 [00:00, 1.22kentry/s]\nApplying changes                                    |0.00 [00:00,     ?file/s]\n'data.dvc' didn't change, skipping                 \nStage 'train' didn't change, skipping\n                                 \nRan experiment(s): mesic-beep\nExperiment results have been applied to your workspace.\nAs we did no change anything in our configuration (see dependencies above), dvc is smart enough to basically just copy the experiment. But we can also change the parameters, either the file directly or interactively as seen in the next command block.\nMLB-DATA$ pdm run dvc exp run --set-param 'PCA.init_args.n_components=5'\n\nReproducing experiment 'sappy-corm'\n1Building workspace index                             |4.00 [00:00,  378entry/s]\nComparing indexes                                   |15.0 [00:00, 1.20kentry/s]\nApplying changes                                     |6.00 [00:00,   391file/s]\n'data.dvc' didn't change, skipping\nRunning stage 'train':\n2&gt; pdm run python src/MECH-M-DUAL-2-MLB-DATA/train.py\nDEBUG:root: Loaded the data with Split of 60 to 20 per category.\nDEBUG:root: Load config\nDEBUG:root: Create classifier\nDEBUG:root: Train classifier\nDEBUG:root: Score classifier\nINFO:root: We have a hard voting train-score of 0.9916666666666667\nINFO:root: We have a hard voting test-score of 0.6\nDEBUG:root: Save clf to skops file dvclive/artifacts/model.skops\nUpdating lock file 'dvc.lock'\n\nRan experiment(s): sappy-corm\nExperiment results have been applied to your workspace.\n\n1\n\ndvc checks the dependencies and applies the changes.\n\n2\n\nRun the command specified in the stage.\n\n\nThe result can be seen in the experiment list, of course under a new commit SHA.\n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  Experiment                 Created        trainscore   testscore   PCA/n_components   LinearDiscriminantAnalysis/solver   RandomForestClassifier/n_estimators   RandomForestClassifier/max_leaf_nodes   RandomForestClassifier/random_state   SVC/kernel   SVC/probability   SVC/random_state   PCA.type                    PCA.init_args.n_components   VotingClassifier.type               VotingClassifier.init_args.flatten_transform   VotingClassifier.estimators                                       LinearDiscriminantAnalysis.type                            LinearDiscriminantAnalysis.init_args.solver   RandomForestClassifier.type               RandomForestClassifier.init_args.n_estimators   RandomForestClassifier.init_args.max_leaf_nodes   RandomForestClassifier.init_args.random_state   SVC.type          SVC.init_args.kernel   SVC.init_args.probability   SVC.init_args.random_state   data                                   params.yaml                        src/MECH-M-DUAL-2-MLB-DATA            \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  workspace                  -                 0.99167         0.6   5                  svd                                 500                                   2                                       6020                                  linear       True              6020               sklearn.decomposition.PCA   5                            sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         5987e80830fc2caf6d475da3deca1dfe.dir   0ad678c7c338214916d88a106b4fe90a   c93360b2cf461f6b2f8e9656882331a7.dir  \n  main                       01:36 PM                -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         5987e80830fc2caf6d475da3deca1dfe.dir   cb73b44317c559fce7c5e035ba5be854   c93360b2cf461f6b2f8e9656882331a7.dir  \n  └── 9c326d3 [sappy-corm]   01:36 PM          0.99167         0.6   5                  svd                                 500                                   2                                       6020                                  linear       True              6020               sklearn.decomposition.PCA   5                            sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         5987e80830fc2caf6d475da3deca1dfe.dir   0ad678c7c338214916d88a106b4fe90a   c93360b2cf461f6b2f8e9656882331a7.dir  \n  519a854                    10:15 AM                -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         -                                      -                                  -                                     \n  ├── 1b92871 [mesic-beep]   01:26 PM                -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         5987e80830fc2caf6d475da3deca1dfe.dir   cb73b44317c559fce7c5e035ba5be854   c93360b2cf461f6b2f8e9656882331a7.dir  \n  └── 1f2bee5 [sural-cyma]   10:15 AM                1         0.8   41                 svd                                 500                                   2                                       6020                                  linear       True              6020               sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         -                                      -                                  -                                     \n  f264314                    Mar 19, 2025            -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         -                                      -                                  -                                     \n  df38086                    Mar 19, 2025            -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         -                                      -                                  -                                     \n  073fb93                    Mar 19, 2025            -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                         -                                      -                                  -                                     \n  9f7dead                    Mar 19, 2025            -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  -                           -                            -                                   -                                              -                                                                 -                                                          -                                             -                                         -                                               -                                                 -                                               -                 -                      -                           -                            -                                      -                                  -                                     \n  22788a6                    Mar 19, 2025            -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  -                           -                            -                                   -                                              -                                                                 -                                                          -                                             -                                         -                                               -                                                 -                                               -                 -                      -                           -                            -                                      -                                  -                                     \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \nIf we want to restore a specific experiment we can use dvc exp apply to restore it, note that this will also restore the code and the data.\nIn order to share experiments we need to push them to the remote, git and dvc. This is done via\nMLB-DATA$ pdm run dvc exp push origin -A\n\nExperiment sural-cyma is up to date on Git remote 'origin'.\nPushed experiment sappy-corm and mesic-beep to Git remote 'origin'.\n\n\n\n\n\n\nWarning: Be careful with changed git SHAs\n\n\n\nAs we can see dvc attaches experiments to git SHAs. This is an excellent idea as they identify a code state uniquely. Nevertheless, this can backfire.\nSome commands and actions can change a git SHA. This includes squash merges, or a rebase. Be careful when using such actions in your dvc repository together with experiment storage.\n\n\n\n\n\n\n\n\n\nExercise 5.3 (dvc queue) Checkout the option --queue for dvc exp run. Use it to plan various experiments with different parameters from the params.yaml file.\nRun them via dvc queue start.\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.4 (dvc visualization) dvc can help us visualize our experiments. Have a look at dvc plots to visualize the experiments and there differences.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code persistence</span>"
    ]
  },
  {
    "objectID": "data/code.html#add-a-remote-for-dvc",
    "href": "data/code.html#add-a-remote-for-dvc",
    "title": "5  Code persistence",
    "section": "5.5 Add a remote for dvc",
    "text": "5.5 Add a remote for dvc\nThe biggest thing missing from our example project with dvc is a remote to store our data and make it available for cooperation.\n\n\n\n\n\n\nImportant\n\n\n\nFor this lecture we use the storage on Sakai, this also means that this part is tricky to follow if you are not part of the lecture.\nIf a storage that can be accessed via WebDAV7 is available to you use it. Alternatively, use local storage, see File systems (local remotes).\n\n\nWe can add the remote with the dvc remote add command.\nMLB-DATA$ pdm run dvc remote add -d myremote webdavs://sakai.mci4me.at/dav/Course-ID-SLVA-46549/MECH-M-DUAL-2-MLB-DATA\n\nSetting 'myremote' as a default remote.\nThis create the file .dvc/config with the following content (a toml file).\n[core]\n    remote = myremote\n['remote \"myremote\"']\n    url = webdavs://sakai.mci4me.at/dav/Course-ID-SLVA-46549/MECH-M-DUAL-2-MLB-DATA\nTo nicely separate the sensitive information for the access, there also exist a .dvc/config.local file that is in the .gitignore and will not be committed. We add our user information to this file or via a command in the terminal\nMLB-DATA$ pdm run dvc remote modify --local myremote user ***\n\nMLB-DATA$ pdm run dvc remote modify --local myremote password ***\n(the user is without the @mci4me.at)\nTo handle WebDAV dvc requires the package dvc-webdav, install it via pdm. See afe0848 on how this is reflected in our reference project.\nNow we can run dvc push and our data is stored remotely,\nMLB-DATA$ pdm run dvc push\n\nCollecting                                           |14.0 [00:00,  381entry/s]\nPushing                                   \n10 files pushed\nAnd of course, dvc pull to get the files on another computer.\n\n\n\n\n\n\n\nExercise 5.5 (dvc remote)  \n\nSynch the data from the remote described here.\nOn the course sakai cloud your have write rights in the subfolder students_remote, create a folder with your name and use it as remote.\n\nSynchronize the performed experiments with the remote (dvc and git).\n\nUse a local directory as remote and see if you can make sense of the content, i.e. how is it structured.\n\n\n\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code persistence</span>"
    ]
  },
  {
    "objectID": "data/code.html#footnotes",
    "href": "data/code.html#footnotes",
    "title": "5  Code persistence",
    "section": "",
    "text": "see the lecture MECH-M-DUAL-1-SWD, Chapter 3 or follow the direct link↩︎\nsee the lecture MECH-M-DUAL-1-SWD, Chapter 2 or follow the direct link↩︎\nsee the lecture MECH-M-DUAL-1-SWD, Chapter 11 or follow the direct link↩︎\nsee the Python documentation or follow the direct link↩︎\nsee the lecture MECH-M-DUAL-1-SWD, Chapter 3 or follow the direct link↩︎\nThe long version uses some options for git log precisely git log --graph --abbrev-commit --decorate --format=format:'%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)' --all↩︎\nSee Wiki for a quick reference, accessed on 21.03.2025↩︎",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code persistence</span>"
    ]
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "6  Data persistence",
    "section": "",
    "text": "In our example the data is downloaded from a remote and added to dvc. This already covers the basic data persistence problems as we described it. We can always make sure that the data content is verified and we know the state. If we run an experiment we see it in the output:\nApplying changes                                     |0.00 [00:00,     ?file/s]\n'data.dvc' didn't change, skipping     \nIf we change the data we know that we need to update the training as well. Nevertheless, we want to highlight some different aspects in this context.\nIt is quite common, if not always the case, that the model we want to train is not using the raw data but rather processed data. On common pattern to get from the raw data to the processed data is called ETL (extract, transform, load).\nIn Figure 6.1 we can see an illustration of the pattern and Reis (2022) covers this concept as well.\n\n\n\n\n\n\nFigure 6.1: Illustration of an ETL pattern for data processing\n\n\n\nThe main idea is that in order to create a report, train a model, or any other ML tasks the raw data (illustrated via the Data sources) needs some processing. Maybe some sources are combined, cumulative data is computed, images are cropped and transformed, converted into a different format etc.. At the end of the process the new data is stored again. This is often called a Data warehouse in the context of data science. From this storage our tasks can load the data again for direct processing.\nSeparating these tasks has some advantages.\n\nThe processes can run asynchronous.\nA state of the processed data can be frozen for processing.\nThere is a well defined and reproducible way to come from the raw data to our input data (the code is under version control).\nUnstructured data can be transformed into structured data that is easier to process.\nIf a format changes this can be incorporated into the ETL to allow backwards or forwards compatibility.\nDepending on our use-case we can extend this list.\n\nWe illustrate this task in the context of our example project. This means, instead of directly using catData_w.mat we start of from the raw data catData.mat, see b097ba6 for the implementation details.\nWe introduce a new module called etl with three files plus the module file to accomplish this task.\netl\n├── extract.py\n├── __init__.py\n├── load.py\n└── transform.py\n\n1 directory, 4 files\nNow a simple call to etl.load(\"catData_w.mat\") will run the following chain:\n\nIf the file does not exist:\n\nextract:\n\naccess catData.mat and extract the content as a np.array\n\ntransform:\n\ntransform the image in the same fashion as described in Section A.1\nstore the data in the .mat format in catData_w.mat\n\n\nload:\n\nreturn the content as a `np.array\n\n\nWe try to only store the processed data and not the raw data itself, as this would be a duplication. In this case, we therefore do not store catData.mat locally but just the final result. Depending on your ETL you might build up a buffer locally to optimize processing time over access time.\n\n\n\n\n\n\nStorage\n\n\n\nWhile for image processing, computational resources are often called out as the bottleneck, the influence of storage and the correct structure of data should not be ignored.\nModern GPU architectures are designed for high data throughput. This comes with the drawback that we need to provide the data as quickly as it gets processed. Consequently, a high throughput for storage is required or the performance is not optimal.\nNote: as dvc uses a cache and links the data, this means the performance of the cache location is important.\n\n\n\n\n\n\n\n\n\nExercise 6.1 (Extension of the ETL) To get more into image processing extend the ETL with the following steps:\n\nFrom the original .mat files extract the single images and store them in an appropriate format - consider compression loss - separate for the two classes.\nSplit them up into a test and a training set. The structure should look like the following tree.\nMLB-DATA$ tree data\n\ndata\n └── raw\n     ├── cat\n     │   ├── test\n     │   │   └── cat60\n     │   └── train\n     │       └── cat0\n     └── dog\n         ├── test\n         │   └── dog60\n         └── train\n             └── dog0\n\n 8 directories, 4 files\nExtend the transformation capabilities to allow an image as input.\nExtend the store capabilities to store the transformed images in an appropriate image format.\nExtend the load capabilities to only specify a folder (in our case data/raw) and the classes as well as the test and training set a generated automatically from these files.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.2 (Optional: Changed accuracy) Optional: If we rerun our training, we can see that the results change slightly. Find out what has changed.\nNote: This is not optimal but the upside is we control the ETL so we can actually make sure that a new image is processed in the same fashion and we do not need to ask the authors of Brunton and Kutz (2022) to help us out.\n\n\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nReis, Joe. 2022. Fundamentals of Data Engineering. Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "Data Management and Data Engineering",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data persistence</span>"
    ]
  },
  {
    "objectID": "nn/index.html",
    "href": "nn/index.html",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "Let us try to demystify the mathematics behind NN\nNeural Networks (NNs) are inspired by the work of Hubel and Wiesel (1962) about the visual cortex of cats. They showed how hierarchical layers of cells process visual stimuli. This insight was the base for the first mathematical model of a NN, published in 1980 (Fukushima (1980)), that would in the modern terminology be best described as a deep convolutional neural network (DCNN). It contains a multi-layer structure, convolution, max pooling, and nonlinear dynamical nodes.\nThe current high interest in NNs was arguably kickstarted with Krizhevsky, Sutskever, and Hinton (2012) and their introduction of ImageNet. The used with high resolution images, facilitated with 15 million labels in over 22 000 categories. This transformed the field of computer vision tasks for classification and identification. All of this was made possible by the large labelled dataset and the available compute power1.\nAt its core a neural network consists of multiple layers (some with specific names) of interconnected nodes (or neurons).\nSimilar to our view of the brain, the neurons process information and make decisions based on this information and their state (weights, biases, activation functions, etc.). Given enough data, computing power, the correct architecture, and hyperparameters they are universal functions approximation machines. They can be used to learn about complex relationships between the input and the output without requiring any rules or programming. They are ideal for pattern recognition, data classification, and regression analysis.\nFollowing the discussion of (Brunton and Kutz 2022, chap. 6), mathematically speaking the main task in NNs lies in optimization. Specifically, NNs optimize over a composite function \\[\n\\underset{A_j}{\\operatorname{arg min}}\\, \\big( f_m(A_m, f_{m-1}(A_{m-1}, \\cdots, f_2(A_2, f_1(A_1, x)))) + \\lambda g(A_j) \\big)\n\\]\nwhere \\(x\\) denotes the input, the matrix \\(A_k\\) denotes the weights connecting layer \\(k\\) with layer \\(k+1\\), \\(\\lambda\\) and \\(g\\) are regularizations (bias) and \\(f_k\\) is the activation function \\(k\\).\nIn the context of deep learning models this is often written in the compact and generic form \\[\n\\underset{\\Theta}{\\operatorname{arg min}}\\, f_\\Theta(x)\n\\] for \\(\\Theta\\) denoting the weights of the network and \\(f\\) the characteristics of the network (layers, structure, activation functions, etc.).\nThe training of the network uses the labelled data to find the best weights such that the labels are recovered best from the observations (input), i.e. map \\(x_j\\) to \\(y_j\\). This is often done via backpropagation algorithms and stochastic gradient descent.\nThe main building block is a so called perceptron (a single neutron, see Figure 2) that takes its set of inputs, multiplies each by a weight, sums them up, adds a bias term, and applies a function to produce an output.\nFor multiple perceptions in one layer we get weights for each of them and they are combined into the matrix \\(A\\), see Figure 3.\nIf we extend again to a multilayer NN with multiple neurons in each layer we can simplify this a bit by limiting us for a moment to a linear NN without a bias. In this case the functions \\(f_k\\) are all linear maps (as we will see shortly, it is in fact the identity) and the composition is the matrix-matrix computation \\[\ny = A_m A_{m-1} \\cdots A_2 A_1 x.\n\\]\nThe matrices might be sparse or dense, depending on the connections they describe. The resulting system is most likely under-determined and requires some constraints to select a unique solution, see Kandolf (2025).\nTo make it more hands on, let us considering a one layer NN for our cats and dogs example with the structure illustrated in Figure 4.\nMathematically, this becomes \\[\ny = A x.\n\\] and to connects back to the perceptron as \\(A = w^\\mathsf{T}\\).\nOne possible solution is via the Moore-Penrose pseudo inverse2 \\(A = Y X^\\dagger\\), where all our training data is combined into \\(X\\) and \\(Y\\) are the corresponding labels. This in return, tells us that a single layer linear NN can be used to build a least-square fit.\nOf course there are other possibilities to solve this system (as discussed in Kandolf (2025)), e.g. the LASSO method or RIDGE can be used to promote certain properties on the matrix \\(A\\).\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\ndef myplot(y):\n    plt.figure()\n    len = y.shape[0]\n    plt.bar(range(len), y)\n    plt.plot([-0.5, len - 0.5], [0, 0], \"k\", linewidth=1.0)\n    plt.plot([len // 2 - 0.5, y.shape[0] // 2 - 0.5], [-1.1, 1.1], \"r-.\", linewidth=3)\n    plt.yticks([-0.5, 0.5], [\"cats\", \"dogs\"], rotation=90, va=\"center\")\n    plt.text(len // 4, 1.05, \"dogs\")\n    plt.text(len // 4 * 3, 1.05, \"cats\")\n    plt.gca().set_aspect(len / (2 * 3))\n\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\ns=40\nX_train = np.concatenate((dogs_w[:, :s], cats_w[:, :s]), axis=1).T\ny_train = np.repeat(np.array([1, -1]), s)\nX_test = np.concatenate((dogs_w[:, s:], cats_w[:, s:]), axis=1).T\ny_test = np.repeat(np.array([1, -1]), 80-s)\n\nA = y_train @ np.linalg.pinv(X_train.T)\ny_test_pinv = np.sign(A @ X_test.T)\nmyplot(y_test_pinv)\n\n# Same result as the above computation via the scikit-learn functions.\nlsq = linear_model.LinearRegression(fit_intercept=False).fit(X_train, y_train)\ny_test_lsq = np.sign(lsq.predict(X_test))\nnp.testing.assert_allclose(A, lsq.coef_)\n#myplot(y_test_lsq)\n\nridge = linear_model.Ridge(random_state=6020).fit(X_train, y_train)\ny_test_ridge = np.sign(ridge.predict(X_test))\nmyplot(y_test_ridge)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Pseudo inverse implemented by hand.\n\n\n\n\n\n\n\n\n\n\n\n(b) RIDGE solution via scikit-learn.\n\n\n\n\n\n\nFigure 5: Linear single layer neural network for different solutions of the resulting system.\nNote that we compressed via the matrix \\(A\\) (actually it is just a vector) our input of 1024 pixels to 1 value. Depending on the method we can promote sparsity (not all inputs are used for the output) or other properties.\nNevertheless, we can only cover a limited range of possibilities with linear functions. Therefore, the extension to arbitrary activation functions is a natural progression.",
    "crumbs": [
      "Neural Networks and Deep Learning"
    ]
  },
  {
    "objectID": "nn/index.html#let-us-try-to-demystify-the-mathematics-behind-nn",
    "href": "nn/index.html#let-us-try-to-demystify-the-mathematics-behind-nn",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "Figure 2: Basic building block perceptron with its input, weights, bias, and activation function.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA perceptron is the equivalent of the scikit-learn classifier SGDClassifier with loss=\"perceptron\".\n\n\n\n\n\n\n\n\n\nFigure 3: First weight matrix for a generic NN, where no connection is present a 0 is inserted.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: A single layer structure for the cats vs. dogs classification.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nFukushima, Kunihiko. 1980. “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.” Biological Cybernetics 36 (4): 193–202. https://doi.org/10.1007/BF00344251.\n\n\nHubel, D., and T. Wiesel. 1962. “Receptive Fields, Binocular Interaction, and Functional Architecture in the Cat’s Visual Cortex.” Journal of Physiology 160: 106–54.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.",
    "crumbs": [
      "Neural Networks and Deep Learning"
    ]
  },
  {
    "objectID": "nn/index.html#footnotes",
    "href": "nn/index.html#footnotes",
    "title": "Neural Networks and Deep Learning",
    "section": "",
    "text": "As a reference: The IBM Personal Computer (PC) was introduced in 1981, after the first NN.↩︎\nSee (Kandolf 2025, Definition 4.3) or access it directly via the Linkf↩︎",
    "crumbs": [
      "Neural Networks and Deep Learning"
    ]
  },
  {
    "objectID": "nn/nn.html",
    "href": "nn/nn.html",
    "title": "7  Neural Networks",
    "section": "",
    "text": "7.1 The trainable parameters of a Neural Network\nNow that we constructed one of the simplest NNs possible (if we can even call it a NN), we build on this and extend our model. The first step is to introduce some (non-linear) activation functions or transfer functions \\[\ny = f(A, b, x).\n\\]\nSome common functions are:\nWe can visualize them and their derivatives (we need them later).\nBefore we build our first NN in the next section, we need to get a better understanding on the dimensions and available parameters for our optimization involved in training a NN. As discussed, these parameters are the matrices combining the weights and the biases. Of course they are related to the neurons.\nIn Figure 7.2 we also used the formulation of hidden layers for all the layers inside the NN. This is a common formulation reflecting the nature of the NN. Their activations (transfer from their specific input to output \\(f_j(A_j, b_j, x^{(j-1)})\\)) are not exposed to the user and can’t be observed by the user directly.\nNow let us build our first NN.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/nn.html#the-trainable-parameters-of-a-neural-network",
    "href": "nn/nn.html#the-trainable-parameters-of-a-neural-network",
    "title": "7  Neural Networks",
    "section": "",
    "text": "Figure 7.2: Generic linear NN with weights and biases.\n\n\n\n\n\n\n\n\n\n\nExercise 7.1 (Compute the parameters of the Neural Network) For the NN display in Figure 7.2 answer the following questions:\n\nWhat are the shapes of the matrices \\(A_1, A_2, A_3, A_4\\)?\nWhich of these matrices are sparse (have multiple zeros)?\nWhat are the shapes of the biases \\(b_1, b_2, b_3, b_4\\)?\nWrite down the (expanded) formula for the output \\(y\\) with respect to the input \\(x\\) resulting from the composition of the different layers for \\(f(x) = x\\) in each layer (as seen in Figure 7.2).\nIs this formulation a good option to compute \\(y\\) from \\(x\\) (provide some reasoning)?",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/nn.html#sec-nn-nn-firstnn",
    "href": "nn/nn.html#sec-nn-nn-firstnn",
    "title": "7  Neural Networks",
    "section": "7.2 A Neural Network with pytorch",
    "text": "7.2 A Neural Network with pytorch\n\n\n\n\n\n\nImportant\n\n\n\nThere are multiple frameworks available for NNs in Python. We will focus on pytorch for these notes.\nNevertheless, see Appendix B for an implementation of the same NN with an alternative framework.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are a couple of steps required and some foreshadow later decisions. We tried to make the split to provide better understanding and an easy way to follow the material.\n\n\nLet us start with the input and how to prepare our data for for the model to-be. Before we load everything into the pytorch specific structures, we split our data into a training and test set. This time it is important that we rescale scale our image to \\([0, 1]\\), otherwise the optimization algorithms perform poorly. We scale by \\(255\\), our theoretical maximum in a generic image.\n\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import TensorDataset\n%config InlineBackend.figure_formats = [\"svg\"]\n# Initialize everything for reproducibility\ntorch.manual_seed(6020)\nnp.random.seed(6020)\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData_w.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat_wave\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData_w.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog_wave\"]\n\ns = 40\nX_train = np.concatenate((dogs_w[:, :s], cats_w[:, :s]), axis=1).T / 255.0\ny_train = np.repeat(np.array([0, 1]), s)\nX_test = np.concatenate((dogs_w[:, s:], cats_w[:, s:]), axis=1).T / 255.0\ny_test = np.repeat(np.array([0, 1]), 80 - s)\n\n1X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.uint8)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.uint8)\n2dataset = TensorDataset(X_train_tensor, y_train_tensor)\n\n\n1\n\nWe need to convert the training data to PyTorch tensors unfortionalty, this might result in larger data if we are not careful, uint8 works for the loss computation we aim for (see later).\n\n2\n\nCombine \\(X\\) and \\(Y\\) such that it can easily be used for a DataLoader.\n\n\n\n\nThe easiest way to provide data for training is to use the mentioned DataLoader class. As we have only a very limited number of training data, we split our available data into a training and validation. We do so new and random in each epoch (optimization step). To provide a better overview, we use a dedicated function for this procedure. This function will be called during each optimization step in our training.\n\nfrom torch.utils.data import DataLoader, random_split\n\n\ndef get_dataset(dataset: TensorDataset, val_split: float, batch_size: int):\n    val_size = int(val_split * len(dataset))\n    train_size = len(dataset) - val_size\n    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n\n    # Create a dataset\n    train = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n    return train, val\n\nNow that we have taken care of our input, we need to discuss the output of our model. Instead of having a single neuron that is zero or one, we will have two neurons, i.e. \\(y\\in\\mathbb{R^2}\\) using a so called one-hot encoding for our two classes. \\[\ny = \\left[\\begin{array}{c}1 \\\\ 0\\end{array}\\right] = \\text{dog},\n\\quad\\text{and}\\quad\ny = \\left[\\begin{array}{c}0 \\\\ 1\\end{array}\\right] = \\text{cat}.\n\\]\nThe idea is, that the model will provide us with the probability, that an image shows a dog on \\(y_1\\) or a cat on \\(y_2\\) (the sum of both will be \\(1\\)).\nNext, we define our activation function in the first layer by the non-linear function \\(f_1 = \\tanh\\). The idea is to use a more complicated function the reflect the nature of our images. The rest remains unchanged, with the weights aggregated to matrix \\(A_1\\) and the bias to the vector \\(b_1\\). In a the next layer we have a linear transform to provide some more freedom and than use the softmax function \\(\\sigma\\) (see Section A.3 for a more detailed explanation and example) to translate the result into a probability \\[\n\\sigma: \\mathbb{R^n} \\to (0, 1)^n, \\quad \\sigma(x)_i = \\frac{\\exp(x_i)}{\\sum_j{\\exp(x_j)}}.\n\\]\nWe visualized the resulting network in Figure 7.3.\n\n\n\n\n\n\nFigure 7.3: A two layer structure for the cats vs. dogs classification with a non-linear model.\n\n\n\nThe following couple of code sections translate Figure 7.3 into a pytorch model. The main idea is that we create a model class that inherits from torch.nn.Module and than perform our training on this class, benefiting from the inherited capabilities.\n\n\n\n\n\n\nTip\n\n\n\nAs we will see in Exercise 7.9, softmax and our loss function can be combined in a numerical stable way. Therefore, it is often advised not to include softmax during training but for inference.\nFor the following code snippet, we keep it in as a comment to show where it would be included.\n\n\n\nimport torch\n\n\n1class MyFirstNN(torch.nn.Module):\n2    def __init__(self, input_params):\n3        super(MyFirstNN, self).__init__()\n4        self.model = torch.nn.Sequential(\n            torch.nn.Linear(input_params, 2),           \n            torch.nn.Tanh(),                            \n            torch.nn.Linear(2, 2),                      \n            #torch.nn.Softmax(dim=1),                    \n        )\n\n5    def forward(self, x):\n        y = self.model(x)\n        return y\n\n\n1\n\nCreate a class inheriting from torch.nn.Module.\n\n2\n\nThe layers of the NN are defined in the initialization of the class.\n\n3\n\nDo not forget to initialize the super class.\n\n4\n\nWe define a sequential model, this reflects best our desired structure, the first layer that reduces down to two neurons and applies the function \\(\\tanh\\), and a second for softmax.\n\n5\n\nIn forward we define how data moves through the network.\n\n\n\n\nOf course it is important to check if the code corresponds to the model we have in mind. For this torchinfo is quite a useful tool. It provides a tabular overview.\n\nimport torchinfo\n\nmodel = MyFirstNN(X_train.shape[1])\nbatch_size = 8\ninfo = torchinfo.summary(model, (batch_size, X_train.shape[1]), col_width=12)\n# Nicer formatting for the notes\ninfo.layer_name_width=15\nprint(info)\n\n================================================================\nLayer (type:depth-idx)                   Output Shape Param #\n================================================================\nMyFirstNN                                [8, 2]       --\n├─Sequential: 1-1                        [8, 2]       --\n│    └─Linear: 2-1                       [8, 2]       2,050\n│    └─Tanh: 2-2                         [8, 2]       --\n│    └─Linear: 2-3                       [8, 2]       6\n================================================================\nTotal params: 2,056\nTrainable params: 2,056\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.02\n================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.01\nEstimated Total Size (MB): 0.04\n================================================================\n\n\n\n\n\n\n\n\n\nExercise 7.2 (Explain the output of .summary()) For the (extended) output of .summary() in the following listing answer the questions below.\n\n\nShow the code for the slightly extended summary.\nimport torchinfo\n\nmodel = MyFirstNN(X_train.shape[1])\nbatch_size = 8\ninfo = torchinfo.summary(model, (batch_size, X_train.shape[1]),\n                  col_names=[\"input_size\", \"output_size\", \"num_params\"],\n                  col_width=12)\ninfo.layer_name_width=15\nprint(info)\n\nprint(f\"\\n\\nThe estimated total size in KB {info.total_param_bytes / 1024}\")\n\n\n============================================================================\nLayer (type:depth-idx)                   Input Shape  Output Shape Param #\n============================================================================\nMyFirstNN                                [8, 1024]    [8, 2]       --\n├─Sequential: 1-1                        [8, 1024]    [8, 2]       --\n│    └─Linear: 2-1                       [8, 1024]    [8, 2]       2,050\n│    └─Tanh: 2-2                         [8, 2]       [8, 2]       --\n│    └─Linear: 2-3                       [8, 2]       [8, 2]       6\n============================================================================\nTotal params: 2,056\nTrainable params: 2,056\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.02\n============================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.01\nEstimated Total Size (MB): 0.04\n============================================================================\n\n\nThe estimated total size in KB 8.03125\n\n\n\nExplain how the Param # column is computed and retrace the details.\nIf we expand the Params size to (KB) we get 8.03, what does this imply on the data type of the parameters?\n\n\n\n\n\nIt is often also quite useful to export the model as an image. Here the torchviz package comes in handy. In our case we can work again with a dot file, that is rendered as Figure 7.4.\n\nimport torchviz\nmodel.eval()\n\nx_viz = X_train_tensor[0, :].reshape([1, -1])\ny_viz = model(x_viz)\ndot = torchviz.make_dot(y_viz, params=dict(model.named_parameters()))\ndot.save(\"model.dot\")\n\n\n\n\n\n\n\n\n\n\n\n\n140149512019216\n\n (1, 2)\n\n\n\n140153164565424\n\nAddmmBackward0\n\n\n\n140153164565424-&gt;140149512019216\n\n\n\n\n\n140149513084000\n\nAccumulateGrad\n\n\n\n140149513084000-&gt;140153164565424\n\n\n\n\n\n140149513333616\n\nmodel.2.bias\n (2)\n\n\n\n140149513333616-&gt;140149513084000\n\n\n\n\n\n140149513625920\n\nTanhBackward0\n\n\n\n140149513625920-&gt;140153164565424\n\n\n\n\n\n140149494100800\n\nAddmmBackward0\n\n\n\n140149494100800-&gt;140149513625920\n\n\n\n\n\n140149494090048\n\nAccumulateGrad\n\n\n\n140149494090048-&gt;140149494100800\n\n\n\n\n\n140149513333296\n\nmodel.0.bias\n (2)\n\n\n\n140149513333296-&gt;140149494090048\n\n\n\n\n\n140149494090432\n\nTBackward0\n\n\n\n140149494090432-&gt;140149494100800\n\n\n\n\n\n140149494100848\n\nAccumulateGrad\n\n\n\n140149494100848-&gt;140149494090432\n\n\n\n\n\n140149513333376\n\nmodel.0.weight\n (2, 1024)\n\n\n\n140149513333376-&gt;140149494100848\n\n\n\n\n\n140149519294688\n\nTBackward0\n\n\n\n140149519294688-&gt;140153164565424\n\n\n\n\n\n140149494090192\n\nAccumulateGrad\n\n\n\n140149494090192-&gt;140149519294688\n\n\n\n\n\n140149513333536\n\nmodel.2.weight\n (2, 2)\n\n\n\n140149513333536-&gt;140149494090192\n\n\n\n\n\n\n\n\nFigure 7.4: Model with all its parameters visualized.\n\n\n\n\n\nIn pytorch we are responsible for our training function. This allows for a lot of flexibility. The structure is always quite similar.\n\ndef train_model(model, dataset, loss_fn, optimizer,\n                epochs=250, validation_split=0.1, \n                batch_size=8):\n\n1    met = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"train_acc\": [],\n        \"val_acc\": []}\n\n2    for epoch in range(epochs):\n3        model.train()\n        train_loss, train_corr = 0, 0\n        train_dl, val_dl = get_dataset(dataset, validation_split, batch_size)\n\n4        for X_batch, y_batch in train_dl:\n            y_pred = model(X_batch)            # Forward pass through the model\n            loss = loss_fn(y_pred, y_batch)    # Compute the loss\n            loss.backward()                    # Backpropagation\n            optimizer.step()                   # Update the model parameters\n            optimizer.zero_grad()              # Reset the gradients\n\n            train_loss += loss.item()\n            train_corr += (y_pred.argmax(1) == y_batch).sum().item()\n\n5        model.eval()\n        val_loss, val_corr = 0, 0                                       \n        with torch.no_grad():                   # No gradient calculation\n6            for X_val, y_val in val_dl:\n                y_val_pred = model(X_val)\n                val_loss += loss_fn(y_val_pred, y_val).item()\n                val_corr += (y_val_pred.argmax(1) == y_val).sum().item()\n\n7        met[\"train_loss\"].append(train_loss / len(train_dl))\n        met[\"val_loss\"].append(val_loss / len(val_dl))\n        met[\"train_acc\"].append(train_corr / len(train_dl.dataset))\n        met[\"val_acc\"].append(val_corr / len(val_dl.dataset))\n\n    return met\n\n\n1\n\nCreate a dict for our metrics.\n\n2\n\nThe training loop for each epoch.\n\n3\n\nThe model needs to be in the train mode, otherwise the parameters can not be changed.\n\n4\n\nThe actual training, the data is processed in batches and the optimization is computed for each batch.\n\n5\n\nTo allow validation, the model needs to be in eval mode.\n\n6\n\nValidation step, that computes the necessary metrics for our validation set.\n\n7\n\nKeep track of the metrics of our training. We log the average loss and the accuracy.\n\n\n\n\nNow all parts are available and we can finally train our model.\n\n1loss_fn = torch.nn.CrossEntropyLoss()\n2optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n\nepochs = 120\n3history = train_model(model, dataset, loss_fn, optimizer,\n                      epochs=epochs,\n                      validation_split=0.1,\n                      batch_size=batch_size)\n\n\n1\n\nDefine the loss function.\n\n2\n\nDefine the optimizer.\n\n3\n\nCall the training loop.\n\n\n\n\nThis looks like a lot, but actually it is boiler plate code if needed and allows for quite a lot of flexibility if required.\nNow that training is complete we can have a look at the performance.\n\n1model.eval()\nwith torch.no_grad():\n2    y_proba = torch.nn.Softmax(dim=1)(model(X_test_tensor))\n3y_predict = y_proba.argmax(axis=-1)\n4acc = (y_predict == y_test_tensor).sum().item() / len(y_test)\n\n\n1\n\nSwitch to evaluation model.\n\n2\n\nApply softmax to the output of the model.\n\n3\n\nConvert the probability into a class.\n\n4\n\nCompute the accuracy for the test images.\n\n\n\n\nWe can also visualize the findings for a better understanding.\n\n\nShow the code for the figure\ndef myplot(y):\n    plt.figure()\n    n = y.shape[0]\n    plt.bar(range(n), y)\n    plt.plot([-0.5, n - 0.5], [0, 0], \"k\", linewidth=1.0)\n    plt.plot([n // 2 - 0.5, y.shape[0] // 2 - 0.5], [-1.1, 1.1], \"r-.\", linewidth=3)\n    plt.yticks([-0.5, 0.5], [\"cats\", \"dogs\"], rotation=90, va=\"center\")\n    plt.text(n // 4, 1.05, \"dogs\")\n    plt.text(n // 4 * 3, 1.05, \"cats\")\n    plt.gca().set_aspect(n / (2 * 3))\n\n\nmyplot(y_predict * (-2) + 1)\n\nn = y_proba.shape[0]\nplt.figure()\nplt.bar(range(n), y_proba[:, 0], color='y', label=r\"p(dog)\")\nplt.bar(range(n), y_proba[:, 1], color='b', label=r\"p(cat)\", \n        bottom=y_proba[:, 0])\nplt.plot([-0.5, n - 0.5], [0.5, 0.5], \"r\", linewidth=1.0)\nplt.gca().set_aspect(n / (1 * 3))\nplt.legend()\n\nplt.figure()\n\nepochs_range = range(len(history[\"train_acc\"]))\nplt_list = [[\"train_acc\", \"-\"], [\"train_loss\", \":\"],\n            [\"val_acc\", \"--\"], [\"val_loss\", \"-.\"]]\n\nfor name, style in plt_list:\n    plt.plot(epochs_range, history[name], style, label=name)\nplt.legend(loc=\"lower right\")\nplt.gca().set_aspect(epochs / (1 * 3))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Classification for our test set.\n\n\n\n\n\n\n\n\n\n\n\n(b) Probabilities of the two classes - our actual output of the model.\n\n\n\n\n\n\n\n\n\n\n\n(c) Summary of the key metrics of the model training.\n\n\n\n\n\n\nFigure 7.5: Performance of our model.\n\n\n\n\nIn Figure 7.5 (a) we can see the final classification of our model with regards to the test set. For 11 dogs our model is convinced they are not good dogs but cats, and 3 cats are classified as dogs. If we look at the probabilities Figure 7.5 (b), we can see that we have a couple of close calls, but in general our model is quite sure about the classification. Regarding the history of our optimization, we can see three phases, first our accuracy stays constant, right about for the first 80 iterations. Than the network starts learning up to 120 and after that only the loss function declines, but accuracy stays high.\nAt the end, we have an accuracy of 82.5% for our test set, a bit better than with our linear models Figure 7.1.\n\n\n\n\n\n\n\nExercise 7.3 (Learning rate and momentum) The used optimizer SGD has two options we want to investigate further. The learning rate lr and the momentum.\n\nChange the learning rate and see how this influences performance, you might want to increase epochs as well. Try \\(lr \\in [10^{-1}, 10^{-4}]\\).\nChange the momentum between \\(0\\) and \\(1\\), can this improve the predictions for different learning rates and such that the NN is more sure of its decision (in Figure 7.5 (b) the bars are not almost equal but lean to one side).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.4 (Change the optimizer) As we have seen so often before, the optimizer used for the problem can change how fast convergence is reached (if at all).\n\nHave a look at the possibilites provided in the framework Overview - Optimizers - they are basically all improvements on Stochastic Gradient Descent.\nTest different approaches, especially one of the Adam (Adam, AdamW, Adamax) and Ada (Adadelta, Adafactor, Adagrad, SparseAdam) implementations and record the performance (final accuracy).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.5 (Train and validation split) In the above version of the train loop we split our dataset in each epoch. Change this such that the split is done once per call of the training.\n\nWhat is the influence on the training?\nHow is the performance for different optimizers?\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.6 (Train with softmax) Include softmax into to model for training and see how the performance is influenced.\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.7 (Optional: PyTorch Lightning) The module pytorch lightning1 promises to streamline the process for the training and to reduce the code.\nWith the Lightning in 15 minutes (or any other tutorial) rewrite your code to use this framework.\nNote: This will make the dvclive integration required below slightly easier.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/nn.html#how-to-save-a-pytorch-model",
    "href": "nn/nn.html#how-to-save-a-pytorch-model",
    "title": "7  Neural Networks",
    "section": "7.3 How to save a pytorch model",
    "text": "7.3 How to save a pytorch model\nOf course we can save our pytorch model with the methods discussed in Chapter 4 but it is more convenient to use the dedicated functions, see docs.\nIn short, we only need to call torch.save(model.state_dict(), file) to save in the default format using pickle, be careful when using it, see Section 4.2.\ntorch.save(\"model.pt\")\nloaded_model = model = MyFirstNN(X_train.shape[1])\nloaded_model.load_state_dict(torch.load(\"model.pt\", weights_only=True))\nIt is also possible to implement continuous checkpoints during training, see docs. This allows us to resume training after an interrupt or simply store the model at the end of the training.\nAll of this can be done via the already discussed dvclive interface, the implementation is defined as an exercise (if your module uses lightning this is even easier - see Exercise 7.7).\n\n\n\n\n\n\n\nExercise 7.8 (dvclive integration into the pytorch training) Implement the DVCLive integration to track the metrics.\n\n\n\n\n\n\n\n\n\n\nStore the model as ONNX\n\n\n\nIt is also possible to export the model in the ONNX format, see Section 4.1 and for specifics the docs.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/nn.html#backward-propagation-of-error---backpropagation",
    "href": "nn/nn.html#backward-propagation-of-error---backpropagation",
    "title": "7  Neural Networks",
    "section": "7.4 Backward Propagation of Error - Backpropagation",
    "text": "7.4 Backward Propagation of Error - Backpropagation\nOur first NN is a success, but how did it learn the necessary parameters to perform its task?\nThe answer is a technique called Backward Propagation of Error or in short Backpropagation. This essential component of for machine learning helps us to work out how the loss we compute translates into changes of the weights and biases in our network. In our training loop train_model the lines\ny_pred = model(X_batch)            # Forward pass through the model\nloss = loss_fn(y_pred, y_batch)    # Compute the loss\nloss.backward()                    # Backpropagation\noptimizer.step()                   # Update the model parameters\noptimizer.zero_grad()              # Reset the gradients\nare what we are focusing on in this section.\n\n\n\n\n\n\nImportant\n\n\n\nA very nice and structured introduction by IBM can be found here2.\nAn introduction closer to the mathematics is shown in (Brunton and Kutz 2022, chap. 6.3).\n\n\nThe introduction of the technique is contributed to the paper of Rumelhart, Hinton, and Williams (1986). As usual, predecessors and independent similar proposals go back to the 1960s. As we have seen, our NN can mathematically be described by nested functions, that are called inside the loss function, \\(\\mathscr{L}(\\Theta)\\), for \\(\\Theta\\) being all trainable parameters.\nDuring the training, we can compute the change between the NN output and the provided label and use it in a gradient descent method3. The result is the following iteration \\[\n\\Theta^{(n+1)} = \\Theta^{(n)} - \\delta \\nabla \\mathscr{L}\\left(\\Theta^{(n)}\\right),\n\\] where \\(\\delta\\) is called the learning rate and it is prescribed.\nTo compute the derivative of \\(\\mathscr{L}(\\Theta)\\), we can use the chain rule as this propagates the error backwards through the network. For \\(h(x) = g(f(x))\\) the derivative \\(h'(x)\\) is computed as \\[\nh'(x) = g'(f(x))f'(x) \\quad \\Leftrightarrow \\quad\n\\frac{\\partial\\, h}{\\partial\\, x} (x) = \\frac{\\partial\\, g}{\\partial\\, x}(f(x)) \\cdot \\frac{\\partial\\, f}{\\partial\\, x}(x),\n\\] or in Leibniz notation for a variable \\(z\\) that depends on \\(y\\), which itself depends on \\(x\\) we get \\[\n\\frac{\\mathrm{d}\\, z}{\\mathrm{d}\\, x} = \\frac{\\mathrm{d}\\, z}{\\mathrm{d}\\, y} \\cdot \\frac{\\mathrm{d}\\, y}{\\mathrm{d}\\, x}.\n\\]\n\n\n\n\n\n\n\nExample 7.1 (A simple case) To illustrate the procedure we start with the simplest example, illustrated in Figure 7.6.\n\n\n\n\n\n\nFigure 7.6: One node, one layer model for illustration of the backpropagation algorithm.\n\n\n\nTo get the output \\(y\\) from \\(x\\) the following computation is required \\[\ny = g(z, b) = g(f(x, a), b).\n\\]\nIf we now assume a means square error for the final loss, \\[\n\\mathscr{L} = \\frac12 (y_0 - y)^2,\n\\] we get an error, depending on the weights \\(a\\), \\(b\\), and for \\(y_0\\) being the ground truth or correct result. In order to minimize the error according to \\(a\\) and \\(b\\) we need to compute the partial derivatives with respect to these variables\n\\[\n\\begin{aligned}\n\\frac{\\partial\\, \\mathscr{L}}{\\partial\\, a}\n&= \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, y} \\cdot \\frac{\\partial\\, y}{\\partial\\, a} &=& \\big[y = g(z, b)\\big]\\\\\n&= \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, y}\\cdot \\frac{\\partial\\, g}{\\partial\\, z} \\cdot \\frac{\\partial\\, z}{\\partial\\, a} &=& \\big[z = f(x, a)\\big]\\\\\n&= \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, y}\\cdot \\frac{\\partial\\, g}{\\partial\\, z} \\cdot \\frac{\\partial\\, f}{\\partial\\, a}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial\\, \\mathscr{L}}{\\partial\\, b}\n&= \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, y} \\cdot \\frac{\\partial\\, y}{\\partial\\, b} &=& \\big[y = g(z,b)\\big] \\\\\n&= \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, y}\\cdot \\frac{\\partial\\, g}{\\partial\\, b} \\phantom{ \\cdot \\frac{\\partial\\, f}{\\partial\\, a}}\n\\end{aligned}\n\\]\nFor a particular \\(\\mathscr{L}\\), \\(g\\), and \\(f\\) we can compute it explicitly, e.g. \\(\\mathscr{L}=\\tfrac12(y_0 - y)^2\\), \\(g(z,b) = b z\\), and \\(f(x,a) = \\tanh(a x)\\)\n\\[\n\\begin{aligned}\n\\frac{\\partial\\, \\mathscr{L}}{\\partial\\, a} &=  \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, y} \\frac{\\partial\\, g}{\\partial\\, z}\\frac{\\partial\\, f}{\\partial\\, a} &=& - (y_0 - y) \\cdot b \\cdot (1 - \\tanh^2(a x)) \\cdot x,\\\\\n\\frac{\\partial\\, \\mathscr{L}}{\\partial\\, b} &=  \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, y} \\frac{\\partial\\, g}{\\partial\\, b} &=& - (y_0 - y) \\cdot \\tanh(a x).\n\\end{aligned}\n\\]\nWith this information we can define the gradient descent update \\[\n\\begin{aligned}\na^{(k+1)} &= a^{(k)} - \\delta \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, a} = a^{(k)} - \\delta \\left(- (y_0 - y) \\cdot b^{(k)} \\cdot (1 - \\tanh^2(a^{(k)} x)) \\cdot x\\right), \\\\\nb^{(k+1)} &= b^{(k)} - \\delta \\frac{\\partial\\, \\mathscr{L}}{\\partial\\, b} = b^{(k)} - \\delta \\left( - (y_0 - y) \\cdot \\tanh(a^{(k)} x)\\right).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 7.1 (Backpropagation) Now that we have a better understanding we can define the backdrop procedure, see Brunton and Kutz (2022) as reference:\n\nSpecify the NN along with the labeled training data.\nInitialize the weights and biases of the NN with random values. If they are initialized with zero the gradient method will update all of them in the same fashion which is not what we are looking for.\nIn a loop until convergence or a maximum of iterations is achieved:\n\nRun the training data through the NN to compute \\(y\\). Compute the according loss and its derivatives with respect to each weight and bias.\nFor a given learning rate \\(\\delta\\) update the NN parameters via the gradient method.\n\n\n\n\n\nWe can see this reflected in our code for the NN above.\n\n\n\n\n\n\n\nExercise 7.9 (Dogs and cats) Let us translate this findings to our example visualized in Figure 7.3. In order to do so, we need to specify our variables in more detail. To simplify it a bit we set the biases to the zero vector.\nFirst, we call our output \\(p = [p_1, p_2]\\) and our labels are encoded in one-hot encoding \\(y = [y_1, y_2]\\) where \\(y_1=1\\) and \\(y_2=0\\) if the image is a dog, and vice versa if the image is a cat.\nAs our loss function we use cross-entropy \\[\n\\begin{aligned}\n\\mathscr{L}(\\Theta) &= - \\frac12 \\sum_{i=1}^2 y_i \\log(p_i) = - \\frac{y_1 \\log(p_1) + y_2 \\log(p_2)}{2} \\\\\n&= -\\frac{1}{2} \\left( y_1 \\log(p_1) + (1-y_1) \\log(1-p_1) \\right),\n\\end{aligned}\n\\] for a single sample with the above notation. The last line is true to the fact that the sum of the entries of \\(y\\) and \\(p\\) is equal to \\(1\\).\nIn order to make the computation of the derivates easier we use the variables as described in Figure 7.7.\n\n\n\n\n\n\nFigure 7.7: One node, one layer model for illustration of the backpropagation algorithm.\n\n\n\nTherefore, we get \\(p = g(z)=\\sigma(B v)\\) (softmax), and \\(v = f(u)= \\tanh(A x)\\). Overall we want to compute the change in \\(B_{i, j}\\) (for some fixed indices \\(i\\), and \\(j\\)). \\[\n\\frac{\\partial\\, \\mathscr{L}}{\\partial \\, B_{i, j}} = \\frac{\\partial\\, \\mathscr{L}}{\\partial \\, p} \\cdot \\frac{\\partial\\, p}{\\partial \\, z} \\cdot \\frac{\\partial\\, z}{\\partial \\, B_{i, j}}\n\\]\nPerform this task in the following steps:\n\nCompute \\(\\partial_{p_i} \\mathscr{L}\\).\nThe computation of the Jacobian of \\(\\sigma(z)\\) is tricky but together with the cross-entropy loss it is straight forward, therefore compute \\(\\partial_{z_i} \\mathscr{L}(\\sigma(z))\\).\nCompute \\(\\partial_{B_{i, j}} z\\).\nWrite down the components showing up for the chain rule for \\(\\frac{\\partial\\, \\mathscr{L}}{\\partial \\, A_{i, j}}\\) (similar as above).\n\n\n\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/nn.html#footnotes",
    "href": "nn/nn.html#footnotes",
    "title": "7  Neural Networks",
    "section": "",
    "text": "Install it via pdm add lightning.↩︎\nAccess on 4th April 2025.↩︎\nsee Kandolf (2025), Section 6.1 or follow the direct link.↩︎",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/cnn.html",
    "href": "nn/cnn.html",
    "title": "8  Convolutional Neural Networks",
    "section": "",
    "text": "8.1 The Convolution Operation\nConvolutional neural networks CNNs (often also called deep convolution neural nets DCNNs), are specialized neural networks for processing grid like data. This especially includes image data (a 2D grid) but also time-series (1D grid) data or grids with more dimensions. They have seen a tremendous success in many applications and are now often synomenouse with NNs.\nAs the name suggest, CNNs are nothing else than a NN with at least one convolution layer. Nevertheless some more techniques are usually employed along side convolution, the main ones are discussed in this section.\nConvolution is a specialized linear operation that we apply instead of the matrix multiplication between two layers of a NN.\nUnfortionalty, in many frameworks the term convolution is used for the related function cross-correlation (as e.g. torch.nn.Conv2d)\nLet us walk through a 2D example, for an input of size \\(5 \\times 5\\) and for an averaging kernel (no flipping) of size \\(2 \\times 3\\), illustrated in Figure 8.1.\nThe main ideas behind convolution in NN is to allow for sparse interactions, parameter sharing, and equivalent representations. These concepts allow for a more efficient implementation, both in perms of storage and computation time. We follow the descriptions in Goodfellow, Bengio, and Courville (2016) for these terms.\nThe idea of sparse interaction is, that we do not have fully connected neurons. In particular, by performing convolution with a smaller kernel size, we can reduce the input size drastically. For example, a kernel that detects edges only needs to look at a couple of pixels at once to detect an edge. This allows for a more efficient and memory optimized implementation. It can also be used to downsample an image in case the input size is not homogeneous for all images.\nNext, parameter sharing refers to the process of using the same parameter (weight) for more than one function and input. In particular, the kernel weights are applied to all the different positions in the input. As a result, the CNN only learns one set of parameters (for the kernel) and not different sets for each of the positions, reducing the storage (but not necessarily the runtime) demands.\nFinally, if we talk about equivalence representations we mean that input features transform in a predictable fashion under certain transformations. In particular, it should not mather if we first apply convolution and than said transform or the other way round. For CNNs the most important of these is translation. If we shift a parameter and than perform convolution or reverse the order, we see no change in the output. This allow the CNNs to recognize patterns and features consistently, even when we see a transform of the input, e.g. the cat sits 10 pixels to the left.\nTo illustrate the power of convolution we use it to detect the (vertical) edges of a cat via the simple difference of two neighbouring pixels (the first discrete derivative), see Figure 8.2.\nThis feature is often use for object detection.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/cnn.html#the-convolution-operation",
    "href": "nn/cnn.html#the-convolution-operation",
    "title": "8  Convolutional Neural Networks",
    "section": "",
    "text": "Definition 8.1 (Convolution) From a pure mathematical point of view, convolution is defined for two functions and produces a third \\[\nh(t) = (f \\ast g)(t) = \\int f(\\tau)g(t-\\tau)\\, \\mathrm{d}\\tau.\n\\]\nThis concept popped up when discussing the properties of the Fourier, and Laplace transform1, nevertheless it is more similar to Wavelets2. If \\(g\\) is a probability density function3 the convolution can be understood as the weighted average of \\(f\\) by \\(g\\).\nIf \\(f\\) and \\(g\\) are discrete functions, like in our application an image with discrete pixel values or a time series with a measurement every second, we get the discrete convolution as \\[\nh(i) = (f \\ast g)(i) = \\sum_{m = -\\infty}^{\\infty} f(m)g(i - m).\n\\]\nIn the context of CNNs the function \\(f\\) is called input, \\(g\\) kernel, and \\(h\\) the feature map.\nQuite often, convolution is not only applied in one dimension but along multiple dimensions4. In a discrete setting this looks like \\[\nh(i, j) = (f \\ast g)(i, j) = \\sum_m \\sum_n f(m, n) g(i - m, j - n).\n\\]\nNote, convolution is commutative, a quite useful property for the implementation. This is due to the fact, that the kernel is flipped with regards to the input (the minus sign), i.e. \\[\nh(i, j) = (g \\ast f)(i, j) = \\sum_m \\sum_n f(i - m, j - n) g(m, n).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 8.2 (Cross-correlation) The related function cross-correlation is the same as convolution but without flipping the kernel, i.e.\n\\[\nh(i, j) = (g \\ast f)(i, j) = \\sum_m \\sum_n f(i + m, j + n) g(m, n)\n\\]\nand therefore it is not commutative.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA CNN will either learn the values for a flipped or not-flipped kernel. Depending on how the function is defined. Therefore, from the point of view of a CNN but functions work.\n\n\n\n\n\n\n\n\n\nFigure 8.1: Without any padding and stride equal to \\(1\\) we simply move the kernel over the input starting from the top left. Make an element wise multiplication and a summation of all elements. E.g. the top left entry is computed as \\(2 \\tfrac18 + 6 \\tfrac14 + 8 \\tfrac18 + 2 \\tfrac18 + 7 \\tfrac14 + 2 \\tfrac18 = 5\\). By moving to the right, we move along a row, by moving down we change column.\n\n\n\n\n\n\n\n\n\n\nExercise 8.1 (Output shape per input and kernel shape) Let us assume we have an input of size \\((a \\times b)\\) and kernel of size \\((c \\times d)\\), in Figure 8.1 this would be \\(a=b=5\\) and \\(c=2\\), \\(d=3\\).\nWe call padding the process of extending the input by a boundary of (usual constant - zero) values. For \\(p=1\\) we extend at the left, right, upper, and lower boundary by 1 column and row.\nFurthermore, stride or step size is how much the kernel is moved in each step.\n\nWrite down the general formula for the size of the output for no padding and \\(s=1\\).\nExtend the formula to include padding but keep \\(s=1\\).\nExtend the formula to include stride without padding.\nExtend the formula to include a stride and padding.\nHow large does the padding need to be to have the same output size as input size (for \\(s=1\\)).\n\nNote: Have a look at Dumoulin and Visin (2016) and the corresponding GitHub Repo Link for a nice visual and computational explanation.\n\n\n\n\n\n\n\n\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(np.reshape(cats[:, 0], (64, 64)).T,\n            cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\nplt.figure()\nedge = np.zeros((64, 63))\nfor i, c in enumerate(np.reshape(cats[:, 0], (64, 64)).T):\n    edge[i, :] = np.convolve(c, [-1, 1], mode=\"valid\")\nplt.imshow(edge, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The original image of cat zero.\n\n\n\n\n\n\n\n\n\n\n\n(b) Edges detected via the first derivative (order 1)\n\n\n\n\n\n\n\nFigure 8.2: Application of a simple one dimensional kernel [-1, 1] to an image. The right image has one pixel less per row as a result of the convolution.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can express convolution via matrix-matrix multiplication. The resulting matrices are sparse, highly structured and repeat the same element quite often.\nIn Figure 8.2 we need \\(64 \\times 63 \\times 3 = 12,096\\) operations to compute the right image from the left. If we would implement this with a dense matrix-matrix computation this would mean \\(64 \\times 64 \\times 64 \\times 63 = 16,515,072\\) operations.\nConvolution is an efficient mechanism for applying a consistent linear transformation to localized regions across the entire input domain.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/cnn.html#pooling",
    "href": "nn/cnn.html#pooling",
    "title": "8  Convolutional Neural Networks",
    "section": "8.2 Pooling",
    "text": "8.2 Pooling\nConvolution layers are often followed by pooling layers. They are designed to progressively reduce the spatial size in order to reduce the number of parameters. They make the representation invariant to small translations. This is an efficient strategy to reduce overfitting and to reduce storage demands. Furthermore, it is great if we do not so much care where a feature is but more if it is present or not. In our default example we do not care where the dogs/cats are but only which is present.\nIf we do not simply pool over spatial properties but different convolutions we can become invariant over more complicated transformations like rotation.\nThe most common pooling function are:\n\nmax pooling, returning the maximum over a rectangular neighbourhood,\naveraging, compute the average of a neighbourhood,\nweighted averaging, compute the weighted average where the weights decrease with the distance to the middle,\n\\(L^2\\) norm of a rectangular neighbourhood.\n\nIn image processing the most common form of max pooling is with a \\(2\\times 2\\) filter with a stride of \\(2\\), effectively downsampling to \\(\\frac14\\) parameters, see Figure 8.3.\n\n\n\n\n\n\nFigure 8.3: For the input on the left we compute two different pooling functions. First max pooling with a \\(2\\times 2\\) kernel and stride \\(2\\), second mean with a \\(2\\times2\\) kernel and stride \\(2\\).\n\n\n\n\n\n\n\n\n\n\nExercise 8.2 (Applying pooling to images) Implement max and mean pooling (or find an appropriate library function) for an image and apply it to the (unmodified) cat from above.\nExplain which features the different pooling methods conserve and which are lost.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/cnn.html#dropout",
    "href": "nn/cnn.html#dropout",
    "title": "8  Convolutional Neural Networks",
    "section": "8.3 Dropout",
    "text": "8.3 Dropout\nCNNs often suffer from overfitting and therefore are not good at generalizing. One way to work against this is by introducing dropout. The key idea is to randomly drop (set to zero) some nodes in the network, during the backward propagation. This has shown to have favourable affects on overfitting and has therefore become a stable in CNNs.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/cnn.html#implementation",
    "href": "nn/cnn.html#implementation",
    "title": "8  Convolutional Neural Networks",
    "section": "8.4 Implementation",
    "text": "8.4 Implementation\nNow that we have discussed the basic structure of a CNN we want to implement it. Luckily we can recycle most of the code from Section 7.2. In the following we only show where we needed to make some changes.\n\nX_train_tensor = torch.tensor(\n1    X_train.reshape(-1, 1, size, size).transpose([0, 1, 3, 2]),\n    dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.uint8)\nX_test_tensor = torch.tensor(\n    X_test.reshape(-1, 1, size, size).transpose([0, 1, 3, 2]),\n    dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.uint8)\ndataset = TensorDataset(X_train_tensor, y_train_tensor)\n\nclass MyFirstCNN(torch.nn.Module):\n    def __init__(self, conv_out_channels, size):\n        super(MyFirstCNN, self).__init__()\n2        self.model = torch.nn.Sequential(\n            torch.nn.Conv2d(1, conv_out_channels, 5, padding=2),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(2),\n            torch.nn.Dropout(p=0.2),\n3            torch.nn.Flatten(),\n            torch.nn.Linear(conv_out_channels * size // 2 * size // 2, 2),\n        )\n\n\n    def forward(self, x):\n        return self.model(x)\n\n\n1\n\nWe need to reshape our image to a square and inflate the dimensions to allow processing for Conv2d.\n\n2\n\nThe Convolution part of our model, this could be repeated with different out_channel sizes.\n\n3\n\nClassification step at the end of the model to break everything down to two classes.\n\n\n\n\n\n\nShow the code for the figure\n# Initialize everything for reproducibility\ntorch.manual_seed(6020)\nnp.random.seed(6020)\n\nbatch_size = 8\nmodel = MyFirstCNN(8, size)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n\nepochs = 40\nhistory = train_model(model, dataset, loss_fn, optimizer,\n                      epochs=epochs,\n                      validation_split=0.1,\n                      batch_size=batch_size)\n\nmodel.eval()\nwith torch.no_grad():\n    y_proba = torch.nn.Softmax(dim=1)(model(X_test_tensor))\ny_predict = y_proba.argmax(axis=-1)\nmyplot(y_predict * (-2) + 1)\nacc = (y_predict == y_test_tensor).sum().item() / len(y_test)\n\nn = y_proba.shape[0]\nplt.figure()\nplt.bar(range(n), y_proba[:, 0], color='y', label=r\"p(dog)\")\nplt.bar(range(n), y_proba[:, 1], color='b', label=r\"p(cat)\", \n        bottom=y_proba[:, 0])\nplt.plot([-0.5, n - 0.5], [0.5, 0.5], \"r\", linewidth=1.0)\nplt.gca().set_aspect(n / (1 * 3))\nplt.legend()\n\n\nepochs_range = range(len(history[\"train_acc\"]))\nplt_list = [[\"train_acc\", \"-\"], [\"train_loss\", \":\"],\n            [\"val_acc\", \"--\"], [\"val_loss\", \"-.\"]]\nplt.figure()\nfor name, style in plt_list:\n    plt.plot(epochs_range, history[name], style, label=name)\nplt.legend(loc=\"lower right\")\nplt.gca().set_aspect(epochs / (1 * 3))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Classification for our test set.\n\n\n\n\n\n\n\n\n\n\n\n(b) Probabilities of the two classes - softmax on the model output.\n\n\n\n\n\n\n\n\n\n\n\n(c) Summary of the key metrics of the model training.\n\n\n\n\n\n\nFigure 8.4: Performance of our model.\n\n\n\n\nIn Figure 8.4 (a) we can see the final classification of our model with regards to the test set. For 10 dogs our model is convinced they are not good dogs but cats, and 3 cats are classified as dogs, in comparison to Figure 7.5 (a) we gained one correct classified dog. If we look at the probabilities Figure 8.4 (b), we can see that we have a couple of close calls, but in general our model is quite sure about the classification. Regarding the history of our optimization, we can see learning happen right away with a saturation starting to manifest after about 30 iterations.\nAt the end, we have an accuracy of 82.5% for our test set, slightly better than our NN from before Figure 7.5.\n\n\n\n\n\n\n\nExercise 8.3 (CNN for original cats and dogs images) In the above example we used the wavelet representation of our images. As mentioned before, CNNs are quite capable in finding edges and therefore this step should not be required.\nImplement a version of the above CNN with the original images (note, they have a different size of \\(64\\times 64\\) pixels).\nIt might be beneficial to implement multiple convolution stages with different sizes to get good performance, try to find a good architecture, this include some imagination and tests.\n\n\n\n\n\n\n\n\nDumoulin, Vincent, and Francesco Visin. 2016. “A guide to convolution arithmetic for deep learning.” ArXiv e-Prints, March.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/cnn.html#footnotes",
    "href": "nn/cnn.html#footnotes",
    "title": "8  Convolutional Neural Networks",
    "section": "",
    "text": "see Kandolf (2025), Section 8.2 or follow the direct link↩︎\nsee Kandolf (2025), Section 10 or follow the direct link↩︎\nsee Kandolf (2025), Section 14.2 for some examples or follow the direct link↩︎\nsee Kandolf (2025), Section 11 for a similar concept regarding the two dimensional Wavelet or Fourier transform, or follow the direct link↩︎",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "nn/autoencoder.html",
    "href": "nn/autoencoder.html",
    "title": "9  Autoencoders",
    "section": "",
    "text": "9.1 Applications\nAnother class of neural networks, that exploit low-dimensional structure in high-dimensional data, is called autoencoders. Autoencoders can be seen as a generalization of linear subspace embedding of singular value decomposition/principal component analysis1 to a nonlinear manifold embedding.\nThe main idea, as stated in (Goodfellow, Bengio, and Courville 2016, chap. 14), is to train a NN to copy its input \\(x\\) via the latent space \\(y\\) to the output \\(z\\). Crucially, \\(y\\) is usually a low dimensional space, represents the output of the encoding and the input of the decoding part of the NN. This will force the latent space to reveal some useful properties that we can studied and exploited, same as PCA.\nDuring the training the weights of the (two) neural networks are learned by minimizing a loss function that measures the difference between the input \\(x\\) and the reconstruction \\(z\\), e.g. \\(\\operatorname{argmin}\\|z - x\\|^2\\).\nTo connect this concept back to PCA and what we discussed in the Eigenfaces Example2, where we used truncated SVD to project into a low dimensional space and than back to reconstruct the image, we formulate it in a more mathematical fashion.\nWe call the encoder \\(y=f_\\Theta(x)\\) and the decoder \\(z=g_\\Theta(y)\\). Therefore, \\(z=(g \\circ f)(x) = g(f(x))\\) and \\[\n\\begin{aligned}\nf : \\mathcal{X} \\to \\mathcal{Y}\\\\\ng : \\mathcal{Y} \\to \\mathcal{Z}\\\\\n\\end{aligned}\n\\] and the loss is computed as \\[\n\\mathscr{L}(x, g(f(x))),\n\\] with an appropriate loss function \\(\\mathscr{L}\\).\nThis loss function is most of the time including some more general constraints to enforce something like sparsity, similar as LASSO3 or RIDGE4, this is often referred to regularized autoencoders.\nWe can use Autoencoders for various different applications:\nTo illustrate this we construct an autoencoder for denoising the MNIST dataset we have seen before (Section 1.5).\nFirst, we need to prepare and augment the data with some noise.\nfrom sklearn.datasets import fetch_openml\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nimport torchvision.transforms.v2 as transforms\ntorch.manual_seed(6020)\nnp.random.seed(6020)\n\n1def transform(X, noise=0.4):\n    X_org = torch.tensor(X / 255.0).to(torch.float)                 \n    X_noise = torch.clamp(\n                torch.tensor(                                         \n                    np.random.normal(0.5, noise, X_org.shape)       \n                ).to(torch.float) + X_org,\n                0, 1)\n    return X_noise, X_org\n\nmnist = fetch_openml(\"mnist_784\", as_frame=False)\nsize = 60000\nX_train, X_test = mnist.data[:size], mnist.data[60000:]\n\ntrainset = TensorDataset(*transform(X_train))\ntestset = TensorDataset(*transform(X_test))\n\nval_size = int(0.1 * len(trainset))\ntrain_size = len(trainset) - val_size\ntrain, val = random_split(trainset, [train_size, val_size])\n\ntrainLoader = DataLoader(train, batch_size=32, shuffle=True)\nvalLoader = DataLoader(val, batch_size=32, shuffle=True)\ntestLoader = DataLoader(testset, batch_size=32, shuffle=False)\n\n\n1\n\nTransform the images and return the original as ground truth.\nNext we slightly modify our training function to suit our new task.\nShow the code for the training loop\ndef train_model(model, train_dl, val_dl, loss_fn, optimizer,\n                epochs=10):\n\n    met = {\n        \"train_loss\": [],\n        \"val_loss\": []}\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_corr = 0, 0\n\n        for X_batch, y_batch in train_dl:\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss, val_corr = 0, 0                                       \n        with torch.no_grad():\n            for X_val, y_val in val_dl:\n                y_val_pred = model(X_val)\n                val_loss += loss_fn(y_val_pred, y_val).item()\n\n        met[\"val_loss\"].append(val_loss / len(val_dl))\n        met[\"train_loss\"].append(train_loss / len(train_dl))\n    return met\nOur autoencoder is quite simple with just 3 stages per encoder.\nclass AutoEncoder(nn.Module):\n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n        \n        self.encoder = torch.nn.Sequential(\n            nn.Linear(784, 128),\n            nn.LeakyReLU(),\n            nn.Linear(128, 64),\n            nn.LeakyReLU(),\n            nn.Linear(64, 32),            \n        )\n\n        self.decoder = torch.nn.Sequential(\n            nn.Linear(32, 64),\n            nn.LeakyReLU(),\n            nn.Linear(64, 128),\n            nn.LeakyReLU(),\n            nn.Linear(128, 784),\n            nn.Sigmoid(),\n        )\n        \n    def forward(self, x):\n1        y = self.encoder(x)\n        z = self.decoder(y)\n        return z\n\n\n1\n\nIntermediate - latent space - representation\nFinally we need to train our model.\nmodel = AutoEncoder()\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nepochs = 5\nhistory = train_model(model, trainLoader, valLoader, loss_fn, optimizer,\n                      epochs=epochs)\nNote, we only used 5 epochs for the following results and this time we used Adam as optimizer.\nShow the code for the figure\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nx, y = next(iter(testLoader))\nmodel.eval()\nwith torch.no_grad():\n    z = model(x)\n\nlength = 10\nim = np.zeros((3 * 28, length * 28), float)\nfor j in range(length):\n    im[0:28, 28*j:28*(j+1)] = y[j].reshape(28, 28).numpy()\n    im[28:28*2, 28*j:28*(j+1)] = x[j].reshape(28, 28).numpy()\n    im[28*2:28*3, 28*j:28*(j+1)] = z[j].reshape(28, 28).numpy()\n\nplt.figure()\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9.2: Some of the test images denoised via the model. Top row shows the original image, second with noise and last row after calling the model.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Autoencoders</span>"
    ]
  },
  {
    "objectID": "nn/autoencoder.html#applications",
    "href": "nn/autoencoder.html#applications",
    "title": "9  Autoencoders",
    "section": "",
    "text": "Image denoising: In this case the autoencoder is trained with noisy images and learns how to reconstruct clean images from them.\nAnomaly detection: An autoencoder that is trained with normal images can detect anomalies by identifying inputs that result in a high reconstruction loss, this indicated a deviation from the learned images.\nImage generation: After training an autoencoder it is possible to use the decoder part to generate images for manipulated values in the latent space.\nFeature extraction: The latent representation of the trained autoencoder can provide a compact informative feature for image classification and retrieval.\nImage compression: By storing only the latent space representation an image can be compressed and reconstructed by inferring through the decoder.\nImage enhancement: By providing a low resolution images and their high resolution counterparts images can learn to enhance certain features of images.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.1 (Add more transformations) Rework the transform function to include the following additions.\n\nShuffle the pixels of the image with a fixed permutation.\nRotate the image with a fixed rotation angle, extend the image first, rotate and than crop it again.\nFlip the image - mirror with respect to the middle.\n\nRetrain the autoencoder with these training sets and see how it performs (can it handle all of them at the same time?).\n\n\n\n\n\n\n\n\n\n\n\nExercise 9.2 (Explore the latent space and generate numbers) Rework the above autoencoder in such a fashion that the encoder an decoder can be accessed individually. If for the later tasks reworking the model such that the latent space is smaller is beneficial, feel free to do so, e.g. only 2 dimensions.\n\nExplore the latent space (e.g. similar to what was done in Section 1.2) and see how well the clusters of numbers can be seen.\nUse the knowledge of the previous task to create a number generator by providing different self generated elements from the latent space to the decoder part of our autoencoder.\n\n\n\n\n\n\n\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Autoencoders</span>"
    ]
  },
  {
    "objectID": "nn/autoencoder.html#footnotes",
    "href": "nn/autoencoder.html#footnotes",
    "title": "9  Autoencoders",
    "section": "",
    "text": "see Kandolf (2025), Section 4 for SVD and 4.2 for PCA or follow the direct link.↩︎\nsee Kandolf (2025), Section 4.2.2 or follow the direct link.↩︎\nsee Kandolf (2025), Section 7.1.1 or follow the direct link.↩︎\nsee Kandolf (2025), Section 7.1.2 or follow the direct link.↩︎",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Autoencoders</span>"
    ]
  },
  {
    "objectID": "nn/transfer.html",
    "href": "nn/transfer.html",
    "title": "10  Transfer learning",
    "section": "",
    "text": "10.1 Fine-tuning an ImageNet for cats and dogs\nWhen working with neural networks, it is always a good idea to have a look what other people did. It is very unlikely that we are the first to look at a particular class of problems and somebody else likely solved (or tried to solve) a similar problem. In addition, most of the time we will not have enough training data to do a full training. If we find such a NN we can look at the architecture, reuse parts of it, build on the training included and so forth. This technique is called transfer learning. As mentioned, if our training set is small is small, this is especially beneficial.\nThe main scenarios for transfer learning are:\nOf course it is not easy to decide exactly in which of the categories our problem falls but as a general rules:\nIn (Geron 2022, chap. 14) we can find amongst a detailed explanation how CNNs work also a discussion about several CNNs architectures and models that are available for transfer learning.\nAs example, we use our already well known dogs and cats example. As mentioned before, the dataset is small and we know that the ImageNet has the capabilities to classify for different species in dogs and cats. Therefore, we will switch the final layer of the network but retrain with all parameters trainable.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transfer learning</span>"
    ]
  },
  {
    "objectID": "nn/transfer.html#fine-tuning-an-imagenet-for-cats-and-dogs",
    "href": "nn/transfer.html#fine-tuning-an-imagenet-for-cats-and-dogs",
    "title": "10  Transfer learning",
    "section": "",
    "text": "10.1.1 Model selection\nVia thetorchvision module we have the capabilities to load some pre-existing models and weights, see docs. As mentioned, we want a model trained on the ImageNet dataset, we use EfficientNet (Tan and Le (2019)). We will see how to load the model a bit later but it is important to have a look at the docs to get an idea how we need to prepare our images and how to replace the final hidden layer. There are multiple versions of the network available, we use B0.\n\n\n10.1.2 Data preparation\nAs the ImageNet dataset uses normal images we do not use our images in the wavelet basis but our original images.\n\n\nShow the code for loading the dataset\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision as tv\nimport torchvision.transforms.v2 as transforms\nfrom torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n%config InlineBackend.figure_formats = [\"svg\"]\n# Initialize everything for reproducibility\ntorch.manual_seed(6020)\nnp.random.seed(6020)\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats_w = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData.mat\")\ndogs_w = scipy.io.loadmat(io.BytesIO(response.content))[\"dog\"]\n\nsize = int(np.sqrt(dogs_w.shape[0]))\ns = 40\nX_train = np.concatenate((dogs_w[:, :s], cats_w[:, :s]), axis=1).T\ny_train = np.repeat(np.array([0, 1]), s)\nX_test = np.concatenate((dogs_w[:, s:], cats_w[:, s:]), axis=1).T\ny_test = np.repeat(np.array([0, 1]), 80 - s)\n\n\nFirst, we need to make sure our input is in a format that can be handled by the NN. As the input size of the image is \\(256 \\times 256\\) with RGB channels and with specific normalizations. To do this we use some helper functions from the torchvision.transforms.v2 module.\n\n1T = transforms.Compose([\n2    transforms.ToPILImage(),\n3    transforms.Grayscale(num_output_channels=3),\n4    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n5    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n\n1\n\nDefine a pipeline to apply several transformations to a single image.\n\n2\n\nTransform a numpy array to a PIL image.\n\n3\n\nRepeat the colour channel three times to simulate an RGB image.\n\n4\n\nRescale via interpolation the image from \\(64\\times 64\\) to \\(256 \\times 256\\) pixel.\n\n5\n\nNormalize via the provided parameters.\n\n\n\n\n/home/runner/work/MECH-M-DUAL-2-MLB/MECH-M-DUAL-2-MLB/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning:\n\nThe transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n\n\n\nSome steps can not be directly be applied in the above pipeline, therefore we reshape the flattened image to a square and rotate it before we apply the pipeline to each image.\n\nX_train_tensor = torch.tensor(np.array(\n    [T(img.reshape((size, size)).T) for img in X_train]))\ny_train_tensor = torch.tensor(y_train, dtype=torch.uint8)\nX_test_tensor = torch.tensor(np.array(\n    [T(img.reshape((size, size)).T) for img in X_test]))\ny_test_tensor = torch.tensor(y_test, dtype=torch.uint8)\ndataset = TensorDataset(X_train_tensor, y_train_tensor)\n\nThe functions for preparing the training and validation dataset, the training loop and plotting stay the same.\n\n\nShow the code for some basic functions for training\ndef myplot(y):\n    plt.figure()\n    n = y.shape[0]\n    plt.bar(range(n), y)\n    plt.plot([-0.5, n - 0.5], [0, 0], \"k\", linewidth=1.0)\n    plt.plot([n // 2 - 0.5, y.shape[0] // 2 - 0.5], [-1.1, 1.1],\n             \"r-.\", linewidth=3)\n    plt.yticks([-0.5, 0.5], [\"cats\", \"dogs\"], rotation=90, va=\"center\")\n    plt.text(n // 4, 1.05, \"dogs\")\n    plt.text(n // 4 * 3, 1.05, \"cats\")\n    plt.gca().set_aspect(n / (2 * 3))\n\ndef get_dataset(dataset: TensorDataset, val_split: float, batch_size: int):\n    val_size = int(val_split * len(dataset))\n    train_size = len(dataset) - val_size\n    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n\n    # Create a dataset\n    train = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    return train, val\n\ndef train_model(model, dataset, loss_fn, optimizer,\n                epochs=250, validation_split=0.1, \n                batch_size=8):\n\n    met = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"train_acc\": [],\n        \"val_acc\": []}\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_corr = 0, 0\n        train_dl, val_dl = get_dataset(dataset, validation_split, batch_size)\n\n        for X_batch, y_batch in train_dl:\n            y_pred = model(X_batch)            # Forward pass through the model\n            loss = loss_fn(y_pred, y_batch)    # Compute the loss\n            loss.backward()                    # Backpropagation\n            optimizer.step()                   # Update the model parameters\n            optimizer.zero_grad()              # Reset the gradients\n\n            train_loss += loss.item()\n            train_corr += (y_pred.argmax(1) == y_batch).sum().item()\n\n        model.eval()\n        val_loss, val_corr = 0, 0                                       \n        with torch.no_grad():                   # No gradient calculation\n            for X_val, y_val in val_dl:\n                y_val_pred = model(X_val)\n                val_loss += loss_fn(y_val_pred, y_val).item()\n                val_corr += (y_val_pred.argmax(1) == y_val).sum().item()\n\n        met[\"train_loss\"].append(train_loss / len(train_dl))\n        met[\"val_loss\"].append(val_loss / len(val_dl))\n        met[\"train_acc\"].append(train_corr / len(train_dl.dataset))\n        met[\"val_acc\"].append(val_corr / len(val_dl.dataset))\n\n    return met\n\n\n\n\n10.1.3 Model loading, modification, and training\nNow, we can finally load our pertained model and modify its final layer.\n\n1model = tv.models.efficientnet_b0(weights='IMAGENET1K_V1')\n2num_classes = 2\nin_features = model.classifier[1].in_features\n3model.classifier[1] = torch.nn.Linear(in_features, num_classes)\n\nbatch_size = 8\nepochs = 10\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nhistory = train_model(model, dataset, loss_fn, optimizer,\n                      epochs=epochs,\n                      validation_split=0.1,\n                      batch_size=batch_size)\n\n\n1\n\nLoad the model via the torchvision interface.\n\n2\n\nDefine the input an output size of the new layer\n\n3\n\nExchange the final layer of the network to allow for only two classes\n\n\n\n\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /home/runner/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n  0%|          | 0.00/20.5M [00:00&lt;?, ?B/s] 76%|███████▋  | 15.6M/20.5M [00:00&lt;00:00, 164MB/s]100%|██████████| 20.5M/20.5M [00:00&lt;00:00, 185MB/s]\n\n\nNow that we have our model and it was trained for 1ß epochs we can have a look at its performance.\n\n\nShow the code for the figure\nmodel.eval()\nwith torch.no_grad():\n    y_proba = torch.nn.Softmax(dim=1)(model(X_test_tensor))\ny_predict = y_proba.argmax(axis=-1)\nmyplot(y_predict * (-2) + 1)\nacc = (y_predict == y_test_tensor).sum().item() / len(y_test)\n\nn = y_proba.shape[0]\nplt.figure()\nplt.bar(range(n), y_proba[:, 0], color='y', label=r\"p(dog)\")\nplt.bar(range(n), y_proba[:, 1], color='b', label=r\"p(cat)\", \n        bottom=y_proba[:, 0])\nplt.plot([-0.5, n - 0.5], [0.5, 0.5], \"r\", linewidth=1.0)\nplt.gca().set_aspect(n / (1 * 3))\nplt.legend()\n\n\nepochs_range = range(len(history[\"train_acc\"]))\nplt_list = [[\"train_acc\", \"-\"], [\"train_loss\", \":\"],\n            [\"val_acc\", \"--\"], [\"val_loss\", \"-.\"]]\nplt.figure()\nfor name, style in plt_list:\n    plt.plot(epochs_range, history[name], style, label=name)\nplt.legend(loc=\"lower right\")\nplt.gca().set_aspect(epochs / (1 * 3))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Classification for our test set.\n\n\n\n\n\n\n\n\n\n\n\n(b) Probabilities of the two classes - softmax on the model output.\n\n\n\n\n\n\n\n\n\n\n\n(c) Summary of the key metrics of the model training.\n\n\n\n\n\n\nFigure 10.1: Performance of our model.\n\n\n\n\nIn Figure 10.1 (a) we can see the final classification of our model with regards to the test set. For only 7 dogs our model is convinced they are not good dogs but cats, and 0 cats are classified as dogs, in comparison to Figure 7.5 (a) and Figure 8.4 (a) we gained some correct classifications. If we look at the probabilities Figure 10.1 (b), we can see that we have a no close calls, but rather a model that is quite sure about the classification. Regarding the history of our optimization, we can see that we start quite good (our initial guess of what is a dog and what is a cat seems to be accurate).\nAt the end, we have an accuracy of 91.25% for our test set,\n\n\n\n\n\n\n\nExercise 10.1 (Classifier) Instead of replacing the final layer of the classifier, add an additional layer with only two outputs for our two classes. The simplest way is to use the loaded module as the first element in a torch.nn.Sequential model and add a second linear layer.\n\n\n\n\n\n\n\n\n\n\n\nExercise 10.2 (Freeze the original network) Instead of including the layers of the original model in the backpropagation try to freeze them and only train a final classifier.\nSee if using one of the other EfficientNet versions (e.g. v3) performs better or worse in this scenario.\n\n\n\n\n\n\n\n\n\n\n\nExercise 10.3 (Padding) Instead of bloating up the image via Resize use Pad to imped the image with a boundary to make sure to reach the original size constraints.\nTry the following scenarios and see if there is a difference in the score: - Symmetric padding with black background - Symmetric padding with white background - Asymmetric padding (image in one corner)\n\n\n\n\n\n\n\n\n\n\n\nExercise 10.4 (Use a different model) There are more models trained on EfficientNet, try at least with one other model to see if similar results are possible.\n\n\n\n\n\n\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.\n\n\nTan, Mingxing, and Quoc Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” In Proceedings of the 36th International Conference on Machine Learning, edited by Kamalika Chaudhuri and Ruslan Salakhutdinov, 97:6105–14. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v97/tan19a.html.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Transfer learning</span>"
    ]
  },
  {
    "objectID": "nn/data.html",
    "href": "nn/data.html",
    "title": "11  Data Preparation",
    "section": "",
    "text": "11.1 Data Augmentation\nNow that we know how to design, train and work with neural networks we need to talk about one of the most important topic: data. We could see throughout this notes that the algorithms need to have data to learn. This includes labelled and unlabelled data.\nThere are several topics that need to be addressed when a project does not simply include training on available datasets. We briefly discuss several important topics (some of which we have already heard) before some more extended discussion on a selection. Quite often a data pipeline consists of the steps as listed below (the order may differ).\nBesides these steps that are often part of a data preparation pipeline or directly included in the training there are other important aspects that greatly influence our outcome.\nIn the previous sections we used our image data either in the wavelet basis, raw or in the case of transfer learning with some augmentations to make sure it fits the input requirements of the network.\nQuite often it is advisable to (randomly) augment the data for training and sometimes also for inference. It has shown to be an important technique to improve several aspects of neural networks, like robustness, overfitting, generalization, and more.\nWith simple augmentations like:\nwe can influence the training and the model drastically. For pytorch we can find most of these in torchvison.transforms.v2, see docs for some insights.\nThe most important aspects influenced by these augmentations are:\nFor instance, if a model is trained to classify images of cats and dogs, data augmentation might include flipping images horizontally, rotating them slightly, or adjusting brightness. This ensures the model learns to recognize cats and dogs regardless of orientation or lighting conditions.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "nn/data.html#sec-nn-data-transform",
    "href": "nn/data.html#sec-nn-data-transform",
    "title": "11  Data Preparation",
    "section": "",
    "text": "Geometric transformations: rotation, flipping, scaling, cropping, translation, etc.\nColour adjustments: brightness, contrast, saturation/hue, monochrome images, etc.\nArtificial noise: random noise to simulate real-world imperfections like dust, glare, focus, etc.\nRandom erasing: by masking out random parts of an image robustness can be improved and it simulates occlusions.\nCombination of images: to create new images for training it is sometimes possible to combine existing images.\n\n\n\n\nDataset: With data augmentation we can artificially increase the size of our dataset and therefore expose our network to more training data without the often tedious task of labeling. If we apply this in a random fashion we need to make sure that the seed is set and we can track the changes in case we need to get a deeper understanding of the mechanics behind it. Overall, by always applying some augmentation, overfitting can be drastically reduced. If, for some reason, your dataset is not balanced and one class is less frequent than it should be, these techniques can also produce more instances. This process is called synthetic minority oversampling techniques (SMOTE).\nGeneralization: With data augmentation we can make sure the model learns on features that are invariant to geometric transformations or something like different zoom levels. Overall, this will lead to a better generalization of the network.\nOverfitting: Especially for small dataset overfitting is a serious problem and the augmentation techniques are a reliable tool to mitigate the risk of overfitting to your training set.\nPerformance: Quite often it also helps to introduce data augmentations during training achieve better results on several performance metrics like accuracy.\nSimulates real-world variability: When taking an image there are usually variations included. This might be orientation, occlusions, lighting conditions and so forth. We can simulate these problems via data augmentation.\n\n\n\n\n\n\n\n\n\nExercise 11.1 (Use data augmentation to improve our models) Similar to the pipeline use in Chapter 10 create a pipeline to include some of the above described data augmentations and retrain the models we created with this pipeline in place for the training set. Can you see any of the above described benefits?",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "nn/data.html#sec-nn-data-labelling",
    "href": "nn/data.html#sec-nn-data-labelling",
    "title": "11  Data Preparation",
    "section": "11.2 Data Labeling",
    "text": "11.2 Data Labeling\nLabeling data is a critical step in machine learning projects as the quality directly influences the performance of the model. For image data the process of labeling involves assigning meaningful annotations to images (what is in the image, or what classes it belongs to), bounding boxes or masks for objects visible in the image. Furthermore, these informations should be easy to process by a machine and keeping a change log is also recommended.\nDepending on the task we set out to achieve, the requirements on labelling can be quite different. The following list covers some tasks and also highlights where specific labels can be problematic.\n\nImage classification: Each image has a single label e.g., cat or dog. This can cause problems, especially if we decide to move from superclasses like animal, for lagged, furry to subclasses or specific species of cats.\nObject detection: Annotating bounding boxes around objects within an image. This can be tricky in the case of objects overlapping each other or for a sequence of images where parts get obfuscated.\nSemantic segmentation: Assigning a class label to each pixel in an image. This can cause problems with class imbalance if e.g. background is the dominate class. Another problem occurs when capturing precise object boundaries and the challenges involved here with down and upsampling. Furthermore, generalization can be a problem in these tasks if e.g. background is always black.\nInstance segmentation: This builds on sematic segmentation but distinguishes between different individual of the same instance, e.g. two different cats in one image. One obvious challenge here is that even for a human labelling it might not always be clear if a pixel belongs to one instance or the other if the overlap.\n\nAbove be discussed some specific challenges depending on the topic but there are also more general challenges in the context of labelling image data.\n\nSubjectivity: Images allow for some form of interpretation what exactly is part of the image, instance. This might be where one object begins and another ends, e.g. satellite data and we need do distinguish between a roof with grass and an actual field, not to talk about different plants in the background of image. Furthermore, if we try to label emotions this might be hard due to cultural differences. The same is true for abstract concepts.\nScalability: The larger the dataset, the more time consuming and challenging the labelling process becomes. This is e.g. why old CAPTCHA algorithms employed users to read text to help improve OCR software1. The task is even more challenging for the pixel labels required for segmentation.\n\n\nImbalance: This can be more subtle than we often think. E.g. there are not a lot of images of snow leopards and those existing might be from the same individual leading to problems we already discussed.\nErrors: It is possible that labels are wrong or inconsistent. Such problems can propagate and influence workflow. While we can simply correct an error it might also be beneficial to keep track of the wrong labels to make sure we capture model drift and also to work against systematic errors.\nCost: The costs for high-quality labelled data was made implicitly clear in the above points but we need to mention it separately as it can have serious implications on a project.\n\n\n\n\n\n\n\nBest practices\n\n\n\nIn the light of the above challenges it is a good idea keep the following checks in mind. Start of small to establish a process, gave labeling guidelines in place to allow for reproducible results, if possible allow for multiple annotators, implement a quality control, leverage pre-labelled data if possible.\n\n\n\n11.2.1 Tools for Labeling\nThere are several tools available to help in the labelling process.\n\nLabel Studio: Open-source tool for general data labelling (free, commercial with extended security features available)\nCVAT: Open-source tool specific for computer vision labelling tasks (commercial product available)\nSuperAnnotate: General data labelling (commercial)\nVGG Image Annotator (VIA): Open-source tool for image, audio and video annotation (free)\nAmazon SageMaker Ground Truth: An annotation tool from AWS (commercial)\nLabelbox: General data labelling (commercial)\n\nAs Label Studio is one of the most versatile and free tools available we highlight some of the main features as a general guide on how these tools work and what kind of workflows they provide.\nLabel Studio supports a wide range of labelling/annotation tasks from image classification via object detection to segmentation tasks. This is handled via multiple types of labels like bounding boxes or polygons and allows cooperative processes. We can use tools ans assistant systems to facilitate semi-automated labelling where the human performs fine tuning and checks the results. Furthermore, we can define workflows and have quality control features (consensus scoring) available.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "nn/data.html#footnotes",
    "href": "nn/data.html#footnotes",
    "title": "11  Data Preparation",
    "section": "",
    "text": "Mix some known letters or words to check if the user is correct and collect the data for the second part to build up a statistically significant classification.↩︎",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "nn/challenges.html",
    "href": "nn/challenges.html",
    "title": "12  Common Challenges",
    "section": "",
    "text": "This concludes our introduction to neural networks. We introduced the basic concept, how it relates back to regression and how we can extend it to outperform regression methods. The illustrations might not always be the most evolved but they are crucial to convey the concepts and get started in the topic. We did cover some specific architectures and types of neural networks like CNNs and Autoencoders.\nNevertheless, there are some topics we could not cover but that often surface when working with neural networks. We use this space to briefly discuss some aspects. The idea is not a comprehensive discussion but rather to give some guidance if problems occur. See Geron (2022) for more details on many on the mentioned subjects and original references.\n\nVanishing/Exploding gradient: The working horse of backpropagation is the (stochastic) gradient descent method. The problem that might occur is that the gradient becomes smaller and smaller when propagated through the layers and the change for weights becomes almost null during training. On the other side it can also run in the opposite direction. This problems are often cited for one of the main reasons that NN where abandoned in the early 2000s. In Geron (2022) we can find quite a nice introduction in the corresponding chapter. In short, a combination of activation function and initialization of weights caused the problem is some cases. By introducing new weight initializations this could be mitigated and therefore it is not such a huge problem, if you see it change the weight init or rethink the choice of activation functions.\nDying ReLUs: The vanishing gradient problem helped ReLU to rise to prominence, as it does not saturate like the sigmoid. Nevertheless, there is another problem where some neurons die during training (they output 0 only). This happens when the weights and bias always result in a negative number for any input. To mitigate this problem, leaky ReLu was introduces. The so called SELU and ELU are alternatives as well.\nBatch Normalization: With better weight initialization and ReLU activation vanishing gradients can still happen. To have an additional safeguard batch normalization was introduced. After the activation it normalizes over a batch or training data and therefore the network learns the optimal mean and shift for the data. This is often the first layer and helps preparing the data and makes sure everything is in an expected range.\nModel/Data Drift: Over the lifetime of a model it can happen that the output or performance degenerates. This might be due to a shift in the input data that is not known or considered. E.g. consider changing the camera or updating the software. The output might afterwards change in a way we do not see with the naked eye but the model perceives. Retraining the model with new input will most likely fix the problem. The frequency of required retraining is hard to predict so monitoring the performance of the model is key.\nOverfitting: This is a serious problem that we addressed thought the notes and is listed here as it is often the cause of poor performance and poor generalization. There are some common strategies to mitigate it like data augmentation and others.\nUnderfitting: Our model can also suffer from underfitting, i.e. poor performance. Try training for longer, find better features and if this does not help increase the complexity of the model.\n\n\n\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 3e. 3rd ed. Sebastopol, CA: O’Reilly Media.",
    "crumbs": [
      "Neural Networks and Deep Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Common Challenges</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "This concludes our introduction to machine learning in the context of images. The field of grid bases machine learning is vast and always developing. With this in mind, these notes provide a strong foundation for addressing many challenges in industrial image processing using machine learning tools. By illustrating the concepts with well established examples and directly in Python code we hope to give a sound basis for developing new concepts and workflows.\nIn summary, if none of the methods and tools we’ve discussed here help solve the problem at hand, it indicates that we’ve encountered a challenge worth deeper investigation — perhaps even an opportunity for innovation, where domain expertise and machine learning can come together to create new solutions.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977.\n“Maximum Likelihood from Incomplete Data via the EM\nAlgorithm.” Journal of the Royal Statistical Society: Series\nB (Methodological) 39 (1): 1–22.\n\n\nDumoulin, Vincent, and Francesco Visin. 2016. “A guide to convolution arithmetic for deep\nlearning.” ArXiv e-Prints, March.\n\n\nEster, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996.\n“A Density-Based Algorithm for Discovering Clusters in Large\nSpatial Databases with Noise.” In Proceedings of the Second\nInternational Conference on Knowledge Discovery and Data Mining,\n226–31. KDD’96. Portland, Oregon: AAAI Press.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic\nProblems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nFukushima, Kunihiko. 1980. “Neocognitron: A Self-Organizing Neural\nNetwork Model for a Mechanism of Pattern Recognition Unaffected by Shift\nin Position.” Biological Cybernetics 36 (4): 193–202. https://doi.org/10.1007/BF00344251.\n\n\nGeron, Aurelien. 2022. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow 3e. 3rd ed. Sebastopol, CA:\nO’Reilly Media.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. MIT Press.\n\n\nHubel, D., and T. Wiesel. 1962. “Receptive Fields, Binocular\nInteraction, and Functional Architecture in the Cat’s Visual\nCortex.” Journal of Physiology 160: 106–54.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems.\nSebastopol, CA: O’Reilly Media.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen\nDatenbasierter Methoden.” Management Center Innsbruck, Course\nMaterial. https://doi.org/10.5281/zenodo.14671708.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012.\n“ImageNet Classification with Deep Convolutional Neural\nNetworks.” In Advances in Neural Information Processing\nSystems, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q.\nWeinberger. Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\n\n\nLandup, David. 2022. Practical Deep Learning for Computer Vision\nwith Python.\n\n\nLloyd, Stuart P. 1982. “Least Squares Quantization in\nPCM.” IEEE Trans. Inf. Theory\n28 (2): 129–36. https://doi.org/10.1109/TIT.1982.1056489.\n\n\nReis, Joe. 2022. Fundamentals of Data Engineering. Sebastopol,\nCA: O’Reilly Media.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\n\nTan, Mingxing, and Quoc Le. 2019.\n“EfficientNet: Rethinking Model Scaling\nfor Convolutional Neural Networks.” In Proceedings of the\n36th International Conference on Machine Learning, edited by\nKamalika Chaudhuri and Ruslan Salakhutdinov, 97:6105–14. Proceedings of\nMachine Learning Research. PMLR. https://proceedings.mlr.press/v97/tan19a.html.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendices/explanations.html",
    "href": "appendices/explanations.html",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "A.1 Wavelet decomposition for cats and dogs\nIn this section we collect more detailed explanations to help clear up some things that are only lightly touched in the main notes.\nIn Section 1.2 we discuss how to use the wavelet transformation1 to transform the image into a different basis. Here are the details of how this is performed, with cat zero as example.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(np.reshape(cats[:, 0], (64, 64)).T, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\nFigure A.1: The original image of cat zero.\nWe use the Haar-Wavelet and we only need to do one level of transformation. As per usual we get four images, each half the resolution, that represent the decomposition. The images are, a downsampled version of the original image, one highlighting the vertical features, one highlighting the horizontal features, and one highlighting the diagonal features.\nFigure A.2: The downsampled version of the image\n\n\n\n\n\n\n\n\n\n\n\nFigure A.3: The vertical highlights of the image.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.4: The horizontal highlights of the image.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.5: The diagonal highlights of the image\nFor our purposes, only the vertical and horizontal feature are of interest, and we combine these two images. In order to make sure the features are highlighted optimal, we need to rescale the images before combining them. For this we use a similar function like the MATLAB wcodemat function.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\nFigure A.6: Combination of vertical and horizontal features unaltered.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.7: Combination of vertical and horizontal features rescaled.\nIn total this leads to the following function to transform a list of images (given as row vectors).\nimport pywt\nimport math\n\ndef img2wave(images):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\nNote, the resulting image has only one forth of the pixels as the original image. We can also visualize the transformation steps as follows in Figure A.8.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#sec-appendix-dvc",
    "href": "appendices/explanations.html#sec-appendix-dvc",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "Show the code for the figure\nimport pywt\n\n[A_1, (cH1, cV1, cD1)] = pywt.wavedec2(np.reshape(cats[:, 0], (64, 64)).T,\n                                       wavelet=\"haar\", level=1)\nplt.figure()\nplt.imshow(A_1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cH1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cD1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\nShow the code for the figure\nimport pywt\n\nplt.figure()\nplt.imshow(cH1 + cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(rescale(cH1, 256) + rescale(cV1, 256), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\nFigure A.8: Workflow to get from the original image to the wavelet transformed version.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#sec-appendix-pvsr",
    "href": "appendices/explanations.html#sec-appendix-pvsr",
    "title": "Appendix A — More detailed explanations",
    "section": "A.2 Precision/Recall trade-off",
    "text": "A.2 Precision/Recall trade-off\nIn Section 2.2 we discuss the performance topics and we come across the si called precision/recall trade-off.\nLets remind ourself of the definitions:\nRecall or true positive rate (TPR) is the rate of relevant instances that are retrieved, or true positive over all occurrences \\[\n\\operatorname{recall} = \\frac{TP}{P} = \\frac{TP}{TP + FN}.\n\\]\nPrecision on the other hand is the rate of relevant instances over all retrieved instances, or true positive over the sum of true positive and false positive. \\[\n\\operatorname{precision} = \\frac{TP}{TP + FP}.\n\\]\nIn order to understand why precision and recall influence each other we need to understand how our classifier works.\nInternally each observation given to the classifier is fed into a decision function that returns a score.\nThe score is on some scale and in the default setting, everything above zero is counted as a match, if the threshold is set differently this can change. See Figure A.9. In the presented example we can have a precision from \\(71\\%\\) to \\(100\\%\\) and at the same time a recall from \\(100\\%\\) to \\(60\\%\\).\n\n\n\n\n\n\nFigure A.9: Some representatives and their score and three different thresholds and the corresponding results for precision and recall.\n\n\n\n\n\nCode that provides the basis for the above figure.\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import SGDClassifier\nnp.random.seed(6020)\n\nmnist = fetch_openml('mnist_784', as_frame=False)\n\nX_train, X_test = mnist.data[:60000], mnist.data[60000:]\ny_train, y_test = mnist.target[:60000], mnist.target[60000:]\n\ny_train_5 = (y_train == \"5\")\ny_test_5 = (y_test == \"5\")\n\nSGD = SGDClassifier(random_state=6020)\nSGD.fit(X_train, y_train_5)\n\nindices = [22698, 2, 73, 132, 244, 50, 48, 11, 0, 26873]\nSGD.decision_function(X_train[indices])\n\nim = np.zeros((1 * 28, 10 * 28), int)\nfor i, j in enumerate(indices):\n    im[:28, 28*i: 28*(i+1)] = X_train[j].reshape(28,28)\n\nplt.imshow(im, cmap=\"binary\")\nplt.axis(\"off\")\n\n\nWe can also plot the entire precision/recall curves\n\n\nShow the code for the figure\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import cross_val_predict\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ny_scores = cross_val_predict(SGD, X_train, y_train_5, cv=5,\n                             method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\nplt.figure()\nplt.plot(thresholds, precisions[:-1], label=\"Precision\")\nplt.plot(thresholds, recalls[:-1], \"--\", label=\"Recall\")\nplt.xlim([-95000, 22000])\nplt.xlabel(\"score\")\nplt.grid()\nplt.legend()\nplt.gca().set_aspect( 117000 / 3)\n\nplt.figure()\nplt.plot(recalls, precisions)\nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.grid()\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.show()\n\n\n\n\n\n\n\n\nFigure A.10: Precision and recall vs. the score of the decision function.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.11: Precision vs. recall.\n\n\n\n\n\nWith the help of the precision vs. recall curve we can select a threshold appropriate for our classification, i.e. level between precision and recall as we see fit and our classification allows.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#sec-appendix-softmax",
    "href": "appendices/explanations.html#sec-appendix-softmax",
    "title": "Appendix A — More detailed explanations",
    "section": "A.3 Details on softmax",
    "text": "A.3 Details on softmax\nFor Neural Networks (NNs) a common activation function is the so called softmax function, defined as \\[\n\\sigma: \\mathbb{R^n} \\to (0, 1)^n, \\quad \\sigma(x)_i = \\frac{\\exp(x_i)}{\\sum_j{\\exp(x_j)}}.\n\\]\nIn this section we are going to try explain in more detail what it does and what it is used for.\nThis particular activation function is most commonly used as the output layer of an NN for a multi-class classification/learning problem, as it is the case in our dogs vs. cats example.\nLoosely speaking, the main idea is, that it transforms a vector into probabilities. By scaling via the sum of all entries we end with a total sum of 1. This makes us independent from the actual scaling the previous layers of the network worked with and lands us alway in the interval \\([0, 1]\\). The probability we get can be interpreted as the confidence of the network in the classification per class.\nLets look at it via an example, we assume we have a simple NN with only the two layers, the last being our softmax layer.\n\n\n\n\n\n\nFigure A.12: A small NN for classifying hand written digits, we only distinguish between 5, 6 and all others.\n\n\n\nWe use the hand written letters from Section A.2 as our example, where we only have three classes: 5, 6, else. Here else should be understood as neither 5 nor 6. We classify the images from right to left. The output of our fist layer is a number with no apparent scaling, after the softmax we can see the most likely class suggested from our NN.\n\n\n\n\n\n\nFigure A.13: From several images to the output of the NN.\n\n\n\nAs we can see, the exponential scaling makes sure that we separate quite well and we can easily destinguish between the three classes We can also see, when it is very close and the NN is not very sure what the correct class is.\n\n\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#footnotes",
    "href": "appendices/explanations.html#footnotes",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "see Kandolf (2025), Section 10 or follow the direct link↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/keras.html",
    "href": "appendices/keras.html",
    "title": "Appendix B — Our first Neural Network in tensorflow.keras",
    "section": "",
    "text": "B.1 How to save a keras model\nAs mentioned there are multiple frameworks to work neural networks (NNs) and we focus on pytorch. Nevertheless, tensorflow.keras is often used in industry and to give an idea how it works we also implemented the first model in this framework.\nIn this section we will focus on keras with TensorFlow as backend. This means we find our main modules under tensorflow.keras. We will, in general, reference the documentation via tensorflow, the content on keras.io Api Docs is mostly equivalent. Where some tutorials and examples might differ.\nIf we translate it into a tensorflow.keras model this looks like the following code (already including a summary at the end).\nIt is often also quite useful to export the model as an image. In our case we can work again with a dot file rendered as Figure B.1.\nNext, we need to compile the model to prepare it for execution/training. In this step we define the optimizer to use for solving the included optimization method and how loss is computed. After this we can\nAfter we compiled the model we can train it using our data\nNow that training is complete we can have a look at the performance.\nOverall, we have an accuracy of 83.75% for our test set, better than with our linear models.\nOf course we can save a keras model with the methods discussed in Chapter 4 but it is more convenient to use the dedicated backend from keras, see docs.\nIn short, we only need to call .save(file) to save in the default .keras format. This is also the recommended format to use. There also exist a tf and h5 format, note that for the last we need to install the Python package h5py 1. The tf version might execute arbitrary code during loading the model, be careful when using it, see Section 4.2\nIt is also possible to implement continuous checkpoints during training. This allows us to resume training after an interrupt or simply store the model at the end of the training.\nAll of this can be done via the already discussed dvclive interface, the implementation is defined as an exercise.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Our first Neural Network in `tensorflow.keras`</span>"
    ]
  },
  {
    "objectID": "appendices/keras.html#how-to-save-a-keras-model",
    "href": "appendices/keras.html#how-to-save-a-keras-model",
    "title": "Appendix B — Our first Neural Network in tensorflow.keras",
    "section": "",
    "text": "model.save(\"model.keras\")\nloaded_model = tf.keras.saving.load_model(\"model.keras\")\n\n\n\n\n\n\n\n\n\nExercise B.4 (dvclive integration into the keras training) Implement the DVCLive integration to track the metrics.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Our first Neural Network in `tensorflow.keras`</span>"
    ]
  },
  {
    "objectID": "appendices/keras.html#footnotes",
    "href": "appendices/keras.html#footnotes",
    "title": "Appendix B — Our first Neural Network in tensorflow.keras",
    "section": "",
    "text": "HDF5 is a file format for storing large complex heterogeneous data often used for large simulations in High Performance Computing, see details on the project page hdfgroup.org.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Our first Neural Network in `tensorflow.keras`</span>"
    ]
  }
]