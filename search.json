[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "",
    "text": "Preface\nThese are the lecture notes for the Maschinelles Lernen in der industriellen Bildverarbeitung class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the summer term 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning in Industrial Image Processing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nThese notes are built with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basic concepts behind modern day Data Science techniques for industrial image processing. We will always try to not only discuss the theory but also use Python to illustrate the content programmatically.\nThe plan for this notes it to cover not only the basics of modern methods for machine learning in image processing but also topics in the near vicinity like\n\nData management, labeling and preprocessing\nLife-cycle management of models\nmachine learning operations (MLOps)\ncurrent and research topics\n\nThese notes are intended for engineering students and therefore the mathematical concepts will rarely include rigorous proofs.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "clustering/index.html",
    "href": "clustering/index.html",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Feature selection and generation of a feature space\nOne aspect if machine learning is binning data into meaningful and distinct categories that can be used for decision making. This is often called clustering and classification. To achieve this task the main modus of operation is to find a low-rank feature space that is informative and interpretable.\nOne way is to extract the dominate features in a data set are for example the singular value decomposition (SVD) or principal component analysis, see (Kandolf 2025). The idea is to not work in the heigh dimensional measurement space, but instead in the low rank feature space. This allows us to significantly reduce the cost by performing the our analysis in this low dimensional space.\nIn order to find the feature space there are two main paths for machine learning:\nAs the names suggest in unsupervised learning no labels are given to the algorithm and it must find patterns in the data to generate clusters and labels such that predictions can be made. This is often used to discover previously unknown patterns in the (low-rank) subspace of the data and leads to feature engineering or feature extraction. These features can than be used for building a model.\nSupervised learning, on the other hand, uses labelled data sets. The training data is labelled for cross-validated by a teacher or expert. I.e. the input and output for a model is explicitly provided and regression methods are used to find the best model for the given labelled data. There are several different form of supervised learning like reinforcement learning, semi-supervised learning, or active learning.\nThe main idea behind machine learning is to construct or exploit the intrinsic low-rank feature space of a given data set, how to find these is determined by the algorithm.\nBefore we go into specific algorithms lets have a general look at what we are going to talk about. We will follow (Brunton and Kutz 2022, sec. 5.1) for this introduction.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "href": "clustering/index.html#feature-selection-and-generation-of-a-feature-space",
    "title": "Clustering and Classification",
    "section": "",
    "text": "Fischer Iris data set\nFirst we look into one of the standard data sets well known in the community, the so called Fischer Iris data set FISHER (1936).\n\n\n\n\n\n\nNote\n\n\n\nLuckily for us there are several data packages available for general use. One of them is the Fisher Iris data set.\n\n\n\n\n\n\nFigure 1: Illustration of the recorded features and structure of the iris dataset from the Medium article - Exploring the Iris flower dataset.\n\n\n\nWe use the possibilities of sklearn 1 to access the dataset in Python.\n\n\n\n\nShow the code for the figure\nimport plotly.express as px\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nfig = px.scatter_3d(df, x='sepal length (cm)', y='sepal width (cm)',\n                    z='petal width (cm)', color='target')\ncamera = dict(\n    eye=dict(x=2, y=2, z=0.1)\n)\nfig.update_layout(scene_camera=camera)\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a scatter 3D plot along the features sepal length, sepal width, and petal width, the forth feature petal width is not shown.\n\n\n\n\nAs can be seen in Figure 2 the three features are enough to easily separate Sentosa and to a very good degree Versicolor from Virginica, where the last two have a small overlap in the samples provided.\nThe petal length seems to be the best feature for the classification and no highly sophisticated machine learning algorithms are required. We can see the this even better if we use a so called Pair plot from the seaborn package as seen in Figure 3.\n\n\nShow the code for the figure\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n%config InlineBackend.figure_formats = [\"svg\"]\n\n_ = sns.pairplot(iris.frame, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\nFigure 3: Iris dataset with 150 samples from three distinct species, Setosa, Versicolor, and Virginica, as a pair grid plot. The diagonal shows the univariate distribution of the feature.\n\n\n\n\n\nHere a grid with plots is created where each (numeric) feature is shared across a row and a column. Therefore, we can see the combination of two and in the diagonal we see the univariate distribution of the values. After a close inspection we can see that petal width provides us with the best distinction feature and sepal width with the worst.\nEven though we already have enough to give a good classification for this dataset it is worth to investigate a bit further in order to illustrate how, for example the principal component analysis (PCA) (see Kandolf 2025, sec. 4.2) Link can be applied here.\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nA = df.iloc[: , :4].to_numpy()\n\nXavg = A.mean(0)\nB = A - np.tile(Xavg, (150, 1))\nU, S, VT = np.linalg.svd(B, full_matrices=False)\nC = B @ VT[:2, :].T\nq = np.linalg.norm(S[:2])**2 / np.linalg.norm(B, \"fro\")**2\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(C[index, 0], C[index, 1], label=name)\nplt.legend()\nplt.xlabel(r\"$PC_1$\")\nplt.ylabel(r\"$PC_2$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Feature reduction with PCA for the Iris dataset. We show the first two components principal components.\n\n\n\n\n\nAs can be seen in the ?fig-clustering-pca the first two components cover about 97.77% of the total variation in the samples and in accordance with this result a very good separation of the three species.\nNow let us take a look at a more complicated example from Brunton and Kutz (2022).\n\n\nDogs and Cats\nIn this data set we explore how distinguish between dogs and cats. The data set contains \\(80\\) images of dogs and \\(80\\) images of cats, each with \\(64 \\times 64\\) pixels and therefore a total of \\(4096\\) feature measurements per image.\n\n\n\n\n\n\nImportant\n\n\n\nThe following data set and the basic structure of the code is from (Brunton and Kutz 2022, Code 5.2 - 5.4). Also see GitHub.\n\n\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\ndef createImage(data, x, y, width=64, length=64):\n    image = np.zeros((width * x, length * y))\n    counter = 0\n    for i in range(x):\n        for j in range(y):\n            img = np.flipud(np.reshape(data[:, counter], (width, length)))\n            image[length * i: length * (i + 1), width * j: width * (j + 1)] \\\n                    = img.T\n            counter += 1\n    return image\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(createImage(cats, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/dogData.mat\")\ndogs = scipy.io.loadmat(io.BytesIO(response.content))[\"dog\"]\nplt.figure()\nplt.imshow(createImage(dogs, 4, 4), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\nFigure 5: The first 16 cats from the data set.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The first 16 dogs from the data set.\n\n\n\n\n\n\nAgain, we use PCA to reduce the feature space of our data.\n\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nCD = np.concatenate((dogs, cats), axis=1)\nU, S, VT = np.linalg.svd(CD - np.mean(CD), full_matrices=False)\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U[:, j], (64, 64)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 7: First four modes of the PCA of the 160 images.\n\n\n\nIn Figure 7 we can see the first four modes of the PCA. While mode 2 (Figure 7 (b)) especially features the pointy ears common in cats, mode 3 (Figure 7 (c)) is more dog like. Consequently, will the two different animals have either add or subtract this feature form the mean.\nOf course this is not the only possible representation of the initial data. As discussed in (Kandolf 2025, sec. 10) Link we can also use the Wavelet transform.\nThe multi resolution analysis features of the Wavelet transformation can be used to transform the image in such a way that the resulting principal components yield a better separation of the two classes.\n\n\n\n\n\n\nNote\n\n\n\nWe use the following workflow to get from the original image \\(A_0\\) to the \\(\\tilde{A}\\) image in a sort of Wavelet basis.\n\n\n\n\n\n\nFigure 8: Workflow to get from the original image to the wavelet transformed version.\n\n\n\nIn short, we combine the vertical and horizontal rescaled feature images, as we are most interested in edge detection for this example.\nNote that the image has now only a quarter of pixels as the original image.\nFor a more detailed explanation see @#appendix-dvc.\n\n\nShow code required for the transformation.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\n\nimport pywt\nimport math\n\ndef img2wave(data):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\n\n\n\n\nLet us try how this changes our first four modes.\n\nShow the code for the figures\nCD_w = img2wave(CD)\nU_w, S_w, VT_w = np.linalg.svd(CD_w - np.mean(CD_w), full_matrices=False)\nl = math.isqrt(CD_w[:, 0].shape[0])\n\nfor j in range(4):\n    plt.figure()\n    U_ = np.flipud(np.reshape(U_w[:, j], (l, l)))\n    plt.imshow(U_.T, cmap=plt.get_cmap(\"gray\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First principal component.\n\n\n\n\n\n\n\n\n\n\n\n(b) Second principal component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Third principal component.\n\n\n\n\n\n\n\n\n\n\n\n(d) Forth principal component.\n\n\n\n\n\n\n\nFigure 9: First four modes of the PCA of the 160 images in our wavelet basis.\n\n\n\nRight away we can see that the ears and eyes of the cats show up more pronounced in the second mode Figure 9 (b).\nNow how does this influence our classification problem? For this let us first see how easy it is to distinguish the two classes in a scatter plot for the first three modes.\n\nShow the code for the figure\nimport seaborn as sns\nimport pandas as pd\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\ntarget = [\"cat\"] * 80 + [\"dog\"] * 80\nC = U[:, :4].T @ (CD - np.mean(CD))\ndf = pd.DataFrame(C.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf[\"target\"] = target\n\nC_w = U_w[:, :4].T @ (CD_w - np.mean(CD))\ndf_w = pd.DataFrame(C_w.T, columns=[\"PV1\", \"PV2\", \"PV3\", \"PV4\"])\ndf_w[\"target\"] = target\n\n_ = sns.pairplot(df, hue=\"target\", height=1.45)\n_ = sns.pairplot(df_w, hue=\"target\", height=1.45)\n\n\n\n\n\n\n\n\n\n\nFigure 10: First four modes of the raw images for cats and dogs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: First four modes of the wavelet images for cats and dogs.\n\n\n\n\n\n\nThe two figures above give us a good idea what the different principal values can tell us. It is quite clear, that the first mode is not of much use for differentiation of the two classes. In general the second mode seems to work best (the ears), in contrast to the raw image we can also see that the wavelet basis is slightly better for the separation.\n\n\nShow the code for the figures\nimport plotly.express as px\n\n\nfig = px.scatter_3d(df, x='PV2', y='PV3', z='PV4', color=\"target\")\nfig.show()\nfig = px.scatter_3d(df_w, x=\"PV2\", y=\"PV3\", z=\"PV4\", color=\"target\")\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 12: Scatter plots of modes 2 to 4 for the raw images.\n\n\n\n\n\n\n                                                \n\n\nFigure 13: Scatter plots of modes 2 to 4 for the wavelet images.\n\n\n\n\nNow that we have an idea what we are dealing with we can start looking into the two previously described paths in machine learning. The difference between supervised and unsupervised learning can be summarized by the following two images.\n\nShow the code for the figure\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\niris = load_iris(as_frame=True)\niris.frame[\"target\"] = iris.target_names[iris.target]\ndf = iris.frame\n\nplt.figure()\nfor name in iris.target_names:\n    index = df[\"target\"] == name\n    plt.scatter(df.iloc[:, 0][index], df.iloc[:, 2][index], label=name)\n\nplt.figure()\nplt.scatter(df.iloc[:, 0], df.iloc[:, 2], color=\"gray\")\n\n\n\n\n\n\n\n\n\n\nFigure 14: Labelled data for supervised learning.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Unlabelled data for unsupervised learning.\n\n\n\n\n\n\nYou either provide/have labels or not.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nFISHER, R. A. 1936. “THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS.” Annals of Eugenics 7 (2): 179–88. https://doi.org/https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen Datenbasierter Methoden.” Management Center Innsbruck, Course Material. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "clustering/index.html#footnotes",
    "href": "clustering/index.html#footnotes",
    "title": "Clustering and Classification",
    "section": "",
    "text": "To add it to pdm us pdm add scikit-learn.↩︎",
    "crumbs": [
      "Clustering and Classification"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "T.B.D.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nFISHER, R. A. 1936. “THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC\nPROBLEMS.” Annals of Eugenics 7 (2): 179–88.\nhttps://doi.org/https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nKandolf, Peter. 2025. “MECH-m-DUAL-1-DBM - Grundlagen\nDatenbasierter Methoden.” Management Center Innsbruck, Course\nMaterial. https://doi.org/10.5281/zenodo.14671708.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendices/explanations.html",
    "href": "appendices/explanations.html",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "A.1 Wavelet decomposition for cats and dogs\nIn Section 1.2 we discuss using the wavelet transformation to transform the image into a different basis. Here are the details of how this is performed with cat zero as example.\nShow the code for the figure\nimport numpy as np\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/dynamicslab/databook_python/\"\n    \"raw/refs/heads/master/DATA/catData.mat\")\ncats = scipy.io.loadmat(io.BytesIO(response.content))[\"cat\"]\n\nplt.figure()\nplt.imshow(np.reshape(cats[:, 0], (64, 64)).T, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\nFigure A.1: The original cat.\nWe use the Haar-Wavelet and we only need to do one level of transformation. As per usual we get four images, each half the resolution, that represent the decomposition. The images are, a downsampled version of the original image, one highlighting the vertical features, one highlighting the horizontal features, and one highlighting the diagonal features.\nFigure A.2: Wavelet transformation of the cat.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure A.4\n\n\n\n\n\n\n\n\n\n\n\nFigure A.5\nFor our purposes only the vertical and horizontal feature are of interest and we combine these two images. In order to make sure the features are highlighted optimal we need to rescale the images before the combination. For this we use a similar function like the MATLAB wcodemat function.\ndef rescale(data, nb):\n    x = np.abs(data)\n    x = x - np.min(x)\n    x = nb * x / np.max(x)\n    x = 1 + np.fix(x)\n    x[x&gt;nb] = nb\n    return x\nFigure A.6: Combination of vertical and horizontal features unaltered.\n\n\n\n\n\n\n\n\n\n\n\nFigure A.7: Combination of vertical and horizontal features rescaled.\nIn total this leads to the following function to transform a list of the images, given as row vectors.\nimport pywt\nimport math\n\ndef img2wave(images):\n    l, w = data.shape\n    data_w = np.zeros((l // 4, w))\n    for i in range(w):\n        A = np.reshape(data[:, i], (math.isqrt(l), math.isqrt(l)))\n        [A_1, (cH1, cV1, cD1)] = pywt.wavedec2(A, wavelet=\"haar\", level=1)\n        data_w[:, i] = np.matrix.flatten(rescale(cH1, 256) + rescale(cV1, 256))\n    return data_w\nNote that the resulting image has only one forth of the pixels as the original image. We can also visualize the transformation steps as follows in Figure A.8.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  },
  {
    "objectID": "appendices/explanations.html#appendix-dvc",
    "href": "appendices/explanations.html#appendix-dvc",
    "title": "Appendix A — More detailed explanations",
    "section": "",
    "text": "Show the code for the figure\nimport pywt\n\n[A_1, (cH1, cV1, cD1)] = pywt.wavedec2(np.reshape(cats[:, 0], (64, 64)).T,\n                                       wavelet=\"haar\", level=1)\nplt.figure()\nplt.imshow(A_1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cH1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(cD1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\nShow the code for the figure\nimport pywt\n\nplt.figure()\nplt.imshow(cH1 + cV1, cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\nplt.figure()\nplt.imshow(rescale(cH1, 256) + rescale(cV1, 256), cmap=plt.get_cmap(\"gray\"))\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\nFigure A.8: Workflow to get from the original image to the wavelet transformed version.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>More detailed explanations</span>"
    ]
  }
]