# Model persistence {#sec-data-mp}

So far we either loaded a data set or generated it on the fly for excursion into classification.
Therefore, we should start by looking into ways to persist the models we generated so far.

The general idea is to simply store the object we generate and load it at some later time. 
Nevertheless, this can be quite tricky. 

For example it might be that you do your training in a different environment than the evaluation or prediction.
It might even be the case, that you switch programming language for these tasks.

As we mainly worked with `scikit-learn` we should check the documentation for a start [docs - model persistence](https://scikit-learn.org/stable/model_persistence.html).

Let us use the following toy example with our cats and dogs as reference. 
```{python}
#| lst-label: lst-data-mp-toyexample
#| lst-cap: Code for the toy example
#| classes: styled-output

import numpy as np
import scipy
import requests
import io
import sklearn
from sklearn import svm
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
import sklearn.metrics
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC

%config InlineBackend.figure_formats = ["svg"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

X_train = np.concatenate((cats_w[:60, :], dogs_w[:60, :]))
y_train = np.repeat(np.array([1, -1]), 60)
X_test = np.concatenate((cats_w[60:80, :], dogs_w[60:80, :]))
y_test = np.repeat(np.array([1, -1]), 20)

voting_clf = make_pipeline(
    PCA(n_components=41),
    VotingClassifier(
        estimators=[
            ("lda", LinearDiscriminantAnalysis()),
            ("rf", RandomForestClassifier(
                n_estimators=500,
                max_leaf_nodes=2,
                random_state=6020)),
            ("svc", SVC(
                kernel="linear",
                probability=True,
                random_state=6020)),
        ],
        flatten_transform=False,
    )
)

voting_clf.fit(X_train, y_train)
score = voting_clf.score(X_test, y_test)
print(f"We have a hard voting score of {score}")
```

## Open Neural Network Exchange - ONNX

> ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers. [LEARN MORE](https://onnx.ai/about.html)
>
> [https://onnx.ai/](https://onnx.ai/)

The use-case for ONNX is when the persisted model is used without necessarily using the Python object itself.
This is especially the case when the runtime for distributing the model is not Python.

Now let us see how we can persist the model of @lst-data-mp-toyexample as an ONNX. 

```{python}
#| classes: styled-output
from skl2onnx import to_onnx
onx = to_onnx(voting_clf, X_train[:1].astype(np.int64))                 # <1>
with open("model.onnx", "wb") as f:
    f.write(onx.SerializeToString())
```
1. Not all data types are supported, so we need to convert to `int64`.

As mentioned, the file format is binary so it does not make a lot of sense to actually read the image in plain text but we can have a look at the size
```{python}
#| classes: styled-output
#| echo: false
%%bash
du -h model.onnx
```
which is not very large.

Unfortunately, there is no method to convert back to our sklearn model.
What we can use it in the `onnxruntime` and see if we still get the same score:
```{python}
#| classes: styled-output
import onnxruntime as ort

model = ort.InferenceSession("model.onnx")
input_name = model.get_inputs()[0].name
predictions = model.run(None, {input_name: X_test.astype(np.int64)})

score = sklearn.metrics.accuracy_score(y_test, predictions[0])
print(f"We have a score of for {score} for the recovered model.")
```

```{python}
#| echo: false
%%bash
# Code to remove above files
rm model.onnx
```
As we can see, the score is actually better than before. 

::: {.callout-important}
This is due to the fact, that `skl2onnx` is not able to convert all `sklearn` models.
This is especially true for the `SVC` class included in our composite model.
:::

Furthermore, if we inspect our predictions output from above a bit more it looks like we have switched to soft voting.

Overall, we can see that ONNX is a way to persist a model such that we can make predictions with it but we do no longer have the Python object.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-mp-onnx}

## Test how the recovery works for SVC

Try to rewrite the model and check the resulting score after recovery vs. the original score for the following modifications.

1. Remove the `probability=True` for `SVC`.
1. Replace `SVC` by `LinearSVC`.
1. Remove the `SVC` all together and replace it with a `LogisticRegression` classifier.

::::
:::

### `pickle` - Python object serialization

We can also swing the pendulum in the other direction and use the Python Standard Library
[`pickle`](https://docs.python.org/3/library/pickle.html#module-pickle) to persist a model. 

Before we go into more details we should emphasise the potential security problem included with `pickle` as stated in its own docs:

::: {.callout-important}

## The `pickle` module is not secure. Only _unpickle_ data you trust.

It is possible to construct malicious pickle data which will **execute arbitrary code during unpickling**. Never unpickle data that could have come from an untrusted source, or that could have been tampered with.

Consider signing data with [`hmac`](https://docs.python.org/3/library/hmac.html#module-hmac) if you need to ensure that it has not been tampered with.

Safer serialization formats such as [`json`](https://docs.python.org/3/library/json.html#module-json) may be more appropriate if you are processing untrusted data. See [Comparison with json](https://docs.python.org/3/library/pickle.html#comparison-with-json).
:::

As `pickle` is the native implementation in Python.
It is easy to use and works for (almost) all models and configurations.
The downside is, that we need to absolutely trust the source of our model and where it was stored as well as the different steps it takes to arrive in our storage.

Furthermore, the environment we load the model into needs to be the same as the one we stored it from.
As we have already seen how the _dependency hell_[^dh] influences our development, we bring theses issues with us.

It is not guaranteed that a model can be loaded with a different `scikit-learn` version or let alone a different `numpy` version that is only a sub-dependency of `scikit-learn`.
Furthermore, if a different hardware is involved there might be problems as well. 
As a consequence, if we use `pickle` a thorough version control with package management is key!

If we have a model that moves around different processes via the disc or is restored frequently from storage but can not be permanently in storage and therefore performance for loading and storing is of interest we can also use [`joblib`](https://joblib.readthedocs.io/en/latest/index.html#module-joblib).

[^dh]: see [MECH-M-DUAL-1-SWD, Section 4.1](https://kandolfp.github.io/MECH-M-DUAL-1-SWD/basics/epilogue.html#sec-intro-pm-reproducibility)

Now let us see how we can persist the model of @lst-data-mp-toyexample as an `pickle`. 

```{python}
#| classes: styled-output
from pickle import dump
with open("model.pkl", "wb") as f:
    dump(voting_clf, f, protocol=5)
```

As mentioned, the file format is binary so it does not make a lot of sense to actually read the image in plain text but we can have a look at the size
```{python}
#| classes: styled-output
#| echo: false
%%bash
du -h model.pkl
```
and we can see that the storage demands are slightly higher than for ONNX.

We restore the model via
```{python}
#| classes: styled-output
from pickle import load
with open("model.pkl", "rb") as f:
    clf = load(f)
score = clf.score(X_test, y_test)
print(f"We have score of {score} after loading the object again.")
```
```{python}
#| echo: false
%%bash
# Code to remove above files
rm model.pkl
```
As we can see, the score stays the same. 


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-mp-pickle}

## Further investigations for `pickle`

1. For the loaded model, switch to soft voting by calling
   ```{.python}
   clf[1].voting = "soft"
   clf[1].named_estimators["svc"].probability = True
   clf.fit(X_train, y_train)
   clf.score(X_test, y_test)
   ```
1. Use [`joblib`](https://joblib.readthedocs.io/en/latest/index.html#module-joblib) to persist and load the module, also check the file size.
1. Switch for the `SVC` to a `"rbf"` kernel and see if you can fully recover the object
1. Some user defined functions can cause problems for `pickle` try persisting the model with [`cloudpickle`](https://github.com/cloudpipe/cloudpickle) and test with the kernel function `rbf = lambda x, y: np.exp(1e-2 * np.abs(x@y.T))`.

::::
:::
