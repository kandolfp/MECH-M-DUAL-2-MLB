# Model persistence {#sec-data-mp}

So far we either loaded a data set or generated it on the fly for excursion into classification.
Therefore, we should start by looking into ways to persist the models we generated so far.

The general idea is to simply store the object we generate and load it at some later time. 
Nevertheless, this can be quite tricky. 

For example it might be that you do your training in a different environment than the evaluation or prediction.
It might even be the case, that you switch programming language for these tasks.

As we mainly worked with `scikit-learn` we should check the documentation for a start [docs - model persistence](https://scikit-learn.org/stable/model_persistence.html).

Let us use the following toy example with our cats and dogs as reference. 
```{python}
#| lst-label: lst-data-mp-toyexample
#| lst-cap: Code for the toy example
#| classes: styled-output

import numpy as np
import scipy
import requests
import io
import sklearn
from sklearn import svm
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
import sklearn.metrics
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC

%config InlineBackend.figure_formats = ["svg"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/catData_w.mat")
cats_w = scipy.io.loadmat(io.BytesIO(response.content))["cat_wave"]

response = requests.get(
    "https://github.com/dynamicslab/databook_python/"
    "raw/refs/heads/master/DATA/dogData_w.mat")
dogs_w = scipy.io.loadmat(io.BytesIO(response.content))["dog_wave"]

X_train = np.concatenate((cats_w[:60, :], dogs_w[:60, :]))
y_train = np.repeat(np.array([1, -1]), 60)
X_test = np.concatenate((cats_w[60:80, :], dogs_w[60:80, :]))
y_test = np.repeat(np.array([1, -1]), 20)

voting_clf = make_pipeline(
    PCA(n_components=41),
    VotingClassifier(
        estimators=[
            ("lda", LinearDiscriminantAnalysis()),
            ("rf", RandomForestClassifier(
                n_estimators=500,
                max_leaf_nodes=2,
                random_state=6020)),
            ("svc", SVC(
                kernel="linear",
                probability=True,
                random_state=6020)),
        ],
        flatten_transform=False,
    )
)

voting_clf.fit(X_train, y_train)
score = voting_clf.score(X_test, y_test)
print(f"We have a hard voting score of {score}")
```

::: {.callout-note}
In the next couple of exercises we create different version of our model and persist it to storage.
Try to keep track of what model version corresponds to which exercise/code block.
:::

## Open Neural Network Exchange - ONNX

> ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers. [LEARN MORE](https://onnx.ai/about.html)
>
> Source: [https://onnx.ai/](https://onnx.ai/), accessed 07.03.2025.

The use-case for ONNX is when the persisted model is used without necessarily using the Python object itself.
This is especially the case when the runtime for distributing the model is not Python.

Now let us see how we can persist the model of @lst-data-mp-toyexample as an ONNX. 

```{python}
#| classes: styled-output
from skl2onnx import to_onnx
onx = to_onnx(voting_clf, X_train[:1].astype(np.int64))                 # <1>
with open("model.onnx", "wb") as f:
    f.write(onx.SerializeToString())
```
1. Not all data types are supported, so we need to convert to `int64`.

As mentioned, the file format is binary so it does not make a lot of sense to actually read the image in plain text but we can have a look at the size
```{python}
#| classes: styled-output
#| echo: false
%%bash
du -h model.onnx
```
which is not very large.

Unfortunately, there is no method to convert back to our sklearn model.
What we can use it in the `onnxruntime` and see if we still get the same score:
```{python}
#| classes: styled-output
import onnxruntime as ort

model = ort.InferenceSession("model.onnx")
input_name = model.get_inputs()[0].name
predictions = model.run(None, {input_name: X_test.astype(np.int64)})

score = sklearn.metrics.accuracy_score(y_test, predictions[0])
print(f"We have a score of for {score} for the recovered model.")
```

```{python}
#| echo: false
%%bash
# Code to remove above files
rm model.onnx
```
As we can see, the score is actually better than before. 

::: {.callout-important}
This is due to the fact, that `skl2onnx` is not able to convert all `sklearn` models.
This is especially true for the `SVC` class included in our composite model.
:::

Furthermore, if we inspect our predictions output from above a bit more it looks like we have switched to soft voting.

Overall, we can see that ONNX is a way to persist a model such that we can make predictions with it but we do no longer have the Python object.

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-mp-onnx}

## Test how the recovery works for SVC

Try to rewrite the model and check the resulting score after recovery vs. the original score for the following modifications.

1. Remove the `probability=True` for `SVC`.
1. Replace `SVC` by `LinearSVC`.
1. Remove the `SVC` all together and replace it with a `LogisticRegression` classifier.

::::
:::

### `pickle` - Python object serialization

We can also swing the pendulum in the other direction and use the Python Standard Library
[`pickle`](https://docs.python.org/3/library/pickle.html#module-pickle) to persist a model. 

Before we go into more details we should emphasise the potential security problem included with `pickle` as stated in its own docs:

::: {.callout-important}

## The `pickle` module is not secure. Only _unpickle_ data you trust.

It is possible to construct malicious pickle data which will **execute arbitrary code during unpickling**. Never unpickle data that could have come from an untrusted source, or that could have been tampered with.

Consider signing data with [`hmac`](https://docs.python.org/3/library/hmac.html#module-hmac) if you need to ensure that it has not been tampered with.

Safer serialization formats such as [`json`](https://docs.python.org/3/library/json.html#module-json) may be more appropriate if you are processing untrusted data. See [Comparison with json](https://docs.python.org/3/library/pickle.html#comparison-with-json).
:::

As `pickle` is the native implementation in Python.
It is easy to use and works for (almost) all models and configurations.
The downside is, that we need to absolutely trust the source of our model and where it was stored as well as the different steps it takes to arrive in our storage.

Furthermore, the environment we load the model into needs to be the same as the one we stored it from.
As we have already seen how the _dependency hell_[^dh] influences our development, we bring theses issues with us.

It is not guaranteed that a model can be loaded with a different `scikit-learn` version or let alone a different `numpy` version that is only a sub-dependency of `scikit-learn`.
Furthermore, if a different hardware is involved there might be problems as well. 
As a consequence, if we use `pickle` a thorough version control with package management is key!

If we have a model that moves around different processes via the disc or is restored frequently from storage but can not be permanently in storage and therefore performance for loading and storing is of interest we can also use [`joblib`](https://joblib.readthedocs.io/en/latest/index.html#module-joblib).

[^dh]: see [MECH-M-DUAL-1-SWD, Section 4.1](https://kandolfp.github.io/MECH-M-DUAL-1-SWD/basics/epilogue.html#sec-intro-pm-reproducibility)

Now let us see how we can persist the model of @lst-data-mp-toyexample as an `pickle`. 

```{python}
#| classes: styled-output
from pickle import dump
with open("model.pkl", "wb") as f:
    dump(voting_clf, f, protocol=5)
```

As mentioned, the file format is binary so it does not make a lot of sense to actually read the image in plain text but we can have a look at the size
```{python}
#| classes: styled-output
#| echo: false
%%bash
du -h model.pkl
```
and we can see that the storage demands are slightly higher than for ONNX.

We restore the model via
```{python}
#| classes: styled-output
from pickle import load
with open("model.pkl", "rb") as f:
    clf = load(f)
score = clf.score(X_test, y_test)
print(f"We have score of {score} after loading the object again.")
```
```{python}
#| echo: false
%%bash
# Code to remove above files
rm model.pkl
```
As we can see, the score stays the same. 


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-mp-pickle}

## Further investigations for `pickle`

1. For the loaded model, switch to soft voting by calling
   ```{.python}
   clf[1].voting = "soft"
   clf[1].named_estimators["svc"].probability = True
   clf.fit(X_train, y_train)
   clf.score(X_test, y_test)
   ```
1. Use [`joblib`](https://joblib.readthedocs.io/en/latest/index.html#module-joblib) to persist and load the module, also check the file size.
1. Switch for the `SVC` to a `"rbf"` kernel and see if you can fully recover the object
1. Some user defined functions can cause problems for `pickle` try persisting the model with [`cloudpickle`](https://github.com/cloudpipe/cloudpickle) and test with the kernel function `rbf = lambda x, y: np.exp(1e-2 * np.abs(x@y.T))`.

::::
:::

## `skops.io` - the more secure Python alternative

As an alternative to `pickle` we can use [`skops.io`](https://skops.readthedocs.io/en/stable/persistence.html#persistence) as a more secure alternative. 
It is developed as a secure alternative for `pickle` and therefore supports a wide range of objects.
The main idea is, that only _trusted_ functions are loaded and not anything included in the file.
It is also possible to verify our data before load it, increasing the security further.
Still, it returns the Python object, if it can be loaded and we can manipulate it as with `pickle`.

As a downside, the process is slower and some user defined functions/object might not work as desired.
This also implies, that we also need to have the same environment for loading as we had for storing the Python object.

The interface itself is simple and orients itself at `pickle`.

```{python}
#| classes: styled-output
import skops.io as sio
obj = sio.dump(voting_clf, "model.skops")
```

For comparison, we show the size of the file
```{python}
#| classes: styled-output
#| echo: false
%%bash
du -h model.skops
```
and we can see that this format has a significant higher overhead as the other formats. 

Retrieving the model is a two step process, first loading the _untrusted types_ and than loading the verified objects.

```{python}
#| classes: styled-output
unknown_types = sio.get_untrusted_types(file="model.skops")
# investigate the contents of unknown_types, and only load if you trust
# everything you see.
for i, a in enumerate(unknown_types):
    print(f"Unknown type at {i} is {a}.")

clf = sio.load("model.skops", trusted=unknown_types)
score = clf.score(X_test, y_test)
print(f"We have score of {score} after loading the object again.")
```
```{python}
#| echo: false
%%bash
# Code to remove above files
rm model.skops
```

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-mp-skops.io}

## Further investigations for `skops.io`

1. The included PCA works with `float64`, this is not necessary, can we reduce the file size by switching to `float16`? 
Hint: look at `voting_clf[0].components_`.

1. Apply the self defined kernel from @exr-data-mp-pickle and test the load/recover cycle. 
::::
:::

## Comparison of the different approaches

The [docs](https://scikit-learn.org/stable/model_persistence.html#summarizing-the-key-points) are doing an excellent job in summarizing the key differences.

> Based on the different approaches for model persistence, the key points for each approach can be summarized as follows:
> 
> 1. ONNX: It provides a uniform format for persisting any machine learning or deep learning model (other than `scikit-learn`) and is useful for model inference (predictions). It can however, result in compatibility issues with different frameworks.
>
> 1. `skops.io`: Trained `scikit-learn` models can be easily shared and put into production using `skops.io`. It is more secure compared to alternate approaches based on `pickle` because it does not load arbitrary code unless explicitly asked for by the user. Such code needs to be packaged and importable in the target Python environment.
>
> 1. `joblib`: Efficient memory mapping techniques make it faster when using the same persisted model in multiple Python processes when using `mmap_mode="r"`. It also gives easy shortcuts to compress and decompress the persisted object without the need for extra code. However, it may trigger the execution of malicious code when loading a model from an untrusted source as any other `pickle`-based persistence mechanism.
> 
> 1. `pickle`: It is native to Python and most Python objects can be serialized and deserialized using `pickle`, including custom Python classes and functions as long as they are defined in a package that can be imported in the target environment. While `pickle` can be used to easily save and load `scikit-learn` models, it may trigger the execution of malicious code while loading a model from an untrusted source. `pickle` can also be very efficient memorywise if the model was persisted with protocol=5 but it does not support memory mapping.
>
> 1. `cloudpickle`: It has comparable loading efficiency as `pickle` and `joblib` (without memory mapping), but offers additional flexibility to serialize custom Python code such as `lambda` expressions and interactively defined functions and classes. It might be a last resort to persist pipelines with custom Python components such as a `sklearn.preprocessing.FunctionTransformer` that wraps a function defined in the training script itself or more generally outside of any importable Python package. Note that `cloudpickle` offers no forward compatibility guarantees and you might need the same version of `cloudpickle` to load the persisted model along with the same version of all the libraries used to define the model. As the other pickle-based persistence mechanisms, it may trigger the execution of malicious code while loading a model from an untrusted source.
>
> Source: [scikit-learn.org](https://scikit-learn.org/stable/model_persistence.html#summarizing-the-key-points), accessed 07.03.2025.

## Further considerations

Now that we know how to persist our models, or at least hope we do so, we need to talk about how we keep track of our different model versions (parameters, training data, random seeds, etc.).

In the previous exercises we created multiple versions of our model and stored them to disc.
If we now look at the different files, do we still know which version corresponds to which code block? 

As we experiment with different parameters for our composite method in pursuit of better results, we’ll likely generate even more model variations. To ensure reproducibility, we need a way to track our models alongside the code that produced them.
This is what we are going to look at in the next section. 