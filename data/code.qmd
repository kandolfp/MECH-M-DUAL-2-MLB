# Code persistence {#sec-data-repro}

When talking about storing the model we quickly realise that there is more to this task than simply calling `pickle.dump`.
The same is true for persisting the code. 

Luckily, we already have the perfect tool for that in use, `git`[^git].
We can make sure to commit our source files and when using a proper package manager like `pdm`[^pdm] we can hope to reproduce our environment for the formats that require these features. 

To illustrate this, we move the code from @lst-data-mp-toyexample into a project.
In this process we move from the dense script to a proper file structure by splitting up the source script into several parts.
Furthermore, we rework some of the details, like only downloading the data once.

::: {.callout-important}
## Reference repository

The following example is referenced according to the GitHub repository [kandolfp/MECH-M-DUAL-2-MLB-DATA](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA) on GitHub.

Please use it as reference and move along the corresponding commit SHAs to see certain reference points.

This also means some of the code included below is not interactively created but static with regard to the execution time. 
:::

After the redesign we get a structure looking something like the following:

```bash
MLB-DATA$ tree
.
├── pdm.lock
├── pyproject.toml
└── src
    └── MECH-M-DUAL-2-MLB-DATA
        ├── data.py
        ├── inference.py
        ├── model.py
        ├── myio.py
        └── train.py

3 directories, 7 files
```

We can train the model by calling `train.py` (in this case with a logger[^logging] on `DEBUG` )
```bash
MLB-DATA$ pdm run src/MECH-M-DUAL-2-MLB-DATA/train.py 
DEBUG:root: Loaded the data with Split of 60 to 20 per category.
DEBUG:root: Create classifier
DEBUG:root: Train classifier
DEBUG:root: Score classifier
INFO:root: We have a hard voting score of 0.8
DEBUG:root: Save clf to skops file models/model.skops
```

Of course we can also load the model again and do inference with it by calling `inference.py`
```bash
MLB-DATA$ pdm run src/MECH-M-DUAL-2-MLB-DATA/inference.py 
DEBUG:root: Loaded the data with Split of 60 to 20 per category.
DEBUG:root: Load classifier
DEBUG:root: Load clf from skops file models/model.skops
WARNING:root: Unknown type at 0 is sklearn.utils._bunch.Bunch.
DEBUG:root: Score classifier
INFO:root: We have a hard voting score of 0.8
```

Now we can start connecting the model and the code.
Our model was created at commit [`22788a6`](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/22788a608b76d0b3180728c36c0351cc531b1def).

Now there are several things we can do to make sure this is reflected within our little project.

1. Make the convention to never train and store an experiment for later use as long as you have uncommitted changes in your code (easier said than done).
1. Make sure to note somewhere what commit SHA is the current HEAD when storing the model.
This allows ue to reproduce it in case of data corruption or loss and for comparison with other models. 
1. When running inference we can check for the SHA in case the project dependencies have changed and we get problems loading the file. 
1. Every time we change a parameter we get a new commit, which is not very nice. 

This is rather cumbersome and requires a lot of discipline, it will also become tricky if several people work on the same project and run experiments with different parameters. 

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-code-git}

## Check for a _dirty_ `git` repository

Implement a function (or a decorator) in Python that uses [`GitPython`](https://gitpython.readthedocs.io/en/stable/) or some alternative to introduce a safeguard such that training can only be called if the repository is not _dirty_.

Optional: Extend the implementation and check if the local repository is not behind the remote?
::::
:::

## Externalize the parameters/configuration

First thing we do is we externalize the configuration to make sure this is no longer part of our code source and a commit to the code source stands for a real change in the project and not a _simple experiment_.

`yaml` is the format to go for these aspects, see [Wikipedia](https://en.wikipedia.org/wiki/YAML).
It is a human readable data serialization language. 
One possible interpretation of the config (among many others is)

```yaml
PCA:
  type: sklearn.decomposition.PCA
      init_args:
        n_components: 41

VotingClassifier:
  type: sklearn.ensemble.VotingClassifier
      init_args:
        flatten_transform: False
        estimators:
    - LinearDiscriminantAnalysis
    - RandomForestClassifier
    - SVC

LinearDiscriminantAnalysis:
  type: sklearn.discriminant_analysis.LinearDiscriminantAnalysis
            init_args:
              solver: svd

RandomForestClassifier:
  type: sklearn.ensemble.RandomForestClassifier
            init_args:
              n_estimators: 500
              max_leaf_nodes: 2
              random_state: 6020

SVC:
  type: sklearn.svm.SVC
            init_args:
              kernel: linear
              probability: True
              random_state: 6020
```
and we can use the Python package [`omegaconf`](https://omegaconf.readthedocs.io/en/2.3_branch/) to load and use it.

One of feature of the `OmegaConf`class is that we can use _unpacking_ [^docs] and therefore write a line like:
```{.python}
PCA(**params["PCA"].init_args)
```
Have a look at the commit [9f7dead](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/9f7dead95663d9cef7a7cc21ec8133b03acb650c) to see this in action.
As a template we include the above parameters as `params.yaml`.


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-code-params}

## Externalize params

We can generate the entire model from the params, even the different classes.
By using the function `from importlib import import_module` we can dynamically load a class with the following snippet:

```{.python}
module = import_module(params["PCA"].type.rsplit(".", 1)[0])
PCA = getattr(module, params["PCA"].type.rsplit(".", 1)[-1])(
    **params["PCA"].init_args
)
```

Use this to make the model creation more and more dynamic.

1. Replace the array `estimators=[]` by dynamically loading the different estimators.
1. Use the same for the pipeline

Note: If you see an advantage in rewriting the config structure to make your code easier feel free to do so.
::::
:::

Now that the config is externalized we can continue on our quest to persist our work usefully. 

## Data persistence

Or model depends on the code, the configuration but crucially, also on the input data itself.
In order to make sure that we can reliably reproduce a model we also need to make sure our data is reproducible. 

In our little toy example we use some files from GitHub but let us still make sure they are tracked within our system. 

One tool for data version control is [`dvc`](https://dvc.org/doc).
As it is written in Python we can even add and track the version via our package manager `pdm add dvc`. 
Once we have it installed we can run

```bash
MLB-DATA$ pdm run dvc init
Initialized DVC repository.

You can now commit the changes to git.

+---------------------------------------------------------------------+
|                                                                     |
|        DVC has enabled anonymous aggregate usage analytics.         |
|     Read the analytics documentation (and how to opt-out) here:     |
|             <https://dvc.org/doc/user-guide/analytics>              |
|                                                                     |
+---------------------------------------------------------------------+

What's next?
------------
- Check out the documentation: <https://dvc.org/doc>
- Get help and share ideas: <https://dvc.org/chat>
- Star us on GitHub: <https://github.com/iterative/dvc>
```

and commit the directory `.dvc` to git and our project now runs with `dvc`, see commit [df38086](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/df38086b756ed27e70fa2573b8d01c51ea69086c).

To add the `data` directory simply run

```bash
MLB-DATA$ pdm run dvc add data
100% Adding...|███████████████████████████████████████|1/1 [00:00,  5.67file/s]
                                                                                                                                                                                               
To track the changes with git, run:

        git add data.dvc

To enable auto staging, run:

        dvc config core.autostage true
```

and we also add `data.dvc` to the `git` as suggested. 

::: {.callout-note}
At this point we have the two files `catData_w.mat` and `dogData_w.mat` in this directory and they are under version control from `dvc`.
:::

If we take a look into `data.dvc`  we can see that is tracks the files via md5 sha and includes some additional information: 

```bash
MLB-DATA$ cat data.dvc 
outs:
- md5: 5987e80830fc2caf6d475da3deca1dfe.dir
  size: 111165
  nfiles: 2
  hash: md5
  path: data
```

As mentioned, `dvc` works similar to `git` so eventually we will need to include a remote that we _push_ data to. 
For now we just work locally and as we could do with a `git` repository. 

Other than that we can now change the files use `dvc add data` and as soon as we commit the corresponding change in the `data.dvc` to `git` we know exactly what data is used and we can also restore it.

To do these operations `dvc` uses a cache (default it is in `.dvc/cache`).

The most important `dvc` commands are (we link the docs for an extended reference):

- [`dvc add`](https://dvc.org/doc/command-reference/add) to add a file or directory
- [`dvc checkout`](https://dvc.org/doc/command-reference/checkout) brings your work space up to date, according to the `.dvc` files current states
- [`dvc commit`](https://dvc.org/doc/command-reference/checkout) updates the `.dvc` files and stores the content in the cache, most of the time called implicitly
- [`dvc config`](https://dvc.org/doc/command-reference/config) view and change the config for the repo or globally
- [`dvc data status`](https://dvc.org/doc/command-reference/data/status) chow changes to the files in the work space with respect to the`git` `HEAD`
- [`dvc destroy`](https://dvc.org/doc/command-reference/destroy) remove all files and `dvc` structures for the current project, the cache as well but the symlinks will be replaced by the actual data so the current sate is preserved
- [`dvc exp`](https://dvc.org/doc/command-reference/exp) has multiple subcommands and is used to handle experiments, we will use this later
- [`dvc fetch`](https://dvc.org/doc/command-reference/fetch) download files from the remote repository to the cache
- [`dvc pull`](https://dvc.org/doc/command-reference/pull) download files from the remote and make them visible in the working space
- [`dvc push`](https://dvc.org/doc/command-reference/push) upload the tracked files to the remote

For the other commands run `dvc --help` or look at the [docs](https://dvc.org/doc/command-reference).
`dvc` also allows for nice pipelines and automatic computation.
This is far more advanced than we need right now so we will not introduce it her but leave it for exercises or advanced studies.

Now our files are tracked, but as you probably realised we did not add the `module` folder to `dvc`. 
This is due to the fact that we can use the `dvc exp` feature to allow for more fine grained control and even parameter overviews.
Furthermore, we can use logging features to integrate with this system even better.

## `dvclive` for experiment management

`dvclive` works best with the big ML Frameworks like `keras` or `pytorch` but we can also utilize it for our toy example.
The introduction to the experiment management form the `dvc` perspective can be found [here](https://dvc.org/doc/start/experiments).

To show some of the `dvclive` features we reworked to code, see commit [7f407b](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/7f407bab7bfcc191014eeb3b939bb2fe0456fb58) (do not forget `pdm add dvclive`).
Now, when we run our job and it will create the `dvclive` directory with a couple of subdirectories containing our metrics, looking like this:

```bash
MLB-DATA$ pdm run src/MECH-M-DUAL-2-MLB-DATA/train.py 
INFO:root: We have a hard voting train-score of 1.0
INFO:root: We have a hard voting test-score of 0.8
100% Adding...|███████████████████████████████████████|1/1 [00:00,  7.64file/s]
```

::: {.callout collapse="true"}
## Output with logger on DEBUG.
```bash
MLB-DATA$ pdm run python src/MECH-M-DUAL-2-MLB-DATA/train.py 
DEBUG:root: Loaded the data with Split of 60 to 20 per category.
DEBUG:root: Load config
DEBUG:root: Create classifier
DEBUG:root: Train classifier
DEBUG:root: Score classifier
INFO:root: We have a hard voting train-score of 1.0
INFO:root: We have a hard voting test-score of 0.8
DEBUG:root: Save clf to skops file dvclive/artifacts/model.skops
100% Adding...|███████████████████████████████████████|1/1 [00:00,  9.35file/s]
DEBUG:scmrepo.git: Stashing workspace                                                                                                                                                          
DEBUG:scmrepo.git.stash: Stashing changes in 'refs/stash'
DEBUG:scmrepo.git: Detaching HEAD at 'HEAD'
DEBUG:scmrepo.git.stash: Applying stash commit '7d0806908ca50c01b5626e944591e138a8cebc28'
Collecting files and computing hashes in data        |0.00 [00:00,     ?file/s]
DEBUG:fsspec.memoryfs: open file /.UFCO7_lfUrOd1sAFVpO-vw.tmp
DEBUG:fsspec.memoryfs: info: memory://.UFCO7_lfUrOd1sAFVpO-vw.tmp
Collecting files and computing hashes in data        |0.00 [00:00,     ?file/s]
DEBUG:fsspec.memoryfs: open file /.4cDC9-sZfddrOSDnK8EeJA.tmp
DEBUG:fsspec.memoryfs: info: memory://.4cDC9-sZfddrOSDnK8EeJA.tmp
DEBUG:scmrepo.git.stash: Stashing changes in 'refs/exps/stash'                                                                                                                                 
DEBUG:scmrepo.git: Restore HEAD to 'main'
DEBUG:scmrepo.git: Restoring stashed workspace
DEBUG:scmrepo.git.stash: Popping from stash 'refs/stash'
DEBUG:scmrepo.git.stash: Applying stash commit '7d0806908ca50c01b5626e944591e138a8cebc28'
DEBUG:scmrepo.git.stash: Dropping 'refs/stash@{0}'
DEBUG:scmrepo.git.stash: Dropping 'refs/exps/stash@{0}'
DEBUG:scmrepo.git: Detaching HEAD at 'f264314e5442315c74c395a89c2c73a3b7269f90'
DEBUG:scmrepo.git.stash: Applying stash commit 'adac86009edec08745dffc95c3e6eb68c9fb4dcb'
DEBUG:fsspec.memoryfs: open file /.cSw5cWuYdyH1AkprvFGF8g.tmp
DEBUG:fsspec.memoryfs: info: memory://.cSw5cWuYdyH1AkprvFGF8g.tmp
Collecting files and computing hashes in data         0.00 [00:00,     ?file/s]
DEBUG:fsspec.memoryfs: open file /.-QV8fKiPGOS3tR1JS0SE2w.tmp
DEBUG:fsspec.memoryfs: info: memory://.-QV8fKiPGOS3tR1JS0SE2w.tmp
Collecting files and computing hashes in data        |0.00 [00:00,     ?file/s]
DEBUG:fsspec.memoryfs: open file /.nJTqHs5Fi91gZyK82VYbvA.tmp
DEBUG:fsspec.memoryfs: info: memory://.nJTqHs5Fi91gZyK82VYbvA.tmp
DEBUG:scmrepo.git: Restore HEAD to 'main'
```

Here we can also see how `dvc` interacts with `git` to store the files. 
:::

The experiment is automatically added. 
We can check this with:

```
MLB-DATA$ pdm run dvc exp show
 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
  Experiment                 Created        trainscore   testscore   PCA/n_components   LinearDiscriminantAnalysis/solver   RandomForestClassifier/n_estimators   RandomForestClassifier/max_leaf_nodes   RandomForestClassifier/random_state   SVC/kernel   SVC/probability   SVC/random_state   PCA.type                    PCA.init_args.n_components   VotingClassifier.type               VotingClassifier.init_args.flatten_transform   VotingClassifier.estimators                                       LinearDiscriminantAnalysis.type                            LinearDiscriminantAnalysis.init_args.solver   RandomForestClassifier.type               RandomForestClassifier.init_args.n_estimators   RandomForestClassifier.init_args.max_leaf_nodes   RandomForestClassifier.init_args.random_state   SVC.type          SVC.init_args.kernel   SVC.init_args.probability   SVC.init_args.random_state  
 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
  workspace                  -                       1         0.8   41                 svd                                 500                                   2                                       6020                                  linear       True              6020               sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                        
  main                       Mar 19, 2025            -           -   -                  -                                   -                                     -                                       -                                     -            -                 -                  sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                        
  └── 9e83c73 [sural-cyma]   10:03 AM                1         0.8   41                 svd                                 500                                   2                                       6020                                  linear       True              6020               sklearn.decomposition.PCA   41                           sklearn.ensemble.VotingClassifier   False                                          ['LinearDiscriminantAnalysis', 'RandomForestClassifier', 'SVC']   sklearn.discriminant_analysis.LinearDiscriminantAnalysis   svd                                           sklearn.ensemble.RandomForestClassifier   500                                             2                                                 6020                                            sklearn.svm.SVC   linear                 True                        6020                        
 ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 

```

The last command gives us an overview of our experiments (check out the options, especially `--num` and `-A`).
Each experiment has a unique name, as we did not specify anything a random name is created, in our case `sural-cyma` (they are often fun but can be hard to infer meaning and speaking names are most the time more useful).
More importantly, we can see the score for our model and the parameters, this allows for a quick check to see how the current model performs in comparison to another version. 

::: {.callout-note}
We can see that the parameter from `params.yaml` are automatically added and our code somewhat duplicated them.
:::

`dvclive` relies on `git` to do its magic for files and the `dvc` cache for large files. 
How this works is that a reference inside `git` is created for each experiment and stores the data there.
We can see this in the above logger output.
Furthermore, we did not commit our changes to git (bad practice!!!) but they are stored alongside with the experiment, so no information is lost.
To clean up we commit our changes as we know they work and then rerun the code for a new experiment, see [519a854](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/519a8544c82667cec5356f92ddde77993f0a0e76)

By default this is not moved to the `git` remote, to do so we need to run `dvc exp push`.

```bash
MLB-DATA$ pdm run dvc exp push origin
Collecting                                           |0.00 [00:00,    ?entry/s]
Pushing
Experiment sural-cyma is up to date on Git remote 'origin'.
```

We can also see this in the `git reflog`

```bash
MLB-DATA$ git reflog
519a854 (HEAD -> main) HEAD@{0}: dvc: Restore HEAD to 'main'
1f2bee5 HEAD@{1}: commit: dvc: commit experiment 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
519a854 (HEAD -> main) HEAD@{2}: checkout: moving from main to 519a8544c82667cec5356f92ddde77993f0a0e76
519a854 (HEAD -> main) HEAD@{3}: dvc: Restore HEAD to 'main'
519a854 (HEAD -> main) HEAD@{4}: checkout: moving from main to 519a8544c82667cec5356f92ddde77993f0a0e76
519a854 (HEAD -> main) HEAD@{5}: commit: feat: include dvc-live
f264314 HEAD@{6}: dvc: Restore HEAD to 'main'
9e83c73 HEAD@{7}: commit: dvc: commit experiment 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
f264314 HEAD@{8}: checkout: moving from main to f264314e5442315c74c395a89c2c73a3b7269f90
f264314 HEAD@{9}: dvc: Restore HEAD to 'main'
f264314 HEAD@{10}: checkout: moving from main to f264314e5442315c74c395a89c2c73a3b7269f90
f264314 HEAD@{11}: dvc: Restore HEAD to 'main'
4afc6af HEAD@{12}: commit: dvc: commit experiment 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
f264314 HEAD@{13}: checkout: moving from main to f264314e5442315c74c395a89c2c73a3b7269f90
f264314 HEAD@{14}: dvc: Restore HEAD to 'main'
f264314 HEAD@{15}: checkout: moving from main to f264314e5442315c74c395a89c2c73a3b7269f90
``` 

or via `ls` (on Linux)

```bash
MLB-DATA$ ls .git/refs/exps/51/9a8544c82667cec5356f92ddde77993f0a0e76/sural-cyma 
.git/refs/exps/51/9a8544c82667cec5356f92ddde77993f0a0e76/sural-cyma
```

We can also see, that a new file has appeared `dvc.yaml` with the following content
```yaml
params:
- dvclive/params.yaml
metrics:
- dvclive/metrics.json
plots:
- dvclive/plots/metrics:
    x: step
- dvclive/plots/sklearn/confusion_matrix.json:
    template: confusion
    x: actual
    y: predicted
    title: Confusion Matrix
    x_label: True Label
    y_label: Predicted Label
artifacts:
  model:
    path: dvclive/artifacts/model.skops
    type: model
```
reflecting our configuration.

::: {.callout-important}
In order to show the next feature we need to remove the `dvc.yaml` and the `dvclive/artifacts/model.skops.dvc` files.
As the current configuration would produce a conflict. 
:::

Before we add this file to the our `git` repository we introduce the pipeline feature of `dvc` to see some additional possibilities for the configuration modification.
These so called `stages` are also of the `dvc.yaml` as well.


[^git]: see the lecture MECH-M-DUAL-1-SWD, Chapter 3 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-SWD/basics/versioncontrol.html)
[^pdm]: see the lecture MECH-M-DUAL-1-SWD, Chapter 2 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-SWD/basics/packagemanager.html)
[^logging]: see the lecture MECH-M-DUAL-1-SWD, Chapter 11 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-SWD/errorhandling/logging.html)
[^docs]: see the Python documentation or follow the direct [link](https://docs.python.org/3/tutorial/controlflow.html#tut-unpacking-arguments)