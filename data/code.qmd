# Code persistence {#sec-data-repro}

When talking about storing the model we quickly realise that there is more to this task than simply calling `pickle.dump`.
The same is true for persisting the code. 

Luckily, we already have the perfect tool for that in use, `git`[^git].
We can make sure to commit our source files and when using a proper package manager like `pdm`[^pdm] we can hope to reproduce our environment for the formats that require these features. 

To illustrate this, we move the code from @lst-data-mp-toyexample into a project.
In this process we move from the dense script to a proper file structure by splitting up the source script into several parts.
Furthermore, we rework some of the details, like only downloading the data once.

::: {.callout-important}
## Reference repository

The following example is referenced according to the GitHub repository [kandolfp/MECH-M-DUAL-2-MLB-DATA](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA) on GitHub.

Please use it as reference and move along the corresponding commit SHAs to see certain reference points.

This also means some of the code included below is not interactively created but static with regard to the execution time. 
:::

After the redesign we get a structure looking something like the following:

```bash
MLB-DATA$ tree
.
├── pdm.lock
├── pyproject.toml
└── src
    └── MECH-M-DUAL-2-MLB-DATA
        ├── data.py
        ├── inference.py
        ├── model.py
        ├── myio.py
        └── train.py

3 directories, 7 files
```

We can train the model by calling `train.py` (in this case with a logger[^logging] on `DEBUG` )
```bash
MLB-DATA$ pdm run src/MECH-M-DUAL-2-MLB-DATA/train.py 
DEBUG:root: Loaded the data with Split of 60 to 20 per category.
DEBUG:root: Create classifier
DEBUG:root: Train classifier
DEBUG:root: Score classifier
INFO:root: We have a hard voting score of 0.8
DEBUG:root: Save clf to skops file models/model.skops
```

Of course we can also load the model again and do inference with it by calling `inference.py`
```bash
MLB-DATA$ pdm run src/MECH-M-DUAL-2-MLB-DATA/inference.py 
DEBUG:root: Loaded the data with Split of 60 to 20 per category.
DEBUG:root: Load classifier
DEBUG:root: Load clf from skops file models/model.skops
WARNING:root: Unknown type at 0 is sklearn.utils._bunch.Bunch.
DEBUG:root: Score classifier
INFO:root: We have a hard voting score of 0.8
```

Now we can start connecting the model and the code.
Our model was created at commit [`22788a6`](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/22788a608b76d0b3180728c36c0351cc531b1def).

Now there are several things we can do to make sure this is reflected within our little project.

1. Make the convention to never train and store an experiment for later use as long as you have uncommitted changes in your code (easier said than done).
1. Make sure to note somewhere what commit SHA is the current HEAD when storing the model.
This allows ue to reproduce it in case of data corruption or loss and for comparison with other models. 
1. When running inference we can check for the SHA in case the project dependencies have changed and we get problems loading the file. 
1. Every time we change a parameter we get a new commit, which is not very nice. 

This is rather cumbersome and requires a lot of discipline, it will also become tricky if several people work on the same project and run experiments with different parameters. 

::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-code-git}

## Check for a _dirty_ `git` repository

Implement a function (or a decorator) in Python that uses [`GitPython`](https://gitpython.readthedocs.io/en/stable/) or some alternative to introduce a safeguard such that training can only be called if the repository is not _dirty_.

Optional: Extend the implementation and check if the local repository is not behind the remote?
::::
:::


First thing we do is we externalize the configuration to make sure this is no longer part of our code source and a commit to the code source stands for a real change in the project and not a _simple experiment_.

`yaml` is the format to go for these aspects, see [Wikipedia](https://en.wikipedia.org/wiki/YAML).
It is a human readable data serialization language. 
One possible interpretation of the config (among many others is)

```yaml
---
model:
  components:
    - type: sklearn.decomposition.PCA
      init_args:
        n_components: 41
    - type: sklearn.ensemble.VotingClassifier
      init_args:
        flatten_transform: False
        estimators:
          - type: sklearn.discriminant_analysis.LinearDiscriminantAnalysis
            init_args:
              solver: svd
          - type: sklearn.ensemble.RandomForestClassifier
            init_args:
              n_estimators: 500
              max_leaf_nodes: 2
              random_state: 6020
          - type: sklearn.svm.SVC
            init_args:
              kernel: linear
              probability: True
              random_state: 6020
```
and we can use the Python package [`omegaconf`](https://omegaconf.readthedocs.io/en/2.3_branch/) to load and use it.

One of feature of the `OmegaConf`class is that we can use _unpacking_ [^docs] and therefore write a line like:
```{.python}
PCA(**config.model.components[0].init_args)
```
Have a look at the commit [bbe32f](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/bbe32fac13b251738612bc90d198a6a871c18838) to see this in action. 


::: {.callout-caution appearance="simple" icon=false}
:::: {#exr-data-code-config}

## Externalize config

We can generate the entire model from the config, even the different classes.
By using the function `from importlib import import_module` we can dynamically load a class with the following snippet:

```{.python}
module = import_module(config.model.components[0].type.rsplit(".", 1)[0])
PCA = getattr(module, config.model.components[0].type.rsplit(".", 1)[-1])(
    **config.model.components[0].init_args
)
```

Use this to make the model creation more and more dynamic.

1. Replace the array `estimators=[]` by dynamically loading the different estimators.
1. Use the same for the pipeline

Note: If you see an advantage in rewriting the config structure to make your code easier feel free to do so.
::::
:::

Now that the config is externalized we can continue on our quest to persist our work usefully. 

## Data persistence

Or model depends on the code, the configuration but crucially, also on the input data itself.
In order to make sure that we can reliably reproduce a model we also need to make sure our data is reproducible. 

In our little toy example we use some files from github but let us still make sure they are tracked within our system. 

On tool to do data version control is [`dvc`](https://dvc.org/doc).
As it is written in Python we can even add and track the version via our package manager. 
Once we have it installed we can run

```bash
MLB-DATA$ pdm run dvc init
Initialized DVC repository.

You can now commit the changes to git.

+---------------------------------------------------------------------+
|                                                                     |
|        DVC has enabled anonymous aggregate usage analytics.         |
|     Read the analytics documentation (and how to opt-out) here:     |
|             <https://dvc.org/doc/user-guide/analytics>              |
|                                                                     |
+---------------------------------------------------------------------+

What's next?
------------
- Check out the documentation: <https://dvc.org/doc>
- Get help and share ideas: <https://dvc.org/chat>
- Star us on GitHub: <https://github.com/iterative/dvc>
```

and commit the directory `.dvc` to git and our project now runs with `dvc`, see commit [ba8c89](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/ba8c89a99637f8530e697c2a26759f5ce1410d76).

To add the `data` directory simply run

```bash
MLB-DATA$ pdm run dvc add data
100% Adding...|███████████████████████████████████████|1/1 [00:00,  5.67file/s]
                                                                                                                                                                                               
To track the changes with git, run:

        git add data.dvc

To enable auto staging, run:

        dvc config core.autostage true
```

and we also add `data.dvc` to the `git` as suggested. 

::: {.callout-note}
At this point we have the two files `catData_w.mat` and `dogData_w.mat` in this directory and they are under version control from `dvc`.
:::

If we take a look into `data.dvc`  we can see that is tracks the files via md5 sha and includes some additional information: 

```bash
MLB-DATA$ cat data.dvc 
outs:
- md5: 5987e80830fc2caf6d475da3deca1dfe.dir
  size: 111165
  nfiles: 2
  hash: md5
  path: data
```

As mentioned, `dvc` works similar to `git` so eventually we will need to include a remote that we _push_ data to. 
For now we just work locally and as we could do with a `git` repository. 

Other than that we can now change the files use `dvc add data` and as soon as we commit the corresponding change in the `data.dvc` to `git` we know exactly what data is used and we can also restore it.

To do these operations `dvc` uses a cache (default it is in `.dvc/cache`).

The most important `dvc` commands are (we link the docs for an extended reference):

- [`dvc add`](https://dvc.org/doc/command-reference/add) to add a file or directory
- [`dvc checkout`](https://dvc.org/doc/command-reference/checkout) brings your work space up to date, according to the `.dvc` files current states
- [`dvc commit`](https://dvc.org/doc/command-reference/checkout) updates the `.dvc` files and stores the content in the cache, most of the time called implicitly
- [`dvc config`](https://dvc.org/doc/command-reference/config) view and change the config for the repo or globally
- [`dvc data status`](https://dvc.org/doc/command-reference/data/status) chow changes to the files in the work space with respect to the`git` `HEAD`
- [`dvc destroy`](https://dvc.org/doc/command-reference/destroy) remove all files and `dvc` structures for the current project, the cache as well but the symlinks will be replaced by the actual data so the current sate is preserved
- [`dvc exp`](https://dvc.org/doc/command-reference/exp) has multiple subcommands and is used to handle experiments, we will use this later
- [`dvc fetch`](https://dvc.org/doc/command-reference/fetch) download files from the remote repository to the cache
- [`dvc pull`](https://dvc.org/doc/command-reference/pull) download files from the remote and make them visible in the working space
- [`dvc push`](https://dvc.org/doc/command-reference/push) upload the tracked files to the remote

For the other commands run `dvc --help` or look at the [docs](https://dvc.org/doc/command-reference).
`dvc` also allows for nice pipelines and automatic computation.
This is far more advanced than we need right now so we will not introduce it her but leave it for exercises or advanced studies.

Now our files are tracked, but as you probably realised we did not add the `module` folder to `dvc`. 
This is due to the fact that we can use the `dvc exp` feature to allow for more fine grained control and even parameter overviews.
Furthermore, we can use logging features to integrate with this system even better.

## `dvclive` for experiment management

`dvclive` works best with the big ML Frameworks like `keras` or `pytorch` but we can also utilize it for our toy example.
The introduction to the experiment management form the `dvc` perspective can be found [here](https://dvc.org/doc/start/experiments).

To show some of the `dvclive` features we reworked to code, see commit [7f407b](https://github.com/kandolfp/MECH-M-DUAL-2-MLB-DATA/tree/7f407bab7bfcc191014eeb3b939bb2fe0456fb58).
Now, when we run our job and it will create the `dvclive` directory with a couple of subdirectories containing our metrics, looking like this:

```bash
MLB-DATA$ pdm run src/MECH-M-DUAL-2-MLB-DATA/train.py 
INFO:root: We have a hard voting score of 0.8
100% Adding...|███████████████████████████████████████|1/1 [00:00,  7.64file/s]
```

and the experiment is automatically added. 
We can check this with:

```
MLB-DATA$ pdm run dvc exp show
 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────>
  Experiment                 Created    train_score   test_score   PCA/n_components   LinearDiscriminantAnalysis/solver   RandomForestClassifier/n_estimators   RandomForestClassifier/max_lea>
 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────>
  workspace                  -                    1          0.8   41                 svd                                 500                                   2                             >
  main                       07:37 PM             -            -   -                  -                                   -                                     -                             >
  └── 22ae3e7 [faint-hate]   07:42 PM             1          0.8   41                 svd                                 500                                   2                             >
 ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────>
```

The last command gives us an overview of our experiments (check out the options, especially `--num` and `-A`).
Each experiment has a unique name, as we did not specify anything a random name is created, in our case `faint-hate` (they are often fun but can be hard to infer meaning and speaking names are more useful).
More importantly, we can see the score for our model and the parameters, this allows for a quick check to see how the current model performs in comparison to another version. 

`dvclive` relies on `git` to do the magic for files and `dvc` for large files. 
How this works is that a reference inside `git` is created for each experiment and stores the data there.
By default this is not moved to the `git` remote, to do so we need to run `dvc exp push`.


[^git]: see the lecture MECH-M-DUAL-1-SWD, Chapter 3 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-SWD/basics/versioncontrol.html)
[^pdm]: see the lecture MECH-M-DUAL-1-SWD, Chapter 2 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-SWD/basics/packagemanager.html)
[^logging]: see the lecture MECH-M-DUAL-1-SWD, Chapter 11 or follow the direct [link](https://kandolfp.github.io/MECH-M-DUAL-1-SWD/errorhandling/logging.html)
[^docs]: see the Python documentation or follow the direct [link](https://docs.python.org/3/tutorial/controlflow.html#tut-unpacking-arguments)